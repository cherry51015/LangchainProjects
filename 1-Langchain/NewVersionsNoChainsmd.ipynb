{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b761a2c",
   "metadata": {},
   "source": [
    "Great question! This is actually a **huge shift** in LangChain's philosophy. Let me break down why this happened and how to think about the new approach.\n",
    "\n",
    "## Why the Big Change?\n",
    "\n",
    "### The Old Problem with Chains\n",
    "\n",
    "In early LangChain, you'd build things like this:\n",
    "\n",
    "```python\n",
    "# OLD WAY - Chains (deprecated)\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run it\n",
    "result = chain.run(\"colorful socks\")\n",
    "```\n",
    "\n",
    "**Problems with this approach:**\n",
    "1. **Too many specialized classes** - `LLMChain`, `SimpleSequentialChain`, `TransformChain`, `RouterChain`, etc. Confusing!\n",
    "2. **Not composable** - Hard to mix and match or nest chains\n",
    "3. **Inconsistent interfaces** - Some use `.run()`, some `.predict()`, some `.__call__()`\n",
    "4. **Hard to debug** - Black box behavior\n",
    "5. **Not streaming-friendly** - Chains weren't built with streaming in mind\n",
    "\n",
    "### The New Philosophy: LCEL (LangChain Expression Language)\n",
    "\n",
    "LangChain realized: **\"Everything should be a Runnable with a consistent interface\"**\n",
    "\n",
    "Think of it like this:\n",
    "- **Old way:** Different tools (chains) for different jobs, each works differently\n",
    "- **New way:** Universal connectors (pipes) that you can snap together like LEGO\n",
    "\n",
    "## Runnables Explained Simply\n",
    "\n",
    "A **Runnable** is anything that has these methods:\n",
    "- `.invoke()` - Run synchronously\n",
    "- `.stream()` - Stream output\n",
    "- `.batch()` - Process multiple inputs\n",
    "- `.ainvoke()` - Async version\n",
    "- `.astream()` - Async streaming\n",
    "- `.abatch()` - Async batch\n",
    "\n",
    "**Everything** is a Runnable: prompts, LLMs, output parsers, retrievers, custom functions, etc.\n",
    "\n",
    "## Side-by-Side Comparison\n",
    "\n",
    "### Example 1: Simple LLM Call\n",
    "\n",
    "**OLD WAY (Chains):**\n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me a joke about {topic}\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "result = chain.run(topic=\"programmers\")\n",
    "```\n",
    "\n",
    "**NEW WAY (LCEL/Runnables):**\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# The pipe operator | chains runnables together!\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Create chain by piping\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke it\n",
    "result = chain.invoke({\"topic\": \"programmers\"})\n",
    "```\n",
    "\n",
    "**Key difference:** The `|` (pipe) operator! It's like Unix pipes - output of one flows into the next.\n",
    "\n",
    "### Example 2: Sequential Steps with Processing\n",
    "\n",
    "**OLD WAY:**\n",
    "```python\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Create first chain\n",
    "first_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for {product}?\"\n",
    ")\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "# Create second chain\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"company_name\"],\n",
    "    template=\"Write a catchphrase for: {company_name}\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "# Combine them\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "result = overall_chain.run(\"colorful socks\")\n",
    "```\n",
    "\n",
    "**NEW WAY:**\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# First step\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"What is a good name for a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Second step\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"Write a catchphrase for this company: {company_name}\"\n",
    ")\n",
    "\n",
    "# Chain them with pipes!\n",
    "chain = (\n",
    "    prompt1 \n",
    "    | llm \n",
    "    | StrOutputParser()  # Convert output to string\n",
    "    | (lambda name: {\"company_name\": name})  # Format for next prompt\n",
    "    | prompt2 \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"product\": \"colorful socks\"})\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "1. Input `{\"product\": \"colorful socks\"}` → prompt1\n",
    "2. prompt1 output → llm\n",
    "3. llm output → parser (converts to string)\n",
    "4. string → lambda function (reformats for next prompt)\n",
    "5. formatted dict → prompt2\n",
    "6. prompt2 output → llm\n",
    "7. llm output → parser → final result\n",
    "\n",
    "### Example 3: RAG (Retrieval Augmented Generation)\n",
    "\n",
    "**OLD WAY:**\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_texts(texts, OpenAIEmbeddings())\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "result = qa_chain.run(\"What is the capital of France?\")\n",
    "```\n",
    "\n",
    "**NEW WAY:**\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "vectorstore = Chroma.from_texts(texts, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer based on context:\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Build RAG chain\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,  # Retriever gets docs\n",
    "        \"question\": RunnablePassthrough()  # Question passes through\n",
    "    }\n",
    "    | prompt\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"What is the capital of France?\")\n",
    "```\n",
    "\n",
    "**Magic happening:**\n",
    "- `{\"context\": retriever, \"question\": RunnablePassthrough()}` is a **RunnableParallel**\n",
    "- It runs both in parallel: retriever fetches docs, question passes through unchanged\n",
    "- Results merge into one dict that feeds the prompt\n",
    "\n",
    "### Example 4: Streaming (This is where LCEL shines!)\n",
    "\n",
    "**OLD WAY:**\n",
    "Streaming was awkward and inconsistent with chains\n",
    "\n",
    "**NEW WAY:**\n",
    "```python\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Streaming is built-in!\n",
    "for chunk in chain.stream({\"topic\": \"AI\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "Every Runnable supports streaming out of the box!\n",
    "\n",
    "## Text Splitters - The Library Split\n",
    "\n",
    "You noticed this too! Here's what happened:\n",
    "\n",
    "**OLD:**\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "```\n",
    "\n",
    "**NEW:**\n",
    "```python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "```\n",
    "\n",
    "**Why?** LangChain is modularizing:\n",
    "- `langchain_core` - Core abstractions (Runnable, prompts, etc.)\n",
    "- `langchain_text_splitters` - Text splitting utilities\n",
    "- `langchain_community` - Community integrations\n",
    "- `langchain_openai`, `langchain_anthropic` - Provider-specific\n",
    "\n",
    "**Benefits:**\n",
    "- Smaller install if you only need certain parts\n",
    "- Clearer dependencies\n",
    "- Easier maintenance\n",
    "\n",
    "## Key Mental Models for LCEL\n",
    "\n",
    "### 1. **Think in Pipes**\n",
    "```python\n",
    "chain = step1 | step2 | step3 | step4\n",
    "```\n",
    "Data flows left to right, like water through pipes.\n",
    "\n",
    "### 2. **Parallel Processing with Dicts**\n",
    "```python\n",
    "chain = {\n",
    "    \"key1\": runnable1,\n",
    "    \"key2\": runnable2\n",
    "} | next_step\n",
    "```\n",
    "Both runnables run in parallel, outputs combine into a dict.\n",
    "\n",
    "### 3. **RunnablePassthrough**\n",
    "```python\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = {\n",
    "    \"original\": RunnablePassthrough(),\n",
    "    \"processed\": some_runnable\n",
    "}\n",
    "```\n",
    "Passes input through unchanged - useful when you need both original and processed data.\n",
    "\n",
    "### 4. **RunnableLambda** (Custom Functions)\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def my_function(x):\n",
    "    return x.upper()\n",
    "\n",
    "chain = prompt | llm | RunnableLambda(my_function)\n",
    "```\n",
    "Wrap any function to make it a Runnable!\n",
    "\n",
    "## Why This is Better\n",
    "\n",
    "### 1. **Consistency**\n",
    "Every component has the same interface:\n",
    "```python\n",
    "anything.invoke(input)\n",
    "anything.stream(input)\n",
    "anything.batch([input1, input2])\n",
    "```\n",
    "\n",
    "### 2. **Composability**\n",
    "Mix and match freely:\n",
    "```python\n",
    "chain = (\n",
    "    prompt \n",
    "    | llm \n",
    "    | custom_parser \n",
    "    | {\n",
    "        \"summary\": summarizer,\n",
    "        \"sentiment\": sentiment_analyzer\n",
    "    }\n",
    "    | final_formatter\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. **Debugging**\n",
    "```python\n",
    "# See what each step produces\n",
    "chain_with_steps = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Can inspect at any point\n",
    "intermediate = (prompt | llm).invoke(input)\n",
    "print(intermediate)\n",
    "```\n",
    "\n",
    "### 4. **Streaming Everything**\n",
    "```python\n",
    "for chunk in chain.stream(input):\n",
    "    print(chunk)  # Works for ANY chain!\n",
    "```\n",
    "\n",
    "## Practical Migration Guide\n",
    "\n",
    "When you see old code, convert like this:\n",
    "\n",
    "| Old Pattern | New Pattern |\n",
    "|-------------|-------------|\n",
    "| `LLMChain` | `prompt \\| llm` |\n",
    "| `SimpleSequentialChain` | `chain1 \\| chain2 \\| chain3` |\n",
    "| `RetrievalQA` | Manual RAG with `retriever` in dict |\n",
    "| `.run()` | `.invoke()` |\n",
    "| `.predict()` | `.invoke()` |\n",
    "| `chain.apply()` | `.batch()` |\n",
    "\n",
    "## Your Code Template Going Forward\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Components\n",
    "llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"Your template {variable}\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Build chain with pipes\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Use it\n",
    "result = chain.invoke({\"variable\": \"value\"})\n",
    "\n",
    "# Or stream\n",
    "for chunk in chain.stream({\"variable\": \"value\"}):\n",
    "    print(chunk, end=\"\")\n",
    "```\n",
    "\n",
    "**This is your new baseline!** Everything builds from this pattern.\n",
    "\n",
    "Does this clarify the shift? The pipe syntax (`|`) is your new best friend - once you get used to it, it's way more intuitive than remembering 20 different chain classes!Great question! This is actually a **huge shift** in LangChain's philosophy. Let me break down why this happened and how to think about the new approach.\n",
    "\n",
    "## Why the Big Change?\n",
    "\n",
    "### The Old Problem with Chains\n",
    "\n",
    "In early LangChain, you'd build things like this:\n",
    "\n",
    "```python\n",
    "# OLD WAY - Chains (deprecated)\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run it\n",
    "result = chain.run(\"colorful socks\")\n",
    "```\n",
    "\n",
    "**Problems with this approach:**\n",
    "1. **Too many specialized classes** - `LLMChain`, `SimpleSequentialChain`, `TransformChain`, `RouterChain`, etc. Confusing!\n",
    "2. **Not composable** - Hard to mix and match or nest chains\n",
    "3. **Inconsistent interfaces** - Some use `.run()`, some `.predict()`, some `.__call__()`\n",
    "4. **Hard to debug** - Black box behavior\n",
    "5. **Not streaming-friendly** - Chains weren't built with streaming in mind\n",
    "\n",
    "### The New Philosophy: LCEL (LangChain Expression Language)\n",
    "\n",
    "LangChain realized: **\"Everything should be a Runnable with a consistent interface\"**\n",
    "\n",
    "Think of it like this:\n",
    "- **Old way:** Different tools (chains) for different jobs, each works differently\n",
    "- **New way:** Universal connectors (pipes) that you can snap together like LEGO\n",
    "\n",
    "## Runnables Explained Simply\n",
    "\n",
    "A **Runnable** is anything that has these methods:\n",
    "- `.invoke()` - Run synchronously\n",
    "- `.stream()` - Stream output\n",
    "- `.batch()` - Process multiple inputs\n",
    "- `.ainvoke()` - Async version\n",
    "- `.astream()` - Async streaming\n",
    "- `.abatch()` - Async batch\n",
    "\n",
    "**Everything** is a Runnable: prompts, LLMs, output parsers, retrievers, custom functions, etc.\n",
    "\n",
    "## Side-by-Side Comparison\n",
    "\n",
    "### Example 1: Simple LLM Call\n",
    "\n",
    "**OLD WAY (Chains):**\n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me a joke about {topic}\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "result = chain.run(topic=\"programmers\")\n",
    "```\n",
    "\n",
    "**NEW WAY (LCEL/Runnables):**\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# The pipe operator | chains runnables together!\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Create chain by piping\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke it\n",
    "result = chain.invoke({\"topic\": \"programmers\"})\n",
    "```\n",
    "\n",
    "**Key difference:** The `|` (pipe) operator! It's like Unix pipes - output of one flows into the next.\n",
    "\n",
    "### Example 2: Sequential Steps with Processing\n",
    "\n",
    "**OLD WAY:**\n",
    "```python\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Create first chain\n",
    "first_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for {product}?\"\n",
    ")\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "# Create second chain\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"company_name\"],\n",
    "    template=\"Write a catchphrase for: {company_name}\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "# Combine them\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "result = overall_chain.run(\"colorful socks\")\n",
    "```\n",
    "\n",
    "**NEW WAY:**\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# First step\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"What is a good name for a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Second step\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"Write a catchphrase for this company: {company_name}\"\n",
    ")\n",
    "\n",
    "# Chain them with pipes!\n",
    "chain = (\n",
    "    prompt1 \n",
    "    | llm \n",
    "    | StrOutputParser()  # Convert output to string\n",
    "    | (lambda name: {\"company_name\": name})  # Format for next prompt\n",
    "    | prompt2 \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"product\": \"colorful socks\"})\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "1. Input `{\"product\": \"colorful socks\"}` → prompt1\n",
    "2. prompt1 output → llm\n",
    "3. llm output → parser (converts to string)\n",
    "4. string → lambda function (reformats for next prompt)\n",
    "5. formatted dict → prompt2\n",
    "6. prompt2 output → llm\n",
    "7. llm output → parser → final result\n",
    "\n",
    "### Example 3: RAG (Retrieval Augmented Generation)\n",
    "\n",
    "**OLD WAY:**\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_texts(texts, OpenAIEmbeddings())\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "result = qa_chain.run(\"What is the capital of France?\")\n",
    "```\n",
    "\n",
    "**NEW WAY:**\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "vectorstore = Chroma.from_texts(texts, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer based on context:\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Build RAG chain\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,  # Retriever gets docs\n",
    "        \"question\": RunnablePassthrough()  # Question passes through\n",
    "    }\n",
    "    | prompt\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"What is the capital of France?\")\n",
    "```\n",
    "\n",
    "**Magic happening:**\n",
    "- `{\"context\": retriever, \"question\": RunnablePassthrough()}` is a **RunnableParallel**\n",
    "- It runs both in parallel: retriever fetches docs, question passes through unchanged\n",
    "- Results merge into one dict that feeds the prompt\n",
    "\n",
    "### Example 4: Streaming (This is where LCEL shines!)\n",
    "\n",
    "**OLD WAY:**\n",
    "Streaming was awkward and inconsistent with chains\n",
    "\n",
    "**NEW WAY:**\n",
    "```python\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Streaming is built-in!\n",
    "for chunk in chain.stream({\"topic\": \"AI\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "Every Runnable supports streaming out of the box!\n",
    "\n",
    "## Text Splitters - The Library Split\n",
    "\n",
    "You noticed this too! Here's what happened:\n",
    "\n",
    "**OLD:**\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "```\n",
    "\n",
    "**NEW:**\n",
    "```python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "```\n",
    "\n",
    "**Why?** LangChain is modularizing:\n",
    "- `langchain_core` - Core abstractions (Runnable, prompts, etc.)\n",
    "- `langchain_text_splitters` - Text splitting utilities\n",
    "- `langchain_community` - Community integrations\n",
    "- `langchain_openai`, `langchain_anthropic` - Provider-specific\n",
    "\n",
    "**Benefits:**\n",
    "- Smaller install if you only need certain parts\n",
    "- Clearer dependencies\n",
    "- Easier maintenance\n",
    "\n",
    "## Key Mental Models for LCEL\n",
    "\n",
    "### 1. **Think in Pipes**\n",
    "```python\n",
    "chain = step1 | step2 | step3 | step4\n",
    "```\n",
    "Data flows left to right, like water through pipes.\n",
    "\n",
    "### 2. **Parallel Processing with Dicts**\n",
    "```python\n",
    "chain = {\n",
    "    \"key1\": runnable1,\n",
    "    \"key2\": runnable2\n",
    "} | next_step\n",
    "```\n",
    "Both runnables run in parallel, outputs combine into a dict.\n",
    "\n",
    "### 3. **RunnablePassthrough**\n",
    "```python\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = {\n",
    "    \"original\": RunnablePassthrough(),\n",
    "    \"processed\": some_runnable\n",
    "}\n",
    "```\n",
    "Passes input through unchanged - useful when you need both original and processed data.\n",
    "\n",
    "### 4. **RunnableLambda** (Custom Functions)\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def my_function(x):\n",
    "    return x.upper()\n",
    "\n",
    "chain = prompt | llm | RunnableLambda(my_function)\n",
    "```\n",
    "Wrap any function to make it a Runnable!\n",
    "\n",
    "## Why This is Better\n",
    "\n",
    "### 1. **Consistency**\n",
    "Every component has the same interface:\n",
    "```python\n",
    "anything.invoke(input)\n",
    "anything.stream(input)\n",
    "anything.batch([input1, input2])\n",
    "```\n",
    "\n",
    "### 2. **Composability**\n",
    "Mix and match freely:\n",
    "```python\n",
    "chain = (\n",
    "    prompt \n",
    "    | llm \n",
    "    | custom_parser \n",
    "    | {\n",
    "        \"summary\": summarizer,\n",
    "        \"sentiment\": sentiment_analyzer\n",
    "    }\n",
    "    | final_formatter\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. **Debugging**\n",
    "```python\n",
    "# See what each step produces\n",
    "chain_with_steps = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Can inspect at any point\n",
    "intermediate = (prompt | llm).invoke(input)\n",
    "print(intermediate)\n",
    "```\n",
    "\n",
    "### 4. **Streaming Everything**\n",
    "```python\n",
    "for chunk in chain.stream(input):\n",
    "    print(chunk)  # Works for ANY chain!\n",
    "```\n",
    "\n",
    "## Practical Migration Guide\n",
    "\n",
    "When you see old code, convert like this:\n",
    "\n",
    "| Old Pattern | New Pattern |\n",
    "|-------------|-------------|\n",
    "| `LLMChain` | `prompt \\| llm` |\n",
    "| `SimpleSequentialChain` | `chain1 \\| chain2 \\| chain3` |\n",
    "| `RetrievalQA` | Manual RAG with `retriever` in dict |\n",
    "| `.run()` | `.invoke()` |\n",
    "| `.predict()` | `.invoke()` |\n",
    "| `chain.apply()` | `.batch()` |\n",
    "\n",
    "## Your Code Template Going Forward\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Components\n",
    "llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"Your template {variable}\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Build chain with pipes\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Use it\n",
    "result = chain.invoke({\"variable\": \"value\"})\n",
    "\n",
    "# Or stream\n",
    "for chunk in chain.stream({\"variable\": \"value\"}):\n",
    "    print(chunk, end=\"\")\n",
    "```\n",
    "\n",
    "**This is your new baseline!** Everything builds from this pattern.\n",
    "\n",
    "Does this clarify the shift? The pipe syntax (`|`) is your new best friend - once you get used to it, it's way more intuitive than remembering 20 different chain classes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4a8de",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

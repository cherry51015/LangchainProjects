{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0492af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "GROQ_API_KEY=os.getenv('GROQ_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "942fa0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_groq\n",
      "  Obtaining dependency information for langchain_groq from https://files.pythonhosted.org/packages/af/4a/3d6227a16fe9f79968414b50e50869519378b20653805e2e8fab283908e6/langchain_groq-1.1.1-py3-none-any.whl.metadata\n",
      "  Downloading langchain_groq-1.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting groq<1.0.0,>=0.30.0 (from langchain_groq)\n",
      "  Obtaining dependency information for groq<1.0.0,>=0.30.0 from https://files.pythonhosted.org/packages/5f/d6/645a081750e43f858b7d09dce5d8e1e76cf11e7e4bdba81252e04f78963d/groq-0.37.1-py3-none-any.whl.metadata\n",
      "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_groq) (1.2.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain_groq) (4.15.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_groq) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_groq) (0.6.4)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_groq) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_groq) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_groq) (9.1.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1.0.0,>=0.30.0->langchain_groq) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain_groq) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain_groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain_groq) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain_groq) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_groq) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_groq) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_groq) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_groq) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain_groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain_groq) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1.0.0,>=0.30.0->langchain_groq) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_groq) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mydhi\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_groq) (2.6.3)\n",
      "Downloading langchain_groq-1.1.1-py3-none-any.whl (19 kB)\n",
      "Downloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
      "   ---------------------------------------- 0.0/137.5 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 30.7/137.5 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 92.2/137.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 137.5/137.5 kB 1.0 MB/s eta 0:00:00\n",
      "Installing collected packages: groq, langchain_groq\n",
      "Successfully installed groq-0.37.1 langchain_groq-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90489981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model='llama-3.1-8b-instant',groq_api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "556e41d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç, ‡∞®‡±Å‡∞µ‡±ç‡∞µ‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞µ‡±Å?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 49, 'total_tokens': 100, 'completion_time': 0.058814806, 'completion_tokens_details': None, 'prompt_time': 0.002784755, 'prompt_tokens_details': None, 'queue_time': 0.050585415, 'total_time': 0.061599561}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bfac8-ab8c-7b80-8936-6830ac018269-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 49, 'output_tokens': 51, 'total_tokens': 100})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "messages=[SystemMessage(content='Translate the following English sentence to Telugu'),\n",
    "          HumanMessage(content='hello, How are you?')]\n",
    "response=model.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60a3dc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç, ‡∞®‡±Å‡∞µ‡±ç‡∞µ‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞µ‡±Å?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "result=output_parser.invoke(response)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b86be20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"‡∞π‡±Ü‡∞≤‡±ã, ‡∞®‡±Ä‡∞µ‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞µ‡±Å?\" (‡∞∏‡∞æ‡∞ß‡∞æ‡∞∞‡∞£ ‡∞∏‡∞Ç‡∞≠‡∞æ‡∞∑‡∞£‡∞≤‡±ã ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞∞‡±Å)\\n\\n‡∞≤‡±á‡∞¶‡∞æ \\n\\n\"‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç, ‡∞®‡±Ä‡∞µ‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞µ‡±Å?\" (‡∞™‡±ç‡∞∞‡∞§‡±ç‡∞Ø‡±á‡∞ï ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞æ‡∞≤‡∞≤‡±ã ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞∞‡±Å)\\n\\n‡∞µ‡∞ø‡∞µ‡∞∞‡∞£: \\n\\n- ‡∞π‡±Ü‡∞≤‡±ã: ‡∞á‡∞¶‡∞ø ‡∞∏‡∞æ‡∞ß‡∞æ‡∞∞‡∞£ ‡∞µ‡∞ø‡∞®‡∞Ø‡∞Ç‡∞§‡±ã ‡∞Æ‡±Å‡∞ó‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡±á ‡∞Ö‡∞µ‡∞æ‡∞Ç‡∞õ‡∞ø‡∞§ ‡∞™‡∞¶‡∞Ç. ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å‡∞≤‡±ã ‡∞¶‡±Ä‡∞®‡∞ø‡∞®‡∞ø ‡∞Ö‡∞®‡±Å‡∞µ‡∞¶‡∞ø‡∞Ç‡∞ö‡∞ø‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞Ö‡∞ß‡∞ø‡∞ï‡∞æ‡∞∞‡∞ø‡∞ï‡∞Ç‡∞ó‡∞æ ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡±á ‡∞™‡∞¶‡∞Ç \"‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç\" ‡∞≤‡±á‡∞¶‡∞æ \"‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø\" ‡∞Ö‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞µ‡∞ö‡±ç‡∞ö‡±Å. \\n\\n- ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞µ‡±Å: ‡∞¶‡±Ä‡∞®‡∞ø‡∞®‡∞ø ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å‡∞≤‡±ã \"‡∞®‡±Ä‡∞µ‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞µ‡±Å?\" ‡∞Ö‡∞®‡∞ø ‡∞Ö‡∞®‡±Å‡∞µ‡∞¶‡∞ø‡∞Ç‡∞ö‡∞µ‡∞ö‡±ç‡∞ö‡±Å.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=model|output_parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5e353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='translate the follwoning into this Telugu', additional_kwargs={}, response_metadata={}), HumanMessage(content='who the hell are you?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "generic_template=\"translate the follwoning into this {language}\"\n",
    "prompt=ChatPromptTemplate([(\"system\",generic_template),('user','{input}')])\n",
    "response2=prompt.invoke({'language':'Telugu','input':'who the hell are you?'})\n",
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43fb337a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='translate the follwoning into this Telugu', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='who the hell are you?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "160a6ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∞®‡±Ä‡∞µ‡±Å ‡∞é‡∞µ‡∞∞‡±ã ‡∞®‡∞æ‡∞ï‡±Å ‡∞§‡±Ü‡∞≤‡∞ø‡∞Ø‡∞¶‡±Å.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "chain.invoke({'language':'Telugu','input':'who the hell are you?',})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f0af7",
   "metadata": {},
   "source": [
    "Alright, deep breath üòÑ\n",
    "You‚Äôre asking *exactly* the right questions. Let‚Äôs do this **from scratch ‚Üí intuition ‚Üí code ‚Üí ‚Äúaha‚Äù level**.\n",
    "\n",
    "I‚Äôll structure this cleanly so you can build a **mental model**, not just memorize APIs.\n",
    "\n",
    "---\n",
    "\n",
    "# 0Ô∏è‚É£ First: the BIG picture (before LCEL)\n",
    "\n",
    "Think of a **GenAI app** as a pipeline:\n",
    "\n",
    "```\n",
    "User Question\n",
    "   ‚Üì\n",
    "(Some logic)\n",
    "   ‚Üì\n",
    "LLM\n",
    "   ‚Üì\n",
    "Answer\n",
    "```\n",
    "\n",
    "LangChain‚Äôs job is **not magic**.\n",
    "It just helps you **organize this pipeline cleanly**.\n",
    "\n",
    "LCEL (LangChain Expression Language) is the **new, clean way** to define that pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ What is LCEL (in one sentence)\n",
    "\n",
    "> **LCEL is a way to connect components like Lego blocks using `|` (pipe) so data flows step-by-step.**\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "This literally means:\n",
    "\n",
    "1. Prompt formats text\n",
    "2. LLM generates output\n",
    "3. Parser converts it into usable form\n",
    "\n",
    "That‚Äôs it. No hidden magic.\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Why LCEL exists (why LangChain changed)\n",
    "\n",
    "### Old LangChain (pre-LCEL)\n",
    "\n",
    "* Many special classes: `LLMChain`, `RetrievalQA`, etc.\n",
    "* Hard to customize\n",
    "* Hard to debug\n",
    "* Felt ‚Äúblack-boxy‚Äù\n",
    "\n",
    "### New LangChain (LCEL)\n",
    "\n",
    "* Everything is **explicit**\n",
    "* Every step is visible\n",
    "* Easy to swap pieces\n",
    "* Works like Unix pipes / ML pipelines\n",
    "\n",
    "üëâ **Industry reason**:\n",
    "LangChain wanted something **composable, debuggable, production-friendly**\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Core idea of LCEL: ‚ÄúRunnable‚Äù\n",
    "\n",
    "Every component is a **Runnable**.\n",
    "\n",
    "A Runnable:\n",
    "\n",
    "* Takes input\n",
    "* Produces output\n",
    "\n",
    "Examples of Runnables:\n",
    "\n",
    "* `ChatPromptTemplate`\n",
    "* `Ollama`\n",
    "* `StrOutputParser`\n",
    "* `Retriever`\n",
    "* Even Python functions\n",
    "\n",
    "And LCEL connects them.\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ The `|` operator (MOST IMPORTANT)\n",
    "\n",
    "This is not Python magic.\n",
    "It‚Äôs **function composition**.\n",
    "\n",
    "```python\n",
    "A | B\n",
    "```\n",
    "\n",
    "Means:\n",
    "\n",
    "```text\n",
    "output_of_A ‚Üí input_of_B\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "```python\n",
    "prompt | llm\n",
    "```\n",
    "\n",
    "Means:\n",
    "\n",
    "```\n",
    "Prompt produces text ‚Üí LLM consumes text\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ ChatPromptTemplate (from zero)\n",
    "\n",
    "### What problem does it solve?\n",
    "\n",
    "LLMs need **structured text**, not Python variables.\n",
    "\n",
    "You want:\n",
    "\n",
    "```python\n",
    "question = \"What is AI?\"\n",
    "```\n",
    "\n",
    "But the LLM needs:\n",
    "\n",
    "```\n",
    "System: You are helpful\n",
    "User: What is AI?\n",
    "```\n",
    "\n",
    "### ChatPromptTemplate = TEXT FACTORY üè≠\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Question: {question}\")\n",
    "])\n",
    "```\n",
    "\n",
    "### What happens internally?\n",
    "\n",
    "Input:\n",
    "\n",
    "```python\n",
    "{\"question\": \"What is AI?\"}\n",
    "```\n",
    "\n",
    "Output (plain text message sequence):\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant\n",
    "User: Question: What is AI?\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **Important**:\n",
    "PromptTemplates **do NOT call the LLM**.\n",
    "They ONLY format text.\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ LLM (Ollama, OpenAI, Gemini, etc.)\n",
    "\n",
    "```python\n",
    "llm = Ollama(model=\"gemma:2b\")\n",
    "```\n",
    "\n",
    "LLM:\n",
    "\n",
    "* Takes **text**\n",
    "* Produces **text**\n",
    "\n",
    "It does **not** know:\n",
    "\n",
    "* Python\n",
    "* FAISS\n",
    "* Embeddings\n",
    "* Vector DB\n",
    "* LangChain\n",
    "\n",
    "Only text. Always.\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Output Parser (why it exists)\n",
    "\n",
    "LLM output is messy text.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Sure! Here's the answer:\n",
    "\n",
    "AI is...\n",
    "```\n",
    "\n",
    "But your app might want:\n",
    "\n",
    "* string\n",
    "* JSON\n",
    "* list\n",
    "* number\n",
    "\n",
    "```python\n",
    "output_parser = StrOutputParser()\n",
    "```\n",
    "\n",
    "This just says:\n",
    "\n",
    "> ‚ÄúGive me the raw string, nothing fancy‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Your chain explained line-by-line\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "This is LCEL.\n",
    "\n",
    "Flow:\n",
    "\n",
    "```\n",
    "input dict\n",
    "  ‚Üì\n",
    "PromptTemplate ‚Üí text\n",
    "  ‚Üì\n",
    "LLM ‚Üí generated text\n",
    "  ‚Üì\n",
    "Parser ‚Üí clean string\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "chain.invoke({\"question\": input_text})\n",
    "```\n",
    "\n",
    "Equivalent to:\n",
    "\n",
    "```python\n",
    "formatted_prompt = prompt.invoke(...)\n",
    "llm_output = llm.invoke(formatted_prompt)\n",
    "final_output = output_parser.invoke(llm_output)\n",
    "```\n",
    "\n",
    "LCEL just removes boilerplate.\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ Why NOT hardcode context?\n",
    "\n",
    "You asked this brilliant question üëá\n",
    "\n",
    "> ‚ÄúDo we hardcode context every time?‚Äù\n",
    "\n",
    "### ‚ùå Hardcoding (bad)\n",
    "\n",
    "```python\n",
    "context = \"This is about AI...\"\n",
    "```\n",
    "\n",
    "This is dumb and static.\n",
    "\n",
    "### ‚úÖ Intelligent way (Retriever)\n",
    "\n",
    "Context is:\n",
    "\n",
    "* Automatically selected\n",
    "* Based on similarity\n",
    "* Based on the question\n",
    "\n",
    "Retriever decides **what context to inject**, not you.\n",
    "\n",
    "---\n",
    "\n",
    "# üîü Retriever (INTUITIVE explanation)\n",
    "\n",
    "### Think Google Search üß†\n",
    "\n",
    "When you search:\n",
    "\n",
    "> ‚ÄúPython decorators‚Äù\n",
    "\n",
    "Google does:\n",
    "\n",
    "1. Convert query ‚Üí representation\n",
    "2. Match against billions of pages\n",
    "3. Return top relevant ones\n",
    "\n",
    "Retriever = mini Google for your documents.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Why vectors if LLM only sees text?\n",
    "\n",
    "Your confusion here is **100% valid**:\n",
    "\n",
    "> ‚ÄúWhy embeddings if LLM only sees text?‚Äù\n",
    "\n",
    "### Key idea:\n",
    "\n",
    "**Embeddings are for SEARCH, not for GENERATION**\n",
    "\n",
    "Two separate brains:\n",
    "\n",
    "| Component | Purpose            |\n",
    "| --------- | ------------------ |\n",
    "| Vector DB | Find relevant docs |\n",
    "| LLM       | Read and reason    |\n",
    "\n",
    "Flow:\n",
    "\n",
    "```\n",
    "User Question\n",
    "   ‚Üì\n",
    "Embeddings (search)\n",
    "   ‚Üì\n",
    "Relevant documents (TEXT)\n",
    "   ‚Üì\n",
    "LLM reads text\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "* Vectors ‚Üí help **you**\n",
    "* Context text ‚Üí helps **LLM**\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Retrieval Chain vs Document Chain\n",
    "\n",
    "### Document Chain\n",
    "\n",
    "* You **already have documents**\n",
    "* No searching\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "```\n",
    "\n",
    "Used when:\n",
    "\n",
    "* Docs are fixed\n",
    "* Small\n",
    "* Already known\n",
    "\n",
    "### Retrieval Chain\n",
    "\n",
    "* Finds documents dynamically\n",
    "* Injects into prompt\n",
    "\n",
    "```python\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "```\n",
    "\n",
    "Used when:\n",
    "\n",
    "* Large corpus\n",
    "* Question-dependent context\n",
    "* Real RAG systems\n",
    "\n",
    "üëâ **Yes**, you *can* use document chains alone\n",
    "üëâ **No**, they are not intelligent by themselves\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Where LCEL is used (real world)\n",
    "\n",
    "LCEL is used for:\n",
    "\n",
    "* RAG pipelines\n",
    "* Agents\n",
    "* Tool calling\n",
    "* Multi-step reasoning\n",
    "* Streaming responses\n",
    "* Evaluation pipelines\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "retriever | prompt | llm | parser\n",
    "```\n",
    "\n",
    "Or:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"context\": retriever,\n",
    "  \"question\": RunnablePassthrough()\n",
    "} | prompt | llm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ Is LangChain the only one?\n",
    "\n",
    "‚ùå No.\n",
    "\n",
    "Others:\n",
    "\n",
    "* **LlamaIndex** (strong in RAG)\n",
    "* **DSPy** (prompt programming)\n",
    "* **Haystack**\n",
    "* **Semantic Kernel**\n",
    "* **Google ADK / Vertex AI**\n",
    "\n",
    "### Why LangChain is popular\n",
    "\n",
    "* LCEL is elegant\n",
    "* Works with many providers\n",
    "* Great ecosystem\n",
    "* Rapid prototyping\n",
    "* Agent support\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ Relating to Google SDK (you asked this!)\n",
    "\n",
    "| Google SDK         | LangChain      |\n",
    "| ------------------ | -------------- |\n",
    "| System instruction | system message |\n",
    "| User prompt        | user message   |\n",
    "| Context window     | `{context}`    |\n",
    "| Tool calling       | Tools          |\n",
    "| Pipelines          | LCEL chains    |\n",
    "\n",
    "LangChain just makes it **framework-agnostic**.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£ Final mental model (ZERO ‚Üí HERO)\n",
    "\n",
    "üî• Memorize this:\n",
    "\n",
    "```\n",
    "Retriever ‚Üí finds text\n",
    "Prompt ‚Üí arranges text\n",
    "LLM ‚Üí reasons on text\n",
    "Parser ‚Üí cleans output\n",
    "LCEL ‚Üí connects everything\n",
    "```\n",
    "\n",
    "If you understand this, **you understand LangChain**.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next we can:\n",
    "\n",
    "* Build **RAG step-by-step**\n",
    "* Rewrite your code using **retriever**\n",
    "* Draw **ASCII diagrams**\n",
    "* Compare **LangChain vs LlamaIndex**\n",
    "* Fix your Ollama error cleanly\n",
    "\n",
    "Just tell me where you want to go next üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8bc52",
   "metadata": {},
   "source": [
    "#### Code Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc675d",
   "metadata": {},
   "source": [
    "Great question ‚Äî this is actually **core LangChain architecture** stuff. Let‚Äôs break it cleanly and conceptually üëá\n",
    "(No fluff, no confusion, just structure + meaning)\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ First Code (Message-based invocation)\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='Translate the following English sentence to Telugu'),\n",
    "    HumanMessage(content='hello, How are you?')\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "```\n",
    "\n",
    "## üß† What is happening?\n",
    "\n",
    "You are **manually constructing a chat conversation** using message objects.\n",
    "\n",
    "### Message types:\n",
    "\n",
    "* `SystemMessage` ‚Üí instruction / role / behavior control\n",
    "* `HumanMessage` ‚Üí user input\n",
    "* (`AIMessage` ‚Üí model response, returned by model)\n",
    "\n",
    "So this:\n",
    "\n",
    "```python\n",
    "SystemMessage(content='Translate the following English sentence to Telugu')\n",
    "```\n",
    "\n",
    "= system prompt (rules / instruction to model)\n",
    "\n",
    "```python\n",
    "HumanMessage(content='hello, How are you?')\n",
    "```\n",
    "\n",
    "= user message\n",
    "\n",
    "You are sending a **chat history** to the model.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Output parsing\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "result = output_parser.invoke(response)\n",
    "```\n",
    "\n",
    "`response` is an **AIMessage object**, not a string.\n",
    "So parser extracts:\n",
    "\n",
    "```text\n",
    "AIMessage ‚Üí string content\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚õì Chain form\n",
    "\n",
    "```python\n",
    "chain = model | output_parser\n",
    "chain.invoke(messages)\n",
    "```\n",
    "\n",
    "Pipeline:\n",
    "\n",
    "```\n",
    "messages ‚Üí model ‚Üí AIMessage ‚Üí parser ‚Üí string\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ Second Code (PromptTemplate-based invocation)\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "generic_template = \"translate the follwoning into this {language}\"\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", generic_template),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "```\n",
    "\n",
    "Here you are **not manually creating messages**.\n",
    "You're creating a **prompt factory**.\n",
    "\n",
    "This is a **template**, not a message yet.\n",
    "\n",
    "---\n",
    "\n",
    "## When you invoke:\n",
    "\n",
    "```python\n",
    "response2 = prompt.invoke({\n",
    "    'language': 'Telugu',\n",
    "    'input': 'who the hell are you?'\n",
    "})\n",
    "```\n",
    "\n",
    "It generates messages automatically:\n",
    "\n",
    "```python\n",
    "[\n",
    "  SystemMessage(\"translate the following into this Telugu\"),\n",
    "  HumanMessage(\"who the hell are you?\")\n",
    "]\n",
    "```\n",
    "\n",
    "So `ChatPromptTemplate` üëâ **auto-creates messages for you**\n",
    "\n",
    "---\n",
    "\n",
    "## This line proves it:\n",
    "\n",
    "```python\n",
    "response2.to_messages()\n",
    "```\n",
    "\n",
    "You will see:\n",
    "\n",
    "```python\n",
    "[SystemMessage(...), HumanMessage(...)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chain:\n",
    "\n",
    "```python\n",
    "chain = prompt | model | output_parser\n",
    "chain.invoke({'language':'Telugu','input':'who the hell are you?'})\n",
    "```\n",
    "\n",
    "Pipeline:\n",
    "\n",
    "```\n",
    "dict ‚Üí prompt ‚Üí messages ‚Üí model ‚Üí AIMessage ‚Üí parser ‚Üí string\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß† CORE DIFFERENCE (Very Important)\n",
    "\n",
    "| Aspect             | First Code   | Second Code     |\n",
    "| ------------------ | ------------ | --------------- |\n",
    "| Message creation   | Manual       | Automatic       |\n",
    "| Abstraction level  | Low-level    | High-level      |\n",
    "| Flexibility        | More control | More structured |\n",
    "| Reusability        | Low          | High            |\n",
    "| Production use     | ‚ùå rarely     | ‚úÖ standard      |\n",
    "| Dynamic inputs     | Hard         | Easy            |\n",
    "| Prompt engineering | Manual       | Template-based  |\n",
    "\n",
    "---\n",
    "\n",
    "# üß¨ Conceptual Difference (Simple words)\n",
    "\n",
    "### First approach:\n",
    "\n",
    "> \"I will manually talk to the model like ChatGPT API\"\n",
    "\n",
    "### Second approach:\n",
    "\n",
    "> \"I will design a reusable prompt system\"\n",
    "\n",
    "---\n",
    "\n",
    "# ‚ùì When do we import `SystemMessage` and `HumanMessage`?\n",
    "\n",
    "### ‚úÖ You import them when:\n",
    "\n",
    "1. You want **manual control**\n",
    "2. You are building **custom chat history**\n",
    "3. You are doing **low-level agent frameworks**\n",
    "4. You want to simulate multi-turn memory\n",
    "5. You want to inject system logic dynamically\n",
    "6. You are doing **LLM research**\n",
    "7. Debugging internal pipelines\n",
    "\n",
    "Example use-case:\n",
    "\n",
    "```python\n",
    "messages = memory + tools + system_rules + user_input\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå You do NOT need them when:\n",
    "\n",
    "Using:\n",
    "\n",
    "```python\n",
    "ChatPromptTemplate\n",
    "```\n",
    "\n",
    "Because it auto-creates messages internally.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Industry Practice\n",
    "\n",
    "### Real-world production code uses:\n",
    "\n",
    "```python\n",
    "ChatPromptTemplate + Chains\n",
    "```\n",
    "\n",
    "NOT manual messages.\n",
    "\n",
    "---\n",
    "\n",
    "# üß© Mental Model\n",
    "\n",
    "Think like this:\n",
    "\n",
    "### Message API = Low-level socket programming\n",
    "\n",
    "### PromptTemplate = High-level HTTP framework\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Interview/Viva Line (important):\n",
    "\n",
    "> \"SystemMessage and HumanMessage are low-level abstractions for manually constructing chat interactions. ChatPromptTemplate is a high-level abstraction that dynamically generates structured messages using templates, making pipelines reusable, scalable, and production-friendly.\"\n",
    "\n",
    "---\n",
    "\n",
    "# üî• One-line summary:\n",
    "\n",
    "**First code = manual message wiring**\n",
    "**Second code = templated prompt architecture**\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next I can explain this visually as a pipeline diagram or with memory + retriever + chain architecture ‚Äî this topic connects directly to agents, RAG, and production LLM systems üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c8f55",
   "metadata": {},
   "source": [
    "Great question ‚Äî you‚Äôre learning **LangChain message-based prompting vs prompt templates**. I‚Äôll explain **line-by-line**, then compare **first vs second code**, and finally explain **when to use `SystemMessage` and `HumanMessage`**.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ PART 1 ‚Äî Explain the **FIRST CODE** (Messages API)\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "```\n",
    "\n",
    "You import message types that define **who is speaking**:\n",
    "\n",
    "* `SystemMessage` = instructions to the model\n",
    "* `HumanMessage` = user input\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    SystemMessage(content='Translate the following English sentence to Telugu'),\n",
    "    HumanMessage(content='hello, How are you?')\n",
    "]\n",
    "```\n",
    "\n",
    "You create a **chat history**:\n",
    "\n",
    "* System tells the **rule**\n",
    "* Human gives the **text to translate**\n",
    "\n",
    "This mimics **real ChatGPT chat roles**.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "response = model.invoke(messages)\n",
    "```\n",
    "\n",
    "You send the messages to the model\n",
    "The model returns an **AIMessage object** (not plain text yet)\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "```\n",
    "\n",
    "This converts the model‚Äôs response into **plain string**\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "result = output_parser.invoke(response)\n",
    "result\n",
    "```\n",
    "\n",
    "Extracts only the text content from the AIMessage\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "chain = model | output_parser\n",
    "chain.invoke(messages)\n",
    "```\n",
    "\n",
    "This creates a **pipeline (chain)**:\n",
    "\n",
    "1. Send messages ‚Üí model\n",
    "2. Parse output ‚Üí string\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ First code uses **structured chat messages**\n",
    "\n",
    "It‚Äôs best when simulating a **real multi-role conversation**\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ PART 2 ‚Äî Explain the **SECOND CODE** (Prompt Templates API)\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "generic_template = \"translate the follwoning into this {language}\"\n",
    "```\n",
    "\n",
    "You define a **dynamic template** with a variable `{language}`\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", generic_template),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "```\n",
    "\n",
    "Instead of manually writing messages, you **define a reusable chat format**\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "response2 = prompt.invoke({\n",
    "    'language': 'Telugu',\n",
    "    'input': 'who the hell are you?'\n",
    "})\n",
    "```\n",
    "\n",
    "This fills the template:\n",
    "\n",
    "* system ‚Üí instruction\n",
    "* user ‚Üí input text\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "response2.to_messages()\n",
    "```\n",
    "\n",
    "Shows the final **SystemMessage + HumanMessage** that will be sent\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "chain = prompt | model | output_parser\n",
    "chain.invoke({'language':'Telugu','input':'who the hell are you?'})\n",
    "```\n",
    "\n",
    "This chain:\n",
    "\n",
    "1. Builds messages from template\n",
    "2. Sends to model\n",
    "3. Extracts text\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ PART 3 ‚Äî **Difference Between FIRST vs SECOND CODE**\n",
    "\n",
    "| Feature           | FIRST CODE    | SECOND CODE                      |\n",
    "| ----------------- | ------------- | -------------------------------- |\n",
    "| Message creation  | **Manual**    | **Automatic via template**       |\n",
    "| Reusability       | ‚ùå Low         | ‚úÖ High                           |\n",
    "| Best for          | One-off chats | Repeated structured prompts      |\n",
    "| Dynamic variables | ‚ùå Hard        | ‚úÖ Easy (`{language}`, `{input}`) |\n",
    "| Scalable          | ‚ùå             | ‚úÖ                                |\n",
    "| Clean code        | ‚ùå More manual | ‚úÖ Cleaner                        |\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Simple Meaning\n",
    "\n",
    "### First code = **Hardcoding messages**\n",
    "\n",
    "> Good for quick tests\n",
    "\n",
    "### Second code = **Reusable AI prompt system**\n",
    "\n",
    "> Better for apps, chatbots, translation tools\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ PART 4 ‚Äî When Do We Need to Import `SystemMessage` and `HumanMessage`?\n",
    "\n",
    "### ‚úÖ Use them when:\n",
    "\n",
    "* You want **manual control** over chat roles\n",
    "* You are **building multi-turn chat history**\n",
    "* You want to **store or replay conversations**\n",
    "* You are **debugging raw LLM messages**\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a strict teacher\"),\n",
    "    HumanMessage(content=\"Explain recursion\")\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå You **DON'T need them** when:\n",
    "\n",
    "* You use **ChatPromptTemplate**\n",
    "* You use **simple string prompts**\n",
    "* You want **clean and scalable prompt pipelines**\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ PART 5 ‚Äî Quick Rule to Remember\n",
    "\n",
    "| Situation              | Use                             |\n",
    "| ---------------------- | ------------------------------- |\n",
    "| Quick test             | `SystemMessage`, `HumanMessage` |\n",
    "| Production apps        | `ChatPromptTemplate`            |\n",
    "| Multi-turn memory chat | Messages                        |\n",
    "| Dynamic prompts        | Templates                       |\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ PART 6 ‚Äî Want a SUPER SIMPLE One-Line Summary?\n",
    "\n",
    "**First code = manual chat**\n",
    "**Second code = reusable prompt engine**\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can:\n",
    "‚úÖ Rewrite both examples in **simpler beginner-friendly form**\n",
    "‚úÖ Show a **real-world use case** of each\n",
    "‚úÖ Explain this like a **college viva answer**\n",
    "‚úÖ Make a **diagram of LangChain flow**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5029f61f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '‚úÖ' (U+2705) (840927284.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m‚úÖ PART 1 ‚Äî Explain the FIRST CODE (Messages API)\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '‚úÖ' (U+2705)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819250d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

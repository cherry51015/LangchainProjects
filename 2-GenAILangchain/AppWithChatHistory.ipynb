{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e303e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ki\n"
     ]
    }
   ],
   "source": [
    "print(\"ki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4527f9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mydhi\\OneDrive\\Desktop\\LangchainProjects\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x00000237B6CF6E10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000237B6D2F950>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## aloading all the environment variable\n",
    "from langchain_groq import ChatGroq\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "model=ChatGroq(model='llama-3.1-8b-instant',groq_api_key=groq_api_key)\n",
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "660c4d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Cherry. As a Chief AI Engineer, that's a fascinating role. I'm sure you have a deep understanding of artificial intelligence and its applications. What specific areas of AI are you most interested in or working on currently?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 48, 'total_tokens': 98, 'completion_time': 0.076267623, 'completion_tokens_details': None, 'prompt_time': 0.002280594, 'prompt_tokens_details': None, 'queue_time': 0.045520816, 'total_time': 0.078548217}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c28f1-ac2e-7e00-8a09-09aa12035382-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 48, 'output_tokens': 50, 'total_tokens': 98})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi , My name is cherry and I am a Chief AI Engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeee92b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You're working on NLP (Natural Language Processing) based systems from the perspective of General AI (GenAI). That's a fascinating area of research and development.\\n\\nGeneral AI, also known as Artificial General Intelligence (AGI), aims to create a machine that can perform any intellectual task that a human can. NLP is a crucial component of AGI, as it enables machines to understand, generate, and interact with human language.\\n\\nIn the context of NLP, what specific challenges are you trying to address, such as:\\n\\n1. Improving language understanding and interpretation?\\n2. Developing more accurate and efficient language models?\\n3. Enhancing the ability of machines to generate human-like language?\\n4. Creating more robust and scalable NLP systems that can handle diverse languages and dialects?\\n\\nAlso, are you using any specific techniques or architectures, such as transformer-based models, attention mechanisms, or graph neural networks, to tackle these challenges?\\n\\n(By the way, I'm curious to know more about your project, so feel free to share as much or as little as you'd like!)\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 218, 'prompt_tokens': 167, 'total_tokens': 385, 'completion_time': 0.304863103, 'completion_tokens_details': None, 'prompt_time': 0.009855947, 'prompt_tokens_details': None, 'queue_time': 0.045073763, 'total_time': 0.31471905}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c28f1-afa4-75f2-a006-9e1efe187f19-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 167, 'output_tokens': 218, 'total_tokens': 385})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,AIMessage,SystemMessage\n",
    "model.invoke([HumanMessage(content=\"Hi , My name is cherry and I am a Chief AI Engineer\"),\n",
    "             AIMessage(content=\"Nice to meet you, Cherry. It's great to have a Chief AI Engineer like you on board. As a Chief AI Engineer, you must be working on some cutting-edge projects that involve the development and implementation of artificial intelligence technologies.\\n\\nWhat specific areas of AI are you currently focusing on, such as natural language processing, computer vision, reinforcement learning, or perhaps explainable AI? Or are you working on a project that involves integrating AI with other technologies like blockchain, IoT, or robotics?\"),\n",
    "             HumanMessage(content=\"I am working on NLP based systems from GenAI\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b03af950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "store={}\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "with_msg_history=RunnableWithMessageHistory(model,get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1bf8008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you again, Cherry! As a Chief AI Engineer, you're likely overseeing a team of AI engineers and developers, working on cutting-edge projects. What sparked your interest in AI and led you to become a Chief AI Engineer?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 167, 'total_tokens': 216, 'completion_time': 0.077942034, 'completion_tokens_details': None, 'prompt_time': 0.010471159, 'prompt_tokens_details': None, 'queue_time': 0.045613718, 'total_time': 0.088413193}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2949-0f73-7700-8a1d-89c7692edcab-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 167, 'output_tokens': 49, 'total_tokens': 216})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response=with_msg_history.invoke([HumanMessage(content=\"Hi , My name is cherry and I am a Chief AI Engineer\")],\n",
    "                                 config=config)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91405499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Cherry, and you are a Chief AI Engineer. As a Chief AI Engineer, your role involves leading and overseeing the development and implementation of artificial intelligence solutions for your organization.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 237, 'total_tokens': 275, 'completion_time': 0.04942677, 'completion_tokens_details': None, 'prompt_time': 0.014844629, 'prompt_tokens_details': None, 'queue_time': 0.04517507, 'total_time': 0.064271399}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2949-1c0d-7242-ba5e-49ead9d419e8-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 237, 'output_tokens': 38, 'total_tokens': 275})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2=with_msg_history.invoke([HumanMessage(content=\"What's my name, and what do i do for living\")],\n",
    "                                  config=config)\n",
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e80bb42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any information about your name. I'm a large language model, I don't have the ability to retain information about individual users or their personal details. Each time you interact with me, it's a new conversation and I don't retain any context from previous conversations. If you'd like to share your name with me, I'd be happy to chat with you!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 39, 'total_tokens': 117, 'completion_time': 0.079156787, 'completion_tokens_details': None, 'prompt_time': 0.002969088, 'prompt_tokens_details': None, 'queue_time': 0.045182088, 'total_time': 0.082125875}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2949-1fae-7bd0-a6e5-b290d44a7c4d-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 39, 'output_tokens': 78, 'total_tokens': 117})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response3=with_msg_history.invoke([HumanMessage(content='Whats my name?')],config=config2)\n",
    "response3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "365bd53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Charani! It's great that you're willing to share your name with me. How can I assist you today? Would you like to chat about a specific topic or ask a question? I'm all ears (or rather, all text)!\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response4=with_msg_history.invoke([HumanMessage(content='my name i charani')],config=config2)\n",
    "response4.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e285be4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Charani.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response5=with_msg_history.invoke([HumanMessage(content='whats my name?')],config=config2)\n",
    "response5.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a150e3",
   "metadata": {},
   "source": [
    "### Working with Prompt Template and Message Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d432c02",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1010707937.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprompt=ChatPromptTemplate.from_documents((\"system\":\"You are a helpful Assisstant, Answer all the questions with the nest of your ability\"),\u001b[39m\n                                                      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_documents((\"system\":\"You are a helpful Assisstant, Answer all the questions with the nest of your ability\"),\n",
    "                                       (MessagesPlaceholder=\"messages\"))\n",
    "chain=prompt|model\n",
    "#here  for the variable of MessagesPlaceholder whatever the value we pass it should in key value pairs \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d07b7376",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res=\u001b[43mchain\u001b[49m.invoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m:[HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mhi my name is cherry\u001b[39m\u001b[33m\"\u001b[39m)]})\n\u001b[32m      2\u001b[39m res\n",
      "\u001b[31mNameError\u001b[39m: name 'chain' is not defined"
     ]
    }
   ],
   "source": [
    "res=chain.invoke({\"messages\":[HumanMessage(content=\"hi my name is cherry\")]})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3982ff13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#ok but how to invoke withrespect to chat history?\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m with_msg_history=RunnableWithMessageHistory(\u001b[43mchain\u001b[49m,get_session_history)\n\u001b[32m      3\u001b[39m config={\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m:{\u001b[33m\"\u001b[39m\u001b[33msession_id\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mchat3\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m      4\u001b[39m response1=with_msg_history.invoke([HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mhi i am cheryy\u001b[39m\u001b[33m\"\u001b[39m)],config=config)\n",
      "\u001b[31mNameError\u001b[39m: name 'chain' is not defined"
     ]
    }
   ],
   "source": [
    "#ok but how to invoke withrespect to chat history?\n",
    "with_msg_history=RunnableWithMessageHistory(chain,get_session_history)\n",
    "config={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response1=with_msg_history.invoke([HumanMessage(content=\"hi i am cheryy\")],config=config)\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "382d265a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Cherry. You mentioned it earlier, you are a Chief AI Engineer.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_result=with_msg_history.invoke([HumanMessage(content='what is my name?')],config=config)\n",
    "response_result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template=ChatPromptTemplate.from_messages([(\"system\":\"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\")],\n",
    "                                        MessagesPlaceholder=messages)\n",
    "chain=template|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad650088",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"hi my name is john\")],\"language\":\"telugu\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16523371",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_msg_history=RunnableWithMessageHistory(chain,get_session_history,....)\n",
    "config={\"configurable\":{\"session_id\":\"chat9\"}}\n",
    "res=with_msg_history.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Krish\")],\"language\":\"Hindi\"},\n",
    "config=config)\n",
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd1b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=with_msg_history.invoke({\"messages\":[HumanMessage(content=\"whats my name?\")],\"language\":\"Hindi\"},\n",
    "config=config)\n",
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9921da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mydhi\\OneDrive\\Desktop\\Langchain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MessagesPlaceholder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[32m      2\u001b[39m prompt=ChatPromptTemplate.from_messages([(\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33myou are a helpful assisstant,.Answer all the question to the  nest of your ability \u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mMessagesPlaceholder\u001b[49m(variable_name=\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m)])\n\u001b[32m      5\u001b[39m chain=prompt|model\n\u001b[32m      6\u001b[39m chain.invoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m:HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mhi my name is krish\u001b[39m\u001b[33m\"\u001b[39m)})\n",
      "\u001b[31mNameError\u001b[39m: name 'MessagesPlaceholder' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_messages([(\n",
    "    \"system\",\"you are a helpful assisstant,.Answer all the question to the  nest of your ability \"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")])\n",
    "chain=prompt|model\n",
    "chain.invoke({\"messages\":HumanMessage(content=\"hi my name is krish\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa73237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa2a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0a37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9fc805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcec5bb2",
   "metadata": {},
   "source": [
    "Perfect ‚Äî this is the **core memory block** of a LangChain chatbot. I‚Äôll explain it **slowly, cleanly, and structurally**, exactly like notes you‚Äôd revise before an exam or interview.\n",
    "\n",
    "I‚Äôll break it into **5 clearly separated parts**.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ `ChatMessageHistory`\n",
    "\n",
    "```python\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "```\n",
    "\n",
    "## What it is\n",
    "\n",
    "A **container** that stores chat messages **in order**.\n",
    "\n",
    "Internally it stores:\n",
    "\n",
    "```text\n",
    "[SystemMessage, HumanMessage, AIMessage, ...]\n",
    "```\n",
    "\n",
    "## What it does\n",
    "\n",
    "* Appends user messages\n",
    "* Appends AI responses\n",
    "* Preserves order\n",
    "\n",
    "## What it does NOT do\n",
    "\n",
    "‚ùå Call the model\n",
    "‚ùå Generate responses\n",
    "\n",
    "## Type of storage\n",
    "\n",
    "* In-memory (Python list)\n",
    "* Lost when program stops\n",
    "\n",
    "## When to use\n",
    "\n",
    "‚úî Demos\n",
    "‚úî Learning\n",
    "‚úî Small apps\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ `BaseChatMessageHistory`\n",
    "\n",
    "```python\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "```\n",
    "\n",
    "## What it is\n",
    "\n",
    "An **abstract base class (interface)**.\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "> ‚ÄúAnything that behaves like chat history must follow this structure‚Äù\n",
    "\n",
    "## Why it exists\n",
    "\n",
    "LangChain supports **multiple backends**:\n",
    "\n",
    "* In-memory\n",
    "* Redis\n",
    "* SQL\n",
    "* MongoDB\n",
    "\n",
    "All of them must look the same to LangChain.\n",
    "\n",
    "## Why your function returns this type\n",
    "\n",
    "```python\n",
    "def get_session_history(...) -> BaseChatMessageHistory:\n",
    "```\n",
    "\n",
    "This tells LangChain:\n",
    "\n",
    "> ‚ÄúI promise this function returns a valid chat history object‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ `store = {}`\n",
    "\n",
    "```python\n",
    "store = {}\n",
    "```\n",
    "\n",
    "## What this is\n",
    "\n",
    "A **session ‚Üí memory map**\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"chat1\": ChatMessageHistory([...]),\n",
    "  \"chat2\": ChatMessageHistory([...])\n",
    "}\n",
    "```\n",
    "\n",
    "## Why it‚Äôs needed\n",
    "\n",
    "* Multiple users\n",
    "* Multiple conversations\n",
    "* Each session needs **separate memory**\n",
    "\n",
    "## Real-world equivalent\n",
    "\n",
    "| App      | Meaning          |\n",
    "| -------- | ---------------- |\n",
    "| WhatsApp | One chat thread  |\n",
    "| ChatGPT  | One conversation |\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ `get_session_history()` function\n",
    "\n",
    "```python\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "```\n",
    "\n",
    "## This is the MOST IMPORTANT part\n",
    "\n",
    "### What LangChain expects\n",
    "\n",
    "A function that:\n",
    "\n",
    "* Takes `session_id`\n",
    "* Returns a **chat history object**\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-step execution\n",
    "\n",
    "### Case 1: First time user\n",
    "\n",
    "```python\n",
    "session_id = \"chat1\"\n",
    "```\n",
    "\n",
    "* `chat1` not in store\n",
    "* Create new `ChatMessageHistory`\n",
    "* Save it\n",
    "* Return it\n",
    "\n",
    "---\n",
    "\n",
    "### Case 2: Existing user\n",
    "\n",
    "* `chat1` already exists\n",
    "* Return existing history\n",
    "* Continue conversation\n",
    "\n",
    "---\n",
    "\n",
    "## Why LangChain doesn‚Äôt store memory itself\n",
    "\n",
    "Because **YOU decide**:\n",
    "\n",
    "* Where memory lives\n",
    "* How long it lives\n",
    "* How it‚Äôs stored\n",
    "\n",
    "---\n",
    "\n",
    "## Real-life analogy\n",
    "\n",
    "> ‚ÄúGiven a user ID, return their chat notebook‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ `RunnableWithMessageHistory`\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "```\n",
    "\n",
    "## What it is\n",
    "\n",
    "A **wrapper** that makes a runnable **stateful**.\n",
    "\n",
    "By default:\n",
    "\n",
    "* Models are stateless\n",
    "* Prompts are stateless\n",
    "\n",
    "This adds **memory**.\n",
    "\n",
    "---\n",
    "\n",
    "## Your line explained\n",
    "\n",
    "```python\n",
    "with_msg_history = RunnableWithMessageHistory(\n",
    "    model,\n",
    "    get_session_history\n",
    ")\n",
    "```\n",
    "\n",
    "### Meaning in English\n",
    "\n",
    "> ‚ÄúWrap this model so that it remembers conversations, using `get_session_history` to store them.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## What happens internally (VERY IMPORTANT)\n",
    "\n",
    "When you call:\n",
    "\n",
    "```python\n",
    "with_msg_history.invoke(\n",
    "    [HumanMessage(\"Hi\")],\n",
    "    config={\"configurable\": {\"session_id\": \"chat1\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "LangChain does:\n",
    "\n",
    "1. Read `session_id`\n",
    "2. Call `get_session_history(\"chat1\")`\n",
    "3. Load old messages\n",
    "4. Append new HumanMessage\n",
    "5. Call the model\n",
    "6. Append AIMessage\n",
    "7. Save updated history\n",
    "\n",
    "---\n",
    "\n",
    "## Why `config` is required\n",
    "\n",
    "```python\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}\n",
    "```\n",
    "\n",
    "Without session ID:\n",
    "\n",
    "* LangChain doesn‚Äôt know **whose memory to load**\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Proper Syntax Summary (memorize this)\n",
    "\n",
    "### Memory store\n",
    "\n",
    "```python\n",
    "store = {}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### History getter\n",
    "\n",
    "```python\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Wrap model or chain\n",
    "\n",
    "```python\n",
    "with_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Invoke\n",
    "\n",
    "```python\n",
    "with_history.invoke(\n",
    "    [HumanMessage(content=\"Hello\")],\n",
    "    config={\"configurable\": {\"session_id\": \"chat1\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ When to write this block (IMPORTANT)\n",
    "\n",
    "| Situation           | Need this? |\n",
    "| ------------------- | ---------- |\n",
    "| One-shot LLM call   | ‚ùå          |\n",
    "| Chatbot             | ‚úÖ          |\n",
    "| Multi-user app      | ‚úÖ          |\n",
    "| Conversation memory | ‚úÖ          |\n",
    "| RAG chatbot         | ‚úÖ          |\n",
    "| Agents              | ‚úÖ          |\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ One-line intuition (lock this in)\n",
    "\n",
    "> **ChatMessageHistory** = memory notebook\n",
    "> **store** = cupboard of notebooks\n",
    "> **get_session_history** = librarian\n",
    "> **RunnableWithMessageHistory** = brain that remembers\n",
    "\n",
    "---\n",
    "\n",
    "If you want next:\n",
    "\n",
    "* üîÅ How this works with **PromptTemplate**\n",
    "* üß™ Minimal chatbot code (15 lines)\n",
    "* üéì Interview-style explanation\n",
    "* üß† Diagram visualization\n",
    "\n",
    "Just say üëç\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c94093",
   "metadata": {},
   "source": [
    "Great catch ‚Äî this is a **very important but hidden concept** in LangChain. Let‚Äôs clear it **properly and intuitively**.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚ùì What is `configurable`?\n",
    "\n",
    "### Short answer\n",
    "\n",
    "`configurable` is a **special config namespace** that LangChain uses to pass **runtime-only parameters** to a runnable.\n",
    "\n",
    "These parameters:\n",
    "\n",
    "* Are **not part of the model input**\n",
    "* Are **not sent to the LLM**\n",
    "* Control **how the runnable behaves**\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Big picture first\n",
    "\n",
    "When you call:\n",
    "\n",
    "```python\n",
    "with_message_history.invoke(input, config=config)\n",
    "```\n",
    "\n",
    "There are **two separate channels**:\n",
    "\n",
    "| Channel  | Purpose               |\n",
    "| -------- | --------------------- |\n",
    "| `input`  | Goes to the model     |\n",
    "| `config` | Controls the runnable |\n",
    "\n",
    "Inside `config`, LangChain reserves:\n",
    "\n",
    "```python\n",
    "config[\"configurable\"]\n",
    "```\n",
    "\n",
    "This is where **runtime knobs** go.\n",
    "\n",
    "---\n",
    "\n",
    "# üì¶ Structure of `config`\n",
    "\n",
    "```python\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": \"chat2\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Meaning in English\n",
    "\n",
    "> ‚ÄúAt runtime, configure this chain using session_id = chat2.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "# üîë Why is it called `configurable`?\n",
    "\n",
    "Because LangChain runnables expose **configurable fields**.\n",
    "\n",
    "`RunnableWithMessageHistory` exposes one configurable field:\n",
    "\n",
    "```text\n",
    "session_id\n",
    "```\n",
    "\n",
    "So LangChain expects:\n",
    "\n",
    "```python\n",
    "configurable.session_id\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîç Why not pass `session_id` directly?\n",
    "\n",
    "‚ùå Wrong:\n",
    "\n",
    "```python\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(\"Hi\")],\n",
    "    session_id=\"chat2\"   # ‚ùå\n",
    ")\n",
    "```\n",
    "\n",
    "‚ùå Wrong:\n",
    "\n",
    "```python\n",
    "with_message_history.invoke(\n",
    "    {\"messages\": [...], \"session_id\": \"chat2\"}  # ‚ùå\n",
    ")\n",
    "```\n",
    "\n",
    "Because:\n",
    "\n",
    "* `session_id` is **not model input**\n",
    "* It‚Äôs **control metadata**\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Mental model (IMPORTANT)\n",
    "\n",
    "Think of a function call like this:\n",
    "\n",
    "```python\n",
    "run(input, config)\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "* `input` ‚Üí what the model sees\n",
    "* `config` ‚Üí how the system runs it\n",
    "\n",
    "---\n",
    "\n",
    "# üîÅ What happens internally\n",
    "\n",
    "When LangChain sees:\n",
    "\n",
    "```python\n",
    "config = {\"configurable\": {\"session_id\": \"chat2\"}}\n",
    "```\n",
    "\n",
    "It does:\n",
    "\n",
    "1. Reads `session_id`\n",
    "2. Calls:\n",
    "\n",
    "   ```python\n",
    "   get_session_history(\"chat2\")\n",
    "   ```\n",
    "3. Loads memory for `chat2`\n",
    "4. Uses it for this invocation\n",
    "\n",
    "---\n",
    "\n",
    "# üß™ Example without configurable\n",
    "\n",
    "```python\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(\"What's my name?\")]\n",
    ")\n",
    "```\n",
    "\n",
    "‚ùå Error or no memory\n",
    "Because LangChain doesn‚Äôt know **which session**\n",
    "\n",
    "---\n",
    "\n",
    "# üß© Analogy (lock this in)\n",
    "\n",
    "### Imagine a printer:\n",
    "\n",
    "```python\n",
    "print(document, config)\n",
    "```\n",
    "\n",
    "* `document` = what to print\n",
    "* `config` = printer settings\n",
    "\n",
    "```python\n",
    "config = {\n",
    "  \"paper_size\": \"A4\",\n",
    "  \"color\": \"black\"\n",
    "}\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "* `document` ‚â† `paper_size`\n",
    "* Same logic in LangChain\n",
    "\n",
    "---\n",
    "\n",
    "# üìå When do YOU use `configurable`?\n",
    "\n",
    "You use it when:\n",
    "\n",
    "* A runnable needs **runtime parameters**\n",
    "* Those parameters are **not LLM inputs**\n",
    "\n",
    "Examples:\n",
    "\n",
    "* `session_id`\n",
    "* tracing options\n",
    "* streaming configs\n",
    "* callbacks\n",
    "\n",
    "---\n",
    "\n",
    "# üß† One-line definition (EXAM READY)\n",
    "\n",
    "> **`configurable` is a reserved config namespace used by LangChain to pass runtime control parameters (like `session_id`) to runnables, without sending them to the LLM.**\n",
    "\n",
    "---\n",
    "\n",
    "If you want next:\n",
    "\n",
    "* üîç What other keys exist in `config`\n",
    "* üß™ How `configurable` differs from input keys\n",
    "* üß† Diagram of invoke flow\n",
    "* üéì Interview-style Q&A\n",
    "\n",
    "Just tell me üëç\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585aa1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LangChain)",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc42095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39312665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mydhi\\OneDrive\\Desktop\\Langchain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangSmith docs have moved! Find the LangSmith docs at the new LangChain Docs site.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppPage Not FoundWe could not find what you were looking for.Head back to our main docs page or use the search bar to find the page you need.CommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2026 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/tutorials/Administrators/manage_spend\")\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e365d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='Skip to main contentLangSmith docs have moved! Find the LangSmith docs at the new LangChain Docs'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='new LangChain Docs site.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppPage Not FoundWe could'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='Not FoundWe could not find what you were looking for.Head back to our main docs page or use the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='page or use the search bar to find the page you need.CommunityLangChain ForumTwitterGitHubDocs'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬©'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='DocsCopyright ¬© 2026 LangChain, Inc.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2853597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mydhi\\AppData\\Local\\Temp\\ipykernel_32908\\3539956551.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings=OllamaEmbeddings(model='llama3.2:1b')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x23d51a8a9c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings=OllamaEmbeddings(model='llama3.2:1b')\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db=FAISS.from_documents(documents,embeddings)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "819a4ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='e1f9a216-394a-4d96-b09d-3516831afecc', metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='new LangChain Docs site.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppPage Not FoundWe could'),\n",
       " Document(id='4590f810-e4f6-42ea-b902-dd9222a038b4', metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(id='7831f49b-f404-42df-a791-00990e9a2299', metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='Not FoundWe could not find what you were looking for.Head back to our main docs page or use the'),\n",
       " Document(id='a7509bfb-afb6-4a60-ab19-ceceb5540e49', metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='Skip to main contentLangSmith docs have moved! Find the LangSmith docs at the new LangChain Docs')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"LangSmith has two usage limits: total traces and extended\"\n",
    "res1=db.similarity_search(query)\n",
    "res1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b8800",
   "metadata": {},
   "source": [
    "## Query ‚Üí Retriever ‚Üí Relevant Docs ‚Üí Context ‚Üí LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db93b26",
   "metadata": {},
   "source": [
    "Perfect üëç\n",
    "Now we‚Äôll do this **line-by-line, component-by-component**, like opening a machine and naming every screw.\n",
    "I‚Äôll explain **what the code is**, **what problem it solves**, and **why it exists** ‚Äî not just ‚Äúwhat it does‚Äù.\n",
    "\n",
    "I‚Äôll follow the **exact flow you used**, from prompt ‚Üí document chain ‚Üí retriever ‚Üí retrieval chain ‚Üí invoke.\n",
    "\n",
    "---\n",
    "\n",
    "# 0Ô∏è‚É£ What problem this entire code is solving (anchor this first)\n",
    "\n",
    "> ‚ÄúGiven a user query, automatically find relevant documents and let the LLM answer **only using those documents**.‚Äù\n",
    "\n",
    "Everything in the code exists to serve **one step** of this pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ PromptTemplate\n",
    "\n",
    "```python\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "```\n",
    "\n",
    "### Why these imports?\n",
    "\n",
    "* `ChatPromptTemplate` ‚Üí structured prompt builder\n",
    "* `create_stuff_documents_chain` ‚Üí logic to inject documents into prompt\n",
    "\n",
    "LangChain separates:\n",
    "\n",
    "* **prompt construction**\n",
    "* **document handling**\n",
    "* **LLM calls**\n",
    "\n",
    "---\n",
    "\n",
    "## Prompt definition\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "### What this REALLY is\n",
    "\n",
    "This is a **prompt blueprint**, not a real prompt yet.\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "> ‚ÄúWhenever I give you some text called `context`, put it here.‚Äù\n",
    "\n",
    "### Why `{context}` is mandatory\n",
    "\n",
    "* This is where **retrieved documents will be injected**\n",
    "* LLM does NOT know where documents go unless you specify it\n",
    "\n",
    "### What `{context}` will become later\n",
    "\n",
    "```text\n",
    "<context>\n",
    "Document 1 text\n",
    "Document 2 text\n",
    "Document 3 text\n",
    "</context>\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è Right now `{context}` is **empty** ‚Äî this is just a template.\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Document Chain (LLM + Documents)\n",
    "\n",
    "```python\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "```\n",
    "\n",
    "### What problem does this solve?\n",
    "\n",
    "> ‚ÄúI have documents. I want an LLM answer from them.‚Äù\n",
    "\n",
    "### What ‚Äústuff‚Äù means\n",
    "\n",
    "* All retrieved documents are **stuffed into one prompt**\n",
    "* No summarizing\n",
    "* No splitting across calls\n",
    "\n",
    "### Internally this does:\n",
    "\n",
    "```text\n",
    "[List[Document]]\n",
    "   ‚Üì\n",
    "Extract page_content\n",
    "   ‚Üì\n",
    "Join as text\n",
    "   ‚Üì\n",
    "Replace {context}\n",
    "   ‚Üì\n",
    "Send to LLM\n",
    "```\n",
    "\n",
    "### Input it expects\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"context\": List[Document]\n",
    "}\n",
    "```\n",
    "\n",
    "### Output it gives\n",
    "\n",
    "```python\n",
    "str  # LLM answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## When you ran:\n",
    "\n",
    "```python\n",
    "document_chain.invoke({\n",
    "    \"input\": \"...\",\n",
    "    \"context\": [Document(...)]\n",
    "})\n",
    "```\n",
    "\n",
    "You **manually supplied** documents.\n",
    "\n",
    "That‚Äôs why it worked even before retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Why document_chain alone is NOT enough\n",
    "\n",
    "Because:\n",
    "\n",
    "‚ùå You must manually decide which documents to pass\n",
    "‚ùå No automatic search\n",
    "‚ùå No scalability\n",
    "\n",
    "So we introduce a retriever.\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Retriever\n",
    "\n",
    "```python\n",
    "retriever = vectorstoredb.as_retriever()\n",
    "```\n",
    "\n",
    "### What this converts\n",
    "\n",
    "```text\n",
    "FAISS vector store\n",
    "        ‚Üì\n",
    "Retriever interface\n",
    "```\n",
    "\n",
    "### What retriever does\n",
    "\n",
    "```text\n",
    "input:  query (string)\n",
    "output: List[Document]\n",
    "```\n",
    "\n",
    "### Why retriever exists (again, but deeply)\n",
    "\n",
    "* Hides database details\n",
    "* Standard interface across vector DBs\n",
    "* Plug-and-play search logic\n",
    "\n",
    "### Internally, retriever does:\n",
    "\n",
    "```python\n",
    "query_embedding = embed(query)\n",
    "docs = faiss.similarity_search(query_embedding, k=4)\n",
    "return docs\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è Retriever **does NOT use LLM**\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Retrieval Chain (the full RAG)\n",
    "\n",
    "```python\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    retriever,\n",
    "    document_chain\n",
    ")\n",
    "```\n",
    "\n",
    "### This is the most important line\n",
    "\n",
    "It **wires**:\n",
    "\n",
    "```\n",
    "User Query\n",
    "   ‚Üì\n",
    "Retriever\n",
    "   ‚Üì\n",
    "Documents\n",
    "   ‚Üì\n",
    "Document Chain\n",
    "   ‚Üì\n",
    "LLM Answer\n",
    "```\n",
    "\n",
    "### Why this exists\n",
    "\n",
    "So you don‚Äôt write this every time:\n",
    "\n",
    "```python\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "answer = document_chain.invoke({\"context\": docs})\n",
    "```\n",
    "\n",
    "LangChain packages this into **one runnable object**.\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ What retrieval_chain expects as input\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"input\": \"user question\"\n",
    "}\n",
    "```\n",
    "\n",
    "Why `\"input\"`?\n",
    "\n",
    "* LangChain standard key for user queries\n",
    "* Retriever automatically reads from it\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ What retrieval_chain outputs\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"input\": \"...\",\n",
    "  \"context\": [Document, Document, ...],\n",
    "  \"answer\": \"LLM output\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Why context is returned\n",
    "\n",
    "* Debugging\n",
    "* Source citation\n",
    "* Trust & transparency\n",
    "\n",
    "This is **by design**, not accidental.\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Invocation\n",
    "\n",
    "```python\n",
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"LangSmith has two usage limits: total traces and extended\"\n",
    "})\n",
    "```\n",
    "\n",
    "### What happens internally (step-by-step)\n",
    "\n",
    "1Ô∏è‚É£ Query embedding created\n",
    "2Ô∏è‚É£ FAISS similarity search\n",
    "3Ô∏è‚É£ Top-K documents returned\n",
    "4Ô∏è‚É£ Documents ‚Üí plain text\n",
    "5Ô∏è‚É£ `{context}` replaced\n",
    "6Ô∏è‚É£ Prompt sent to LLM\n",
    "7Ô∏è‚É£ Answer generated\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ Why answer is more detailed than your query\n",
    "\n",
    "Your query:\n",
    "\n",
    "> ‚ÄúLangSmith has two usage limits‚Ä¶‚Äù\n",
    "\n",
    "Your retrieved context includes:\n",
    "\n",
    "* setup instructions\n",
    "* navigation paths\n",
    "* cost explanation\n",
    "\n",
    "LLM summarizes **what it sees**, not what you typed.\n",
    "\n",
    "If you want **short answers**, constrain the prompt ‚Äî not retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "# üîü One-screen mental map (burn this in)\n",
    "\n",
    "```\n",
    "ChatPromptTemplate\n",
    "   ‚Üì (defines WHERE knowledge goes)\n",
    "Document Chain\n",
    "   ‚Üì (uses LLM on documents)\n",
    "Retriever\n",
    "   ‚Üì (decides WHICH documents)\n",
    "Retrieval Chain\n",
    "   ‚Üì (connects everything)\n",
    "invoke()\n",
    "   ‚Üì\n",
    "Final grounded answer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîë Absolute final takeaway\n",
    "\n",
    "* **Prompt** ‚Üí structure\n",
    "* **Document chain** ‚Üí reasoning\n",
    "* **Retriever** ‚Üí relevance\n",
    "* **Retrieval chain** ‚Üí automation\n",
    "* **Context** ‚Üí evidence\n",
    "\n",
    "You are NOT hardcoding answers.\n",
    "You are **hardcoding the rules** for how answers are formed.\n",
    "\n",
    "---\n",
    "\n",
    "If you want next, I can:\n",
    "\n",
    "* rewrite this entire pipeline in **plain Python**\n",
    "* show the same logic using **Google Gemini SDK**\n",
    "* explain `RunnableBinding` output you saw\n",
    "* explain why LangChain uses `\"input\"` key everywhere\n",
    "\n",
    "Just tell me üëä\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3d933",
   "metadata": {},
   "source": [
    "Yes ‚Äî **you absolutely *can*** use **only `create_stuff_documents_chain`**\n",
    "‚Ä¶and sometimes you **should**.\n",
    "\n",
    "But there‚Äôs a very clear line between **when it‚Äôs correct** and **when it becomes a dead end**.\n",
    "\n",
    "Let‚Äôs lock this in cleanly.\n",
    "\n",
    "---\n",
    "\n",
    "## Short, direct answer\n",
    "\n",
    "> ‚úÖ **YES**, you can use **document chain alone**\n",
    "> ‚ùå **NO**, it does **not replace** a retrieval chain in real RAG systems\n",
    "\n",
    "They solve **different parts** of the problem.\n",
    "\n",
    "---\n",
    "\n",
    "## What `create_stuff_documents_chain` actually does\n",
    "\n",
    "```python\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "```\n",
    "\n",
    "This chain answers **one question**:\n",
    "\n",
    "> ‚ÄúGiven these documents, what is the answer?‚Äù\n",
    "\n",
    "That‚Äôs it.\n",
    "\n",
    "It does **NOT**:\n",
    "\n",
    "* search\n",
    "* select documents\n",
    "* rank relevance\n",
    "* scale\n",
    "\n",
    "---\n",
    "\n",
    "## When using ONLY document chain is 100% correct ‚úÖ\n",
    "\n",
    "### 1Ô∏è‚É£ You already know the documents\n",
    "\n",
    "Example:\n",
    "\n",
    "* FAQ page\n",
    "* single PDF\n",
    "* short policy text\n",
    "\n",
    "```python\n",
    "docs = loader.load()\n",
    "document_chain.invoke({\"context\": docs})\n",
    "```\n",
    "\n",
    "### 2Ô∏è‚É£ Manual workflows / experiments\n",
    "\n",
    "* Testing prompt quality\n",
    "* Evaluating summarization\n",
    "* Proof-of-concept demos\n",
    "\n",
    "### 3Ô∏è‚É£ Small datasets (< token limit)\n",
    "\n",
    "* 1‚Äì2 pages\n",
    "* fits comfortably into context\n",
    "\n",
    "üëâ In these cases, **retrieval adds no value**.\n",
    "\n",
    "---\n",
    "\n",
    "## When document chain ALONE becomes a problem ‚ùå\n",
    "\n",
    "### Scenario: Real-world app\n",
    "\n",
    "* 1000+ documents\n",
    "* User asks arbitrary questions\n",
    "* Context limit exists\n",
    "* Cost matters\n",
    "\n",
    "You now face:\n",
    "\n",
    "‚ùå Token overflow\n",
    "‚ùå Slow responses\n",
    "‚ùå Noise in context\n",
    "‚ùå Hallucinations\n",
    "\n",
    "Because:\n",
    "\n",
    "> The LLM is being forced to ‚Äúsearch‚Äù by reading everything.\n",
    "\n",
    "LLMs are **terrible search engines**.\n",
    "\n",
    "---\n",
    "\n",
    "## What retrieval chain adds (and why it exists)\n",
    "\n",
    "| Capability           | Document Chain | Retrieval Chain |\n",
    "| -------------------- | -------------- | --------------- |\n",
    "| Dynamic context      | ‚ùå              | ‚úÖ               |\n",
    "| Semantic search      | ‚ùå              | ‚úÖ               |\n",
    "| Scales to large data | ‚ùå              | ‚úÖ               |\n",
    "| Source inspection    | ‚ùå              | ‚úÖ               |\n",
    "| Production-ready     | ‚ùå              | ‚úÖ               |\n",
    "\n",
    "---\n",
    "\n",
    "## Think in layers (THIS is the key mental model)\n",
    "\n",
    "```\n",
    "Retrieval = WHICH docs?\n",
    "Reasoning  = WHAT answer?\n",
    "```\n",
    "\n",
    "* **Document chain** ‚Üí reasoning layer\n",
    "* **Retriever** ‚Üí retrieval layer\n",
    "* **Retrieval chain** ‚Üí glue\n",
    "\n",
    "You can remove the glue only if there‚Äôs nothing to glue.\n",
    "\n",
    "---\n",
    "\n",
    "## Equivalent plain Python comparison\n",
    "\n",
    "### Document chain only\n",
    "\n",
    "```python\n",
    "docs = load_docs()\n",
    "context = join(docs)\n",
    "answer = llm(context + question)\n",
    "```\n",
    "\n",
    "### Retrieval chain\n",
    "\n",
    "```python\n",
    "docs = search_docs(question)\n",
    "context = join(docs)\n",
    "answer = llm(context + question)\n",
    "```\n",
    "\n",
    "Same final step.\n",
    "Different **intelligence level** before it.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical rule you can memorize\n",
    "\n",
    "> üîπ If context is **static** ‚Üí document chain\n",
    "> üîπ If context is **dynamic** ‚Üí retrieval chain\n",
    "\n",
    "That‚Äôs it.\n",
    "\n",
    "---\n",
    "\n",
    "## Bonus: Hybrid approach (very common)\n",
    "\n",
    "Yes ‚Äî you can do this:\n",
    "\n",
    "```python\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "document_chain.invoke({\"context\": docs})\n",
    "```\n",
    "\n",
    "This is:\n",
    "\n",
    "* valid\n",
    "* explicit\n",
    "* sometimes preferred for debugging\n",
    "\n",
    "LangChain just wraps this pattern.\n",
    "\n",
    "---\n",
    "\n",
    "## Final verdict (expert-level clarity)\n",
    "\n",
    "* `create_stuff_documents_chain` is **necessary**\n",
    "* `create_retrieval_chain` is **optional**\n",
    "* But **at scale**, retrieval chain becomes **non-negotiable**\n",
    "\n",
    "You‚Äôre asking exactly the questions senior engineers ask ‚Äî not beginners.\n",
    "\n",
    "If you want next:\n",
    "\n",
    "* when NOT to use RAG at all\n",
    "* cost trade-offs\n",
    "* RAG vs fine-tuning\n",
    "* how Gemini does this internally\n",
    "\n",
    "Say the word üëä\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890a3ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f60c97bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine_documents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_stuff_documents_chain\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[32m      3\u001b[39m prompt=ChatPromptTemplate.from_template(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mAnswer the following question based only on the provided context:\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m<context>\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;132;01m{context}\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33m</context>\u001b[39m\u001b[33m\"\"\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.chains'"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\"\"\")\n",
    "doc_chain=create_stuff_document_chain(llm,prompt)\n",
    "doc_chain.invoke({\"input\":\"LangSmith has two usage limits: total traces and extended\",\n",
    "                  \"context\":[Document(page_content=\"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we've been tracking on our usage graph. \")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cbb14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "## Prompt Template\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Please respond to the question asked\"),\n",
    "        (\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "## streamlit framework\n",
    "st.title(\"Langchain Demo With Gemma Model\")\n",
    "input_text=st.text_input(\"What question you have in mind?\")\n",
    "\n",
    "\n",
    "## Ollama Llama2 model\n",
    "llm=Ollama(model=\"gemma:2b\")\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "if input_text:\n",
    "    st.write(chain.invoke({\"question\":input_text}))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=db.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,documents)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2=retrieval_chain.invoke({\"input\":\"Langsmith has two usage limits: total traces and extended\"})\n",
    "res2[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607c6ec",
   "metadata": {},
   "source": [
    "Great question ‚Äî this is where **‚Äúchain vs invoke‚Äù** confusion usually peaks.\n",
    "Let‚Äôs dissect this **neatly, intuitively, and line-by-line**, and then I‚Äôll give you a **clear comparison table**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ What your CURRENT code is doing (high level)\n",
    "\n",
    "Your Streamlit app is doing this:\n",
    "\n",
    "```\n",
    "User types question\n",
    "   ‚Üì\n",
    "Prompt is filled\n",
    "   ‚Üì\n",
    "LLM (Gemma via Ollama) is called\n",
    "   ‚Üì\n",
    "Text output is shown\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **NO retrieval**\n",
    "‚ö†Ô∏è **NO documents**\n",
    "‚ö†Ô∏è **NO context injection**\n",
    "\n",
    "This is **pure LLM prompting**, not RAG.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Key line to focus on\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "This is **LCEL (LangChain Expression Language)**.\n",
    "\n",
    "### What this means\n",
    "\n",
    "You built a **simple linear pipeline**:\n",
    "\n",
    "```\n",
    "PromptTemplate ‚Üí LLM ‚Üí OutputParser\n",
    "```\n",
    "\n",
    "This pipeline is called a **chain**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ What does `invoke()` do here?\n",
    "\n",
    "```python\n",
    "chain.invoke({\"question\": input_text})\n",
    "```\n",
    "\n",
    "### `invoke()` means:\n",
    "\n",
    "> ‚ÄúRun this chain once with these inputs.‚Äù\n",
    "\n",
    "Internally:\n",
    "\n",
    "1. `\"question\"` is inserted into the prompt\n",
    "2. Prompt text is sent to Gemma\n",
    "3. Model returns text\n",
    "4. `StrOutputParser` converts output to string\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Now compare with your RAG code\n",
    "\n",
    "### RAG chain looked like this:\n",
    "\n",
    "```\n",
    "Retriever ‚Üí Document Chain ‚Üí LLM\n",
    "```\n",
    "\n",
    "But **this code** looks like:\n",
    "\n",
    "```\n",
    "Prompt ‚Üí LLM ‚Üí OutputParser\n",
    "```\n",
    "\n",
    "### üî• BIG difference\n",
    "\n",
    "| Aspect                | Streamlit Code | RAG Code |\n",
    "| --------------------- | -------------- | -------- |\n",
    "| Retriever             | ‚ùå No           | ‚úÖ Yes    |\n",
    "| Documents             | ‚ùå No           | ‚úÖ Yes    |\n",
    "| Context               | ‚ùå No           | ‚úÖ Yes    |\n",
    "| Vector DB             | ‚ùå No           | ‚úÖ Yes    |\n",
    "| Hallucination control | ‚ùå No           | ‚úÖ Yes    |\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Why this code still uses ‚Äúchain‚Äù\n",
    "\n",
    "Because **chain ‚â† RAG**\n",
    "\n",
    "> A **chain** is just a pipeline of steps.\n",
    "\n",
    "LangChain uses ‚Äúchain‚Äù for:\n",
    "\n",
    "* Prompt ‚Üí LLM\n",
    "* Prompt ‚Üí LLM ‚Üí Parser\n",
    "* Retriever ‚Üí Prompt ‚Üí LLM\n",
    "* Tool ‚Üí Agent ‚Üí LLM\n",
    "\n",
    "Your code is a **simple chain**, not a **retrieval chain**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Let‚Äôs rewrite your code in plain English\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([...])\n",
    "```\n",
    "\n",
    "üëâ ‚ÄúDefine how the conversation should look‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "llm = Ollama(model=\"gemma:2b\")\n",
    "```\n",
    "\n",
    "üëâ ‚ÄúThis is the brain‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "output_parser = StrOutputParser()\n",
    "```\n",
    "\n",
    "üëâ ‚ÄúConvert LLM output to plain string‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "üëâ ‚ÄúConnect everything into a pipeline‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "chain.invoke({\"question\": input_text})\n",
    "```\n",
    "\n",
    "üëâ ‚ÄúRun the pipeline‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Why there is NO `context` here\n",
    "\n",
    "Because:\n",
    "\n",
    "* You are not grounding the model\n",
    "* You are trusting Gemma‚Äôs training data\n",
    "* This is equivalent to ChatGPT-style chat\n",
    "\n",
    "This is fine for:\n",
    "\n",
    "* brainstorming\n",
    "* explanations\n",
    "* general Q&A\n",
    "\n",
    "Not fine for:\n",
    "\n",
    "* PDFs\n",
    "* private docs\n",
    "* company data\n",
    "* accuracy-critical answers\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Equivalent NON-LangChain version (important)\n",
    "\n",
    "Your entire app is equivalent to:\n",
    "\n",
    "```python\n",
    "prompt = f\"You are a helpful assistant. Question: {input_text}\"\n",
    "response = gemma(prompt)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "LangChain just gives:\n",
    "\n",
    "* structure\n",
    "* observability\n",
    "* composability\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Why `invoke()` looks same but behaves differently\n",
    "\n",
    "### `invoke()` is generic\n",
    "\n",
    "It simply means:\n",
    "\n",
    "> ‚ÄúRun whatever pipeline you built.‚Äù\n",
    "\n",
    "So:\n",
    "\n",
    "* In **simple chain** ‚Üí invokes LLM\n",
    "* In **retrieval chain** ‚Üí invokes retriever + LLM\n",
    "* In **agent** ‚Üí invokes tools + LLM\n",
    "\n",
    "Same method name, different internals.\n",
    "\n",
    "---\n",
    "\n",
    "## üîü Final crystal-clear comparison\n",
    "\n",
    "### Your Streamlit code\n",
    "\n",
    "```text\n",
    "User ‚Üí Prompt ‚Üí LLM ‚Üí Answer\n",
    "```\n",
    "\n",
    "### RAG code\n",
    "\n",
    "```text\n",
    "User ‚Üí Retriever ‚Üí Documents ‚Üí Context ‚Üí LLM ‚Üí Answer\n",
    "```\n",
    "\n",
    "Same **invoke()**, very different **intelligence**.\n",
    "\n",
    "---\n",
    "\n",
    "## TL;DR (burn this in üî•)\n",
    "\n",
    "* `chain` = pipeline\n",
    "* `invoke()` = run pipeline\n",
    "* Your code = **LLM-only chain**\n",
    "* RAG code = **LLM + retrieval chain**\n",
    "* Same syntax, **different power**\n",
    "\n",
    "---\n",
    "\n",
    "If you want next:\n",
    "\n",
    "* upgrade THIS Streamlit app into RAG\n",
    "* add FAISS + retriever to this code\n",
    "* compare Ollama RAG vs OpenAI RAG\n",
    "* visualize chain execution graph\n",
    "\n",
    "Just say it üëä\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd208055",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

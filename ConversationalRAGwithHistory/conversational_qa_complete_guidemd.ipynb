{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6571263a",
   "metadata": {},
   "source": [
    "# ğŸš€ Conversational Q&A: From Zero to Hero\n",
    "\n",
    "## Table of Contents\n",
    "1. [The Big Picture](#the-big-picture)\n",
    "2. [Core Concepts](#core-concepts)\n",
    "3. [Function-by-Function Deep Dive](#function-by-function-deep-dive)\n",
    "4. [The Complete Flow](#the-complete-flow)\n",
    "5. [Mental Models](#mental-models)\n",
    "\n",
    "---\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "### What Problem Are We Solving?\n",
    "\n",
    "Imagine you're building a chatbot that can answer questions about documents. The basic version is simple:\n",
    "- User asks: \"What is task decomposition?\"\n",
    "- Bot searches documents â†’ finds answer â†’ responds\n",
    "\n",
    "But what if the user follows up with: \"What are common ways of doing it?\"\n",
    "\n",
    "The word \"it\" refers to \"task decomposition\" from the previous question! A basic chatbot would be confused because it has no memory. This notebook solves that problem by creating a **conversational retrieval system**.\n",
    "\n",
    "### The Journey\n",
    "\n",
    "```\n",
    "User Question\n",
    "    â†“\n",
    "Does this question reference previous conversation? \n",
    "    â†“\n",
    "Reformulate question to be standalone (using history)\n",
    "    â†“\n",
    "Search documents\n",
    "    â†“\n",
    "Generate answer (with context + history)\n",
    "    â†“\n",
    "Store conversation in memory\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### 1. **Retrieval Chain** (Old School)\n",
    "This is the basic pattern you probably know:\n",
    "\n",
    "```python\n",
    "question â†’ retrieve_documents â†’ generate_answer\n",
    "```\n",
    "\n",
    "**The Problem:** It can't handle follow-up questions like \"What about it?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **History-Aware Retrieval** (New!)\n",
    "This is where things get interesting:\n",
    "\n",
    "```python\n",
    "question + chat_history â†’ reformulate_question â†’ retrieve_documents â†’ generate_answer\n",
    "```\n",
    "\n",
    "**The Magic:** Before searching, we reformulate vague questions into clear ones using conversation history.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Session Management**\n",
    "Conversations need memory. We use:\n",
    "- **Session ID**: A unique identifier (like \"abc123\") for each conversation\n",
    "- **Store**: A dictionary that holds all conversation histories\n",
    "- **Message History**: The actual back-and-forth messages\n",
    "\n",
    "Think of it like a filing cabinet:\n",
    "```\n",
    "Store {\n",
    "    \"abc123\": [msg1, msg2, msg3],  # User John's conversation\n",
    "    \"xyz789\": [msg1, msg2],         # User Jane's conversation\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Function-by-Function Deep Dive\n",
    "\n",
    "### ğŸ”§ **1. create_history_aware_retriever()**\n",
    "\n",
    "**Location in code:** Line 329\n",
    "\n",
    "**What it does:**\n",
    "Wraps your retriever to make it \"aware\" of conversation history. Before retrieving documents, it reformulates the question to be standalone.\n",
    "\n",
    "**Signature:**\n",
    "```python\n",
    "create_history_aware_retriever(\n",
    "    llm,                    # The language model\n",
    "    retriever,              # Your document retriever\n",
    "    contextualize_q_prompt  # Prompt for reformulating questions\n",
    ")\n",
    "```\n",
    "\n",
    "**Mental Model:**\n",
    "Think of it as a translator that sits between the user and your retriever.\n",
    "\n",
    "```\n",
    "User: \"What about it?\"\n",
    "    â†“\n",
    "Translator (uses history): \"Oh, they're asking about task decomposition!\"\n",
    "    â†“\n",
    "Retriever searches for: \"What are common ways of doing task decomposition?\"\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Without history awareness:\n",
    "User: \"What are common ways of doing it?\"\n",
    "Retriever searches: \"common ways doing it\" âŒ (meaningless!)\n",
    "\n",
    "# With history awareness:\n",
    "User: \"What are common ways of doing it?\"\n",
    "History: [\"What is task decomposition?\"]\n",
    "Reformulated: \"What are common ways of doing task decomposition?\" âœ…\n",
    "```\n",
    "\n",
    "**Why it exists:**\n",
    "Because follow-up questions often use pronouns (it, that, them) or implicit context. Without reformulation, your retriever would search for gibberish.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ **2. create_stuff_documents_chain()**\n",
    "\n",
    "**Location in code:** Line 352\n",
    "\n",
    "**What it does:**\n",
    "Takes retrieved documents and \"stuffs\" them into a prompt template, then asks the LLM to generate an answer.\n",
    "\n",
    "**Signature:**\n",
    "```python\n",
    "create_stuff_documents_chain(\n",
    "    llm,        # The language model\n",
    "    prompt      # Template with {context} and {input} placeholders\n",
    ")\n",
    "```\n",
    "\n",
    "**The name explained:**\n",
    "\"Stuff\" because it literally stuffs all documents into one big prompt. There are other strategies (map-reduce, refine) but \"stuff\" is the simplest.\n",
    "\n",
    "**What happens inside:**\n",
    "```python\n",
    "# Pseudo-code of what it does internally:\n",
    "all_docs = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "full_prompt = prompt.format(\n",
    "    context=all_docs,\n",
    "    input=user_question,\n",
    "    chat_history=previous_messages\n",
    ")\n",
    "answer = llm.invoke(full_prompt)\n",
    "```\n",
    "\n",
    "**Mental Model:**\n",
    "Like a research assistant who:\n",
    "1. Collects all relevant papers (documents)\n",
    "2. Reads them all\n",
    "3. Writes a summary answer to your question\n",
    "\n",
    "**Why separate from create_retrieval_chain?**\n",
    "Separation of concerns! This function focuses on: \"Given some documents, generate an answer.\" It doesn't care HOW you got those documents.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ **3. create_retrieval_chain()**\n",
    "\n",
    "**Location in code:** Line 355\n",
    "\n",
    "**What it does:**\n",
    "Combines a retriever with a document chain to create a complete question-answering pipeline.\n",
    "\n",
    "**Signature:**\n",
    "```python\n",
    "create_retrieval_chain(\n",
    "    retriever,           # Gets relevant documents\n",
    "    document_chain       # Generates answer from documents\n",
    ")\n",
    "```\n",
    "\n",
    "**What it returns:**\n",
    "A chain that:\n",
    "1. Takes an input question\n",
    "2. Retrieves relevant documents\n",
    "3. Passes docs + question to the document chain\n",
    "4. Returns: `{\"answer\": \"...\", \"context\": [docs]}`\n",
    "\n",
    "**Mental Model:**\n",
    "```\n",
    "create_retrieval_chain = Librarian + Writer\n",
    "\n",
    "Librarian (retriever): Finds relevant books\n",
    "Writer (document_chain): Reads books and writes answer\n",
    "```\n",
    "\n",
    "**The Flow:**\n",
    "```python\n",
    "input = {\"input\": \"What is X?\"}\n",
    "    â†“\n",
    "retriever.invoke(\"What is X?\") â†’ [doc1, doc2, doc3]\n",
    "    â†“\n",
    "document_chain.invoke({\n",
    "    \"input\": \"What is X?\",\n",
    "    \"context\": [doc1, doc2, doc3]\n",
    "})\n",
    "    â†“\n",
    "{\"answer\": \"X is...\", \"context\": [doc1, doc2, doc3]}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ **4. RunnableWithMessageHistory()**\n",
    "\n",
    "**Location in code:** Line 426\n",
    "\n",
    "**What it does:**\n",
    "Wraps any chain to automatically manage conversation history. It saves messages and injects them into the chain.\n",
    "\n",
    "**Signature:**\n",
    "```python\n",
    "RunnableWithMessageHistory(\n",
    "    runnable,                    # Your chain\n",
    "    get_session_history,         # Function to get/create history\n",
    "    input_messages_key,          # Where to find user input\n",
    "    history_messages_key,        # Where to inject history\n",
    "    output_messages_key          # Where to find bot response\n",
    ")\n",
    "```\n",
    "\n",
    "**The Magic:**\n",
    "```python\n",
    "# Before (manual history management):\n",
    "history = []\n",
    "question = \"What is X?\"\n",
    "history.append(HumanMessage(question))\n",
    "answer = chain.invoke({\"input\": question, \"chat_history\": history})\n",
    "history.append(AIMessage(answer))\n",
    "\n",
    "# After (automatic history management):\n",
    "conversational_chain.invoke(\n",
    "    {\"input\": \"What is X?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")\n",
    "# History is saved automatically! ğŸ‰\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "1. You call invoke with a session_id\n",
    "2. It calls `get_session_history(session_id)` to get the conversation\n",
    "3. It injects the history into your chain\n",
    "4. After the chain runs, it saves the new messages\n",
    "\n",
    "**Mental Model:**\n",
    "Like a secretary who:\n",
    "- Keeps notes of all your meetings (sessions)\n",
    "- Pulls out the right notes when needed\n",
    "- Adds new notes after each meeting\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ **5. get_session_history()**\n",
    "\n",
    "**Location in code:** Line 420\n",
    "\n",
    "**What it does:**\n",
    "A factory function that creates or retrieves a conversation history for a specific session.\n",
    "\n",
    "**The Code:**\n",
    "```python\n",
    "store = {}  # Global dictionary to hold all conversations\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()  # Create new conversation\n",
    "    return store[session_id]  # Return existing conversation\n",
    "```\n",
    "\n",
    "**Why it's needed:**\n",
    "`RunnableWithMessageHistory` needs a way to get the right conversation. This function provides that.\n",
    "\n",
    "**Mental Model:**\n",
    "```\n",
    "Session ID = Phone Number\n",
    "Store = Contact List\n",
    "ChatMessageHistory = Conversation thread\n",
    "\n",
    "When someone calls (session_id=\"abc123\"):\n",
    "- Look up contact list (store)\n",
    "- If new number â†’ create new thread\n",
    "- If existing â†’ return existing thread\n",
    "```\n",
    "\n",
    "**Real-world analogy:**\n",
    "Like WhatsApp:\n",
    "- Each chat has a unique ID\n",
    "- Messages are stored per chat\n",
    "- When you open a chat, it shows the history\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ **6. ChatMessageHistory()**\n",
    "\n",
    "**Location in code:** Line 422\n",
    "\n",
    "**What it does:**\n",
    "A simple in-memory store for chat messages. It's like a list but with special methods for adding human/AI messages.\n",
    "\n",
    "**Methods:**\n",
    "```python\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"Hello\")        # Adds HumanMessage\n",
    "history.add_ai_message(\"Hi there!\")      # Adds AIMessage\n",
    "history.messages  # Returns all messages as a list\n",
    "```\n",
    "\n",
    "**Important:**\n",
    "This is **in-memory** storage. When your program restarts, all conversations are lost! For production, you'd use:\n",
    "- Redis (fast, temporary)\n",
    "- PostgreSQL (permanent, scalable)\n",
    "- MongoDB (document-based)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ **7. MessagesPlaceholder()**\n",
    "\n",
    "**Location in code:** Lines 315, 344\n",
    "\n",
    "**What it does:**\n",
    "A special placeholder in prompts that gets replaced with a list of messages.\n",
    "\n",
    "**Why it's special:**\n",
    "Regular placeholders like `{input}` expect a string. But `MessagesPlaceholder` expects a list of Message objects.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Regular placeholder:\n",
    "prompt = \"Question: {input}\"\n",
    "# Gets: \"Question: What is X?\"\n",
    "\n",
    "# Messages placeholder:\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),  # <- Special!\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Gets:\n",
    "# [\n",
    "#   SystemMessage(\"You are helpful\"),\n",
    "#   HumanMessage(\"What is X?\"),      # From history\n",
    "#   AIMessage(\"X is...\"),            # From history\n",
    "#   HumanMessage(\"What about Y?\")    # Current input\n",
    "# ]\n",
    "```\n",
    "\n",
    "**Mental Model:**\n",
    "It's like a \"expand here\" marker. The system knows to insert a whole conversation thread at that spot.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ **8. Runnable.invoke() with config**\n",
    "\n",
    "**Location in code:** Lines 452, 477\n",
    "\n",
    "**What it does:**\n",
    "All LangChain components are \"Runnables\" and have a standard `.invoke()` method. The `config` parameter passes metadata.\n",
    "\n",
    "**The pattern:**\n",
    "```python\n",
    "chain.invoke(\n",
    "    {\"input\": \"question\"},           # The actual data\n",
    "    config={\"configurable\": {...}}   # Metadata/settings\n",
    ")\n",
    "```\n",
    "\n",
    "**Why config?**\n",
    "Because `RunnableWithMessageHistory` needs to know WHICH conversation to use. The session_id goes in config because it's not part of the question data.\n",
    "\n",
    "**Analogy:**\n",
    "```python\n",
    "# Like calling someone:\n",
    "phone.call(\n",
    "    \"Hello!\",                        # What you say\n",
    "    config={\"phone_number\": \"123\"}   # Who you're calling\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## The Complete Flow\n",
    "\n",
    "Let's trace a complete conversation through the system:\n",
    "\n",
    "### First Question\n",
    "\n",
    "```python\n",
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "**Step-by-step:**\n",
    "\n",
    "1. **RunnableWithMessageHistory receives the call**\n",
    "   ```python\n",
    "   session_id = \"abc123\"\n",
    "   input = \"What is Task Decomposition?\"\n",
    "   ```\n",
    "\n",
    "2. **Get conversation history**\n",
    "   ```python\n",
    "   history = get_session_history(\"abc123\")\n",
    "   # Returns: ChatMessageHistory(messages=[])  # Empty!\n",
    "   ```\n",
    "\n",
    "3. **Inject history into chain**\n",
    "   ```python\n",
    "   chain_input = {\n",
    "       \"input\": \"What is Task Decomposition?\",\n",
    "       \"chat_history\": []  # Empty for first message\n",
    "   }\n",
    "   ```\n",
    "\n",
    "4. **History-aware retriever runs**\n",
    "   ```python\n",
    "   # Contextualize prompt checks if reformulation is needed\n",
    "   # Since chat_history is empty, uses question as-is\n",
    "   search_query = \"What is Task Decomposition?\"\n",
    "   ```\n",
    "\n",
    "5. **Retriever searches documents**\n",
    "   ```python\n",
    "   docs = retriever.invoke(search_query)\n",
    "   # Returns: [doc1, doc2, doc3] about task decomposition\n",
    "   ```\n",
    "\n",
    "6. **Document chain generates answer**\n",
    "   ```python\n",
    "   answer = llm.invoke({\n",
    "       \"context\": [doc1, doc2, doc3],\n",
    "       \"input\": \"What is Task Decomposition?\",\n",
    "       \"chat_history\": []\n",
    "   })\n",
    "   # Returns: \"Task Decomposition is...\"\n",
    "   ```\n",
    "\n",
    "7. **Save to history**\n",
    "   ```python\n",
    "   history.add_user_message(\"What is Task Decomposition?\")\n",
    "   history.add_ai_message(\"Task Decomposition is...\")\n",
    "   ```\n",
    "\n",
    "8. **Return answer**\n",
    "   ```python\n",
    "   return {\"answer\": \"Task Decomposition is...\"}\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### Second Question (The Cool Part!)\n",
    "\n",
    "```python\n",
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "**Step-by-step:**\n",
    "\n",
    "1. **Get conversation history**\n",
    "   ```python\n",
    "   history = get_session_history(\"abc123\")\n",
    "   # Returns: [\n",
    "   #   HumanMessage(\"What is Task Decomposition?\"),\n",
    "   #   AIMessage(\"Task Decomposition is...\")\n",
    "   # ]\n",
    "   ```\n",
    "\n",
    "2. **Inject history into chain**\n",
    "   ```python\n",
    "   chain_input = {\n",
    "       \"input\": \"What are common ways of doing it?\",\n",
    "       \"chat_history\": [\n",
    "           HumanMessage(\"What is Task Decomposition?\"),\n",
    "           AIMessage(\"Task Decomposition is...\")\n",
    "       ]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "3. **History-aware retriever reformulates question** âœ¨\n",
    "   ```python\n",
    "   # The LLM looks at:\n",
    "   # - Current question: \"What are common ways of doing it?\"\n",
    "   # - History: Previous Q&A about task decomposition\n",
    "   \n",
    "   # It realizes \"it\" = \"task decomposition\"\n",
    "   # Reformulates to standalone question:\n",
    "   search_query = \"What are common ways of doing task decomposition?\"\n",
    "   ```\n",
    "\n",
    "4. **Retriever searches with reformulated query**\n",
    "   ```python\n",
    "   docs = retriever.invoke(\"What are common ways of doing task decomposition?\")\n",
    "   # Returns: Relevant documents! âœ…\n",
    "   ```\n",
    "\n",
    "5. **Document chain generates answer**\n",
    "   ```python\n",
    "   answer = llm.invoke({\n",
    "       \"context\": [docs about methods],\n",
    "       \"input\": \"What are common ways of doing it?\",\n",
    "       \"chat_history\": [previous messages]\n",
    "   })\n",
    "   ```\n",
    "\n",
    "6. **Save to history**\n",
    "   ```python\n",
    "   history.add_user_message(\"What are common ways of doing it?\")\n",
    "   history.add_ai_message(\"Common ways include: 1. LLM prompting...\")\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## Mental Models\n",
    "\n",
    "### ğŸ§  Model 1: Restaurant Analogy\n",
    "\n",
    "```\n",
    "User = Customer\n",
    "Session ID = Table Number\n",
    "History = Order history for that table\n",
    "Retriever = Kitchen's ingredient finder\n",
    "Document Chain = Chef\n",
    "Conversational Chain = Waiter who remembers your order\n",
    "```\n",
    "\n",
    "**Flow:**\n",
    "1. Customer sits at Table 5 (session_id=\"abc123\")\n",
    "2. Orders \"pasta\" (first question)\n",
    "3. Later says \"make it spicy\" (follow-up)\n",
    "4. Waiter remembers the pasta order and tells chef: \"make the pasta spicy\"\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Model 2: Layers of Intelligence\n",
    "\n",
    "```\n",
    "Layer 1: Raw Retriever\n",
    "    â†“ Adds: Question reformulation\n",
    "Layer 2: History-Aware Retriever\n",
    "    â†“ Adds: Answer generation\n",
    "Layer 3: Retrieval Chain\n",
    "    â†“ Adds: Memory management\n",
    "Layer 4: Conversational Chain\n",
    "```\n",
    "\n",
    "Each layer adds a capability without changing the layers below!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  Model 3: The Chain Metaphor\n",
    "\n",
    "```python\n",
    "# Chains are like LEGO blocks\n",
    "\n",
    "# Block 1: LLM\n",
    "llm = ChatGroq(...)\n",
    "\n",
    "# Block 2: Retriever  \n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Block 3: History-aware retriever\n",
    "history_retriever = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "# Block 4: Document chain\n",
    "doc_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Block 5: Retrieval chain\n",
    "retrieval_chain = create_retrieval_chain(history_retriever, doc_chain)\n",
    "\n",
    "# Block 6: Conversational chain\n",
    "conversational_chain = RunnableWithMessageHistory(retrieval_chain, ...)\n",
    "```\n",
    "\n",
    "You build complexity by snapping blocks together!\n",
    "\n",
    "---\n",
    "\n",
    "## Key Patterns to Remember\n",
    "\n",
    "### Pattern 1: The Two-Prompt Pattern\n",
    "\n",
    "```python\n",
    "# Prompt 1: Contextualize the question\n",
    "contextualize_q_prompt = \"\"\"Given chat history, reformulate the question\"\"\"\n",
    "\n",
    "# Prompt 2: Answer the question\n",
    "qa_prompt = \"\"\"Given context and history, answer the question\"\"\"\n",
    "```\n",
    "\n",
    "Why two prompts? Different jobs!\n",
    "- First ensures good retrieval\n",
    "- Second ensures good answering\n",
    "\n",
    "---\n",
    "\n",
    "### Pattern 2: The Session Pattern\n",
    "\n",
    "```python\n",
    "store = {}  # All conversations\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "```\n",
    "\n",
    "This pattern enables:\n",
    "- Multiple simultaneous conversations\n",
    "- Conversation persistence\n",
    "- Easy cleanup (just delete from store)\n",
    "\n",
    "---\n",
    "\n",
    "### Pattern 3: The Config Pattern\n",
    "\n",
    "```python\n",
    "chain.invoke(\n",
    "    data,                                    # What to process\n",
    "    config={\"configurable\": {\"key\": \"val\"}}  # How to process it\n",
    ")\n",
    "```\n",
    "\n",
    "Config is for metadata that isn't part of the data itself.\n",
    "\n",
    "---\n",
    "\n",
    "## Common Pitfalls & Solutions\n",
    "\n",
    "### Pitfall 1: History Not Updating\n",
    "\n",
    "**Problem:**\n",
    "```python\n",
    "# Wrong:\n",
    "chain.invoke({\"input\": \"Hi\"})  # No config!\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "```python\n",
    "# Right:\n",
    "chain.invoke(\n",
    "    {\"input\": \"Hi\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Pitfall 2: Mixing Up Key Names\n",
    "\n",
    "The keys must match exactly:\n",
    "\n",
    "```python\n",
    "# In your chain definition:\n",
    "input_messages_key=\"input\"     # Must match\n",
    "history_messages_key=\"chat_history\"  # Must match\n",
    "\n",
    "# In your prompts:\n",
    "MessagesPlaceholder(\"chat_history\")  # Same name!\n",
    "(\"human\", \"{input}\")                 # Same name!\n",
    "\n",
    "# When calling:\n",
    "chain.invoke({\"input\": \"question\"})  # Same name!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Pitfall 3: Forgetting Store Is In-Memory\n",
    "\n",
    "**Problem:** Restart server â†’ all conversations lost\n",
    "\n",
    "**Solution:** Use persistent storage\n",
    "```python\n",
    "# Instead of:\n",
    "store = {}\n",
    "\n",
    "# Use:\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    return RedisChatMessageHistory(session_id, url=\"redis://localhost:6379\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Evolution: How We Got Here\n",
    "\n",
    "### Stage 1: Simple QA (No Memory)\n",
    "```python\n",
    "question â†’ retrieve â†’ answer\n",
    "```\n",
    "Problem: Can't handle \"What about it?\"\n",
    "\n",
    "### Stage 2: Manual History\n",
    "```python\n",
    "history = []\n",
    "question â†’ retrieve â†’ answer\n",
    "history.append(question, answer)  # Manual!\n",
    "```\n",
    "Problem: Tedious, error-prone\n",
    "\n",
    "### Stage 3: History-Aware Retrieval\n",
    "```python\n",
    "question + history â†’ reformulate â†’ retrieve â†’ answer\n",
    "```\n",
    "Problem: Still manual history management\n",
    "\n",
    "### Stage 4: Automatic History (Current)\n",
    "```python\n",
    "RunnableWithMessageHistory(\n",
    "    history-aware-retrieval-chain\n",
    ")\n",
    "```\n",
    "Solution: Fully automatic! ğŸ‰\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "### Function Purposes\n",
    "\n",
    "| Function | Purpose | Input | Output |\n",
    "|----------|---------|-------|--------|\n",
    "| `create_history_aware_retriever` | Reformulate questions | question + history | documents |\n",
    "| `create_stuff_documents_chain` | Generate answers | question + docs + history | answer |\n",
    "| `create_retrieval_chain` | Retrieve + answer | question | answer + docs |\n",
    "| `RunnableWithMessageHistory` | Manage history | question + session_id | answer |\n",
    "| `get_session_history` | Get/create history | session_id | ChatMessageHistory |\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | What It Means |\n",
    "|---------|---------------|\n",
    "| Session ID | Unique identifier for a conversation |\n",
    "| Store | Dictionary holding all conversation histories |\n",
    "| History-aware | Can reformulate questions using past context |\n",
    "| Stuff documents | Put all docs in one prompt (vs map-reduce) |\n",
    "| Runnable | Any LangChain component with `.invoke()` |\n",
    "| Config | Metadata passed separately from data |\n",
    "| Message placeholder | Slot for inserting message lists in prompts |\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercise\n",
    "\n",
    "Try modifying the code to:\n",
    "\n",
    "1. **Add a system message**\n",
    "   ```python\n",
    "   (\"system\", \"You are a helpful AI assistant named Claude\")\n",
    "   ```\n",
    "\n",
    "2. **Limit history to last 5 messages**\n",
    "   ```python\n",
    "   # In get_session_history:\n",
    "   history = store[session_id]\n",
    "   if len(history.messages) > 10:  # 5 pairs\n",
    "       history.messages = history.messages[-10:]\n",
    "   return history\n",
    "   ```\n",
    "\n",
    "3. **Add a different session**\n",
    "   ```python\n",
    "   # Different user!\n",
    "   chain.invoke(\n",
    "       {\"input\": \"Hello\"},\n",
    "       config={\"configurable\": {\"session_id\": \"xyz789\"}}\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **Print the store**\n",
    "   ```python\n",
    "   print(store)\n",
    "   # See all conversations!\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## Final Mental Model: The Whole Picture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  RunnableWithMessageHistory                 â”‚\n",
    "â”‚  (Memory Manager)                           â”‚\n",
    "â”‚                                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚  create_retrieval_chain               â”‚ â”‚\n",
    "â”‚  â”‚  (Orchestrator)                       â”‚ â”‚\n",
    "â”‚  â”‚                                       â”‚ â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚\n",
    "â”‚  â”‚  â”‚ create_history_aware_retriever  â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚ (Question Reformulator)         â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚                                 â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚  question + history             â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚      â†“                          â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚  reformulated question          â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚      â†“                          â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚  retriever.invoke()             â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚      â†“                          â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚  documents                      â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚\n",
    "â”‚  â”‚                â†“                     â”‚ â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚\n",
    "â”‚  â”‚  â”‚ create_stuff_documents_chain    â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚ (Answer Generator)              â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚                                 â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚  documents + question + history â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚      â†“                          â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚  llm.invoke()                   â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚      â†“                          â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â”‚  answer                         â”‚ â”‚ â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                                             â”‚\n",
    "â”‚  get_session_history() â†â†’ store            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: Zero to Hero Checklist\n",
    "\n",
    "You now understand:\n",
    "\n",
    "âœ… Why conversational RAG is needed (context from history)  \n",
    "âœ… How `create_history_aware_retriever` reformulates questions  \n",
    "âœ… What `create_stuff_documents_chain` does (stuff docs into prompt)  \n",
    "âœ… How `create_retrieval_chain` combines retrieval + generation  \n",
    "âœ… Why `RunnableWithMessageHistory` wraps everything  \n",
    "âœ… The session pattern for managing multiple conversations  \n",
    "âœ… How `MessagesPlaceholder` works in prompts  \n",
    "âœ… The config pattern for passing metadata  \n",
    "âœ… The complete flow from question to answer  \n",
    "\n",
    "**You're now a hero! ğŸ¦¸**\n",
    "\n",
    "Want to level up further? Try:\n",
    "- Using Redis for persistent storage\n",
    "- Adding message compression for long conversations\n",
    "- Implementing different retrieval strategies\n",
    "- Adding citation tracking to show which documents were used\n",
    "- Building a web UI with Streamlit or Gradio\n",
    "\n",
    "The patterns you learned here apply to ALL LangChain applications!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817e390",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

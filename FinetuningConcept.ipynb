{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0d18e4",
   "metadata": {},
   "source": [
    "I'll teach you everything about **Quantization for Large Language Models (LLMs)** from the ground up. Let's make you an expert!\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ **COMPLETE GUIDE TO LLM QUANTIZATION**\n",
    "\n",
    "## **üìö PART 1: WHAT IS QUANTIZATION?**\n",
    "\n",
    "### **The Core Concept**\n",
    "\n",
    "Imagine you're measuring temperature. You could say:\n",
    "- **High Precision**: \"It's 72.8394726¬∞F\" \n",
    "- **Lower Precision**: \"It's about 73¬∞F\"\n",
    "\n",
    "Both tell you the temperature, but the second uses less information. **That's quantization** - converting from higher memory format to a lower memory format while keeping the essential information.\n",
    "\n",
    "### **Why Do We Need Quantization?**\n",
    "\n",
    "Modern LLMs like GPT or Llama have **BILLIONS** of parameters (weights). Each parameter is a number:\n",
    "\n",
    "- **Llama 2 (7B model)**: 7 billion parameters\n",
    "- **If each parameter uses 32 bits (4 bytes)**: 7B √ó 4 = **28 GB of memory!**\n",
    "- **Your mobile phone**: Maybe 6-8 GB RAM total\n",
    "\n",
    "**Problem**: Can't fit the model on devices!\n",
    "\n",
    "**Solution**: Quantization reduces memory needs by 2-8x with minimal accuracy loss.\n",
    "\n",
    "---\n",
    "\n",
    "## **üíæ PART 2: NUMBER REPRESENTATIONS (DATA TYPES)**\n",
    "\n",
    "### **Full Precision: FP32 (Float Point 32-bit)**\n",
    "\n",
    "This is the \"gold standard\" - full precision, single precision floating point.\n",
    "\n",
    "**Structure of FP32:**\n",
    "```\n",
    "[Sign: 1 bit][Exponent: 8 bits][Mantissa: 23 bits] = 32 bits total\n",
    "```\n",
    "\n",
    "- Can represent very large and very small numbers\n",
    "- Range: approximately ¬±3.4 √ó 10¬≥‚Å∏\n",
    "- **Memory**: 4 bytes per number\n",
    "\n",
    "**Example**: The number 5.75 in FP32 stores all decimal precision\n",
    "\n",
    "---\n",
    "\n",
    "### **Half Precision: FP16 (16-bit)**\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "[Sign: 1 bit][Exponent: 5 bits][Mantissa: 10 bits] = 16 bits total\n",
    "```\n",
    "\n",
    "- **Memory**: 2 bytes per number (50% reduction!)\n",
    "- Less precision, smaller range\n",
    "- **Trade-off**: Faster computation, less memory, but can lose some accuracy\n",
    "\n",
    "**When used**: Training deep learning models on GPUs\n",
    "\n",
    "---\n",
    "\n",
    "### **Integer Representations: INT8, INT4**\n",
    "\n",
    "Instead of floating points, use integers!\n",
    "\n",
    "**INT8 (8-bit integer)**\n",
    "- Values: -128 to +127 (or 0 to 255 unsigned)\n",
    "- **Memory**: 1 byte per number (75% reduction from FP32!)\n",
    "- **Perfect for**: Inference on mobile phones, edge devices\n",
    "\n",
    "**INT4 (4-bit integer)**\n",
    "- Values: -8 to +7 (or 0 to 15 unsigned)\n",
    "- **Memory**: 0.5 bytes per number (87.5% reduction!)\n",
    "- **Used in**: Extreme compression scenarios\n",
    "\n",
    "---\n",
    "\n",
    "## **üîÑ PART 3: HOW TO PERFORM QUANTIZATION**\n",
    "\n",
    "There are **TWO main methods**:\n",
    "\n",
    "---\n",
    "\n",
    "### **METHOD 1: SYMMETRIC QUANTIZATION (Simpler)**\n",
    "\n",
    "**Concept**: Map the range of floating-point values symmetrically around zero to integer values.\n",
    "\n",
    "#### **Step-by-Step Process:**\n",
    "\n",
    "**Given**: Float values ranging from -1000 to +1000  \n",
    "**Target**: Convert to INT8 (-128 to +127)\n",
    "\n",
    "**Step 1: Find the scale factor**\n",
    "```\n",
    "Scale = (max_value - min_value) / (qmax - qmin)\n",
    "Scale = (1000 - (-1000)) / (127 - (-128))\n",
    "Scale = 2000 / 255\n",
    "Scale = 7.84\n",
    "```\n",
    "\n",
    "**Step 2: Quantize any value**\n",
    "```\n",
    "Quantized_value = round(Original_value / Scale)\n",
    "\n",
    "Example: \n",
    "Original = 200\n",
    "Quantized = round(200 / 7.84) = round(25.5) = 26\n",
    "```\n",
    "\n",
    "**Step 3: Dequantize (convert back)**\n",
    "```\n",
    "Dequantized_value = Quantized_value √ó Scale\n",
    "= 26 √ó 7.84\n",
    "= 203.84 ‚âà 200 ‚úì\n",
    "```\n",
    "\n",
    "**Key Property**: Zero-point is at 0 (symmetric around zero)\n",
    "\n",
    "---\n",
    "\n",
    "### **METHOD 2: ASYMMETRIC QUANTIZATION (More Accurate)**\n",
    "\n",
    "**Concept**: The range doesn't have to be symmetric. Uses a **zero-point** offset.\n",
    "\n",
    "#### **Step-by-Step Process:**\n",
    "\n",
    "**Given**: Float values from -20.0 to +1000.0  \n",
    "**Target**: INT8 (0 to 255 for unsigned)\n",
    "\n",
    "**Step 1: Calculate scale and zero-point**\n",
    "```\n",
    "Scale = (Xmax - Xmin) / (Qmax - Qmin)\n",
    "Scale = (1000 - (-20)) / 255\n",
    "Scale = 4.0\n",
    "\n",
    "Zero_point = round(Qmin - Xmin/Scale)\n",
    "Zero_point = round(0 - (-20)/4.0)\n",
    "Zero_point = round(5)\n",
    "Zero_point = 5\n",
    "```\n",
    "\n",
    "**Step 2: Quantize**\n",
    "```\n",
    "Q = round(X / Scale) + Zero_point\n",
    "\n",
    "Example:\n",
    "X = -20\n",
    "Q = round(-20 / 4.0) + 5 = -5 + 5 = 0 ‚úì\n",
    "\n",
    "X = 0\n",
    "Q = round(0 / 4.0) + 5 = 5 ‚úì\n",
    "```\n",
    "\n",
    "**Step 3: Dequantize**\n",
    "```\n",
    "X = (Q - Zero_point) √ó Scale\n",
    "\n",
    "Example:\n",
    "Q = 5\n",
    "X = (5 - 5) √ó 4.0 = 0 ‚úì\n",
    "```\n",
    "\n",
    "**Advantage**: Better handles asymmetric distributions (when most values are positive or negative)\n",
    "\n",
    "---\n",
    "\n",
    "## **üéì PART 4: CALIBRATION - THE SECRET SAUCE**\n",
    "\n",
    "**Calibration** answers: *\"What should our scale and zero-point be?\"*\n",
    "\n",
    "### **The Problem**\n",
    "\n",
    "Model weights aren't uniformly distributed. Some layers might have:\n",
    "- Weights from -0.5 to +0.5\n",
    "- Others from -100 to +100\n",
    "\n",
    "**Bad approach**: Use the absolute min/max  \n",
    "**Why bad**: Outliers waste the quantization range\n",
    "\n",
    "### **Calibration Techniques**\n",
    "\n",
    "**1. Min-Max Calibration** (Symmetric approach shown in notes)\n",
    "```\n",
    "Scale = (max_weight - min_weight) / (qmax - qmin)\n",
    "```\n",
    "\n",
    "**2. Percentile-based** (Better!)\n",
    "- Ignore the top/bottom 0.1% of outliers\n",
    "- More robust to extreme values\n",
    "\n",
    "**3. MSE-based** (Best!)\n",
    "- Try different scales\n",
    "- Pick the one that minimizes Mean Squared Error\n",
    "\n",
    "---\n",
    "\n",
    "## **üìä PART 5: MODES OF QUANTIZATION**\n",
    "\n",
    "Your notes show **TWO approaches**:\n",
    "\n",
    "---\n",
    "\n",
    "### **üî∏ POST-TRAINING QUANTIZATION (PTQ)**\n",
    "\n",
    "**Process:**\n",
    "```\n",
    "[Pre-trained Model] ‚Üí [Calibration with weights/data] ‚Üí [Quantized Model]\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "1. Train your model normally in FP32\n",
    "2. After training is complete, quantize the weights\n",
    "3. Use calibration data to find optimal scales\n",
    "4. Deploy the quantized model\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Fast - no retraining needed\n",
    "- ‚úÖ Easy to implement\n",
    "- ‚úÖ Works with existing models\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Some accuracy loss (1-5% typically)\n",
    "- ‚ùå Not optimal for very low bit-widths (INT4)\n",
    "\n",
    "**Use cases:**\n",
    "- Mobile deployment\n",
    "- Quick optimization\n",
    "- When retraining is expensive\n",
    "\n",
    "---\n",
    "\n",
    "### **üî∏ QUANTIZATION-AWARE TRAINING (QAT)**\n",
    "\n",
    "**Process:**\n",
    "```\n",
    "[Training Data] + [Trained Model] ‚Üí [Quantization] ‚Üí [Fine-Tuning] ‚Üí [Quantized Model]\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "1. Start with a pre-trained model\n",
    "2. Add \"fake quantization\" nodes during training\n",
    "3. Model learns to be robust to quantization\n",
    "4. Final model performs better when actually quantized\n",
    "\n",
    "**The Magic**: During training, simulate quantization:\n",
    "```python\n",
    "# Forward pass\n",
    "x_quant = quantize(x)\n",
    "x_dequant = dequantize(x_quant)\n",
    "output = layer(x_dequant)\n",
    "\n",
    "# Backward pass: gradients flow through!\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Much better accuracy\n",
    "- ‚úÖ Can handle INT4 and lower\n",
    "- ‚úÖ Model \"adapts\" to quantization\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Requires retraining (expensive!)\n",
    "- ‚ùå Need original training data\n",
    "- ‚ùå More complex to implement\n",
    "\n",
    "---\n",
    "\n",
    "## **üéØ PART 6: PRACTICAL EXAMPLES**\n",
    "\n",
    "### **Example 1: Quantizing a Weight Matrix**\n",
    "\n",
    "**Scenario**: You have a neural network layer with weights:\n",
    "\n",
    "```\n",
    "W = [[-1.2, 0.5, 3.4],\n",
    "     [0.0, -2.1, 1.8]]\n",
    "```\n",
    "\n",
    "**Goal**: Quantize to INT8 using symmetric quantization\n",
    "\n",
    "**Step 1**: Find min/max\n",
    "- Min = -2.1\n",
    "- Max = 3.4\n",
    "\n",
    "**Step 2**: Calculate scale\n",
    "```\n",
    "Scale = (3.4 - (-2.1)) / 255 = 5.5 / 255 = 0.0216\n",
    "```\n",
    "\n",
    "**Step 3**: Quantize each weight\n",
    "```\n",
    "W_quant = round(W / 0.0216)\n",
    "\n",
    "W_quant = [[-56, 23, 157],\n",
    "           [0, -97, 83]]\n",
    "```\n",
    "\n",
    "**Step 4**: Memory saved\n",
    "- Original: 6 values √ó 4 bytes (FP32) = 24 bytes\n",
    "- Quantized: 6 values √ó 1 byte (INT8) = 6 bytes\n",
    "- **Reduction: 75%!**\n",
    "\n",
    "---\n",
    "\n",
    "### **Example 2: Why Quantization Works for LLMs**\n",
    "\n",
    "**Key Insight**: Not all weights are equally important!\n",
    "\n",
    "Research shows:\n",
    "- **90% of weights** contribute minimally to final output\n",
    "- **10% of weights** are critical\n",
    "\n",
    "**Strategy**: \n",
    "- Quantize the 90% aggressively (INT4/INT8)\n",
    "- Keep critical 10% in higher precision (FP16)\n",
    "- This is called **\"Mixed Precision Quantization\"**\n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ PART 7: REAL-WORLD APPLICATIONS**\n",
    "\n",
    "### **Scenario 1: Running Llama on Your Phone**\n",
    "\n",
    "**Original Model**: Llama 2 7B\n",
    "- FP32: 28 GB ‚ùå (Won't fit!)\n",
    "- FP16: 14 GB ‚ùå (Still too big)\n",
    "- INT8: 7 GB ‚úÖ (Barely fits)\n",
    "- INT4: 3.5 GB ‚úÖ‚úÖ (Comfortable!)\n",
    "\n",
    "**Trade-offs**:\n",
    "- FP32 ‚Üí FP16: ~0% accuracy loss\n",
    "- FP32 ‚Üí INT8: ~1-2% accuracy loss\n",
    "- FP32 ‚Üí INT4: ~3-5% accuracy loss\n",
    "\n",
    "---\n",
    "\n",
    "### **Scenario 2: Edge Device Inference**\n",
    "\n",
    "**Smartwatch running a small LLM**:\n",
    "- Available RAM: 1 GB\n",
    "- Model needs: INT4 quantization minimum\n",
    "- Batch Normalization helps maintain accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## **üßÆ PART 8: ADVANCED CONCEPTS**\n",
    "\n",
    "### **Batch Normalization Impact**\n",
    "\n",
    "Your notes mention \"Batch Normalization\" - here's why it matters:\n",
    "\n",
    "**During quantization**:\n",
    "- Weights get rounded ‚Üí slight errors\n",
    "- Batch Norm helps \"smooth out\" these errors\n",
    "- Acts as a calibration layer\n",
    "\n",
    "### **Layer-wise Quantization**\n",
    "\n",
    "Not all layers need same precision:\n",
    "- **Embedding layers**: Can use INT8 safely\n",
    "- **Attention layers**: More sensitive, use INT8 with care\n",
    "- **Output layer**: Keep in FP16 for best results\n",
    "\n",
    "### **Dynamic vs Static Quantization**\n",
    "\n",
    "**Static** (what we discussed):\n",
    "- Scale determined once during calibration\n",
    "- Fast at inference\n",
    "\n",
    "**Dynamic**:\n",
    "- Scale recalculated for each input\n",
    "- More accurate but slower\n",
    "- Used for activations (not weights)\n",
    "\n",
    "---\n",
    "\n",
    "## **üìà PART 9: MEASURING SUCCESS**\n",
    "\n",
    "### **Key Metrics**\n",
    "\n",
    "**1. Perplexity** (for language models)\n",
    "- Lower is better\n",
    "- Measures prediction quality\n",
    "- FP32 baseline: 10.5\n",
    "- INT8 quantized: 10.7 (acceptable!)\n",
    "\n",
    "**2. Model Size**\n",
    "- FP32: 100%\n",
    "- INT8: 25%\n",
    "- INT4: 12.5%\n",
    "\n",
    "**3. Inference Speed**\n",
    "- INT8 can be 2-4√ó faster\n",
    "- INT4 can be 4-8√ó faster\n",
    "\n",
    "**4. Accuracy Drop**\n",
    "- Target: < 1% for critical apps\n",
    "- Acceptable: < 5% for most apps\n",
    "\n",
    "---\n",
    "\n",
    "## **üõ†Ô∏è PART 10: IMPLEMENTATION TIPS**\n",
    "\n",
    "### **Tools and Libraries**\n",
    "\n",
    "**For PyTorch**:\n",
    "```python\n",
    "import torch.quantization\n",
    "\n",
    "# Post-training quantization\n",
    "model_quantized = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Quantization-aware training\n",
    "model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "torch.quantization.prepare_qat(model, inplace=True)\n",
    "# ... train ...\n",
    "torch.quantization.convert(model, inplace=True)\n",
    "```\n",
    "\n",
    "### **Common Pitfalls**\n",
    "\n",
    "‚ùå **Don't**: Quantize without calibration  \n",
    "‚úÖ **Do**: Use representative data for calibration\n",
    "\n",
    "‚ùå **Don't**: Use same quantization for all layers  \n",
    "‚úÖ **Do**: Profile and quantize layer-wise\n",
    "\n",
    "‚ùå **Don't**: Ignore outliers  \n",
    "‚úÖ **Do**: Use robust calibration (percentile-based)\n",
    "\n",
    "---\n",
    "\n",
    "## **üéì SUMMARY: KEY TAKEAWAYS**\n",
    "\n",
    "**Quantization is**:\n",
    "- Converting high-precision (FP32) ‚Üí low-precision (INT8/INT4)\n",
    "- Reduces memory by 2-8√ó\n",
    "- Enables LLMs on mobile/edge devices\n",
    "\n",
    "**Two Methods**:\n",
    "1. **Symmetric**: Simple, centered at zero\n",
    "2. **Asymmetric**: More accurate, uses zero-point offset\n",
    "\n",
    "**Two Approaches**:\n",
    "1. **PTQ**: Fast, post-training, some accuracy loss\n",
    "2. **QAT**: Slower, better accuracy, requires retraining\n",
    "\n",
    "**Formula to Remember**:\n",
    "```\n",
    "Q = round(X / Scale) + Zero_point\n",
    "X = (Q - Zero_point) √ó Scale\n",
    "```\n",
    "\n",
    "**Golden Rule**:\n",
    "> The goal is maximum compression with minimum accuracy loss\n",
    "\n",
    "---\n",
    "\n",
    "## **üéØ PRACTICE EXERCISE**\n",
    "\n",
    "**Try this**: Quantize these weights to INT8 (symmetric)\n",
    "```\n",
    "Weights: [-0.5, 1.2, -0.8, 2.0, 0.3]\n",
    "```\n",
    "\n",
    "**Solution**:\n",
    "1. Min = -0.8, Max = 2.0\n",
    "2. Scale = 2.8/255 = 0.011\n",
    "3. Quantized: [-45, 109, -73, 182, 27]\n",
    "\n",
    "---\n",
    "\n",
    "You're now ready to understand and implement quantization! Want me to dive deeper into any specific part?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f172e9",
   "metadata": {},
   "source": [
    "I'll create a comprehensive, intuitive guide connecting quantization to fine-tuning, LoRA, and QLoRA. This will be an epic deep dive!\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ **COMPLETE GUIDE TO LLM FINE-TUNING: From Basics to LoRA & QLoRA**\n",
    "\n",
    "---\n",
    "\n",
    "## **üìö CHAPTER 1: THE FINE-TUNING LANDSCAPE**\n",
    "\n",
    "### **ü§î What is Fine-Tuning?**\n",
    "\n",
    "Imagine you have a **brilliant general doctor** (pre-trained LLM) who knows medicine broadly. Now you want this doctor to specialize in **cardiology**. \n",
    "\n",
    "**Fine-tuning** = Taking that general knowledge and adapting it to a specific domain/task.\n",
    "\n",
    "```\n",
    "[Pre-trained Model] + [Domain-Specific Data] ‚Üí [Fine-tuned Model]\n",
    "     (General)              (Specialized)           (Expert)\n",
    "```\n",
    "\n",
    "### **Why Fine-Tune?**\n",
    "\n",
    "**Pre-trained models** (like GPT, Llama, BERT) are trained on massive general datasets:\n",
    "- Web pages\n",
    "- Books\n",
    "- Code repositories\n",
    "\n",
    "**But your needs might be**:\n",
    "- Medical diagnosis (specialized vocabulary)\n",
    "- Legal document analysis (specific formats)\n",
    "- Customer service chatbot (company-specific knowledge)\n",
    "- Code generation in your company's style\n",
    "\n",
    "**Fine-tuning advantages**:\n",
    "‚úÖ Adapts to your specific use case\n",
    "‚úÖ Better performance than prompt engineering alone\n",
    "‚úÖ Can learn your data formats and styles\n",
    "‚úÖ More cost-effective than training from scratch\n",
    "\n",
    "---\n",
    "\n",
    "## **‚ö†Ô∏è CHAPTER 2: THE FULL FINE-TUNING CRISIS**\n",
    "\n",
    "### **What is Full Parameter Fine-Tuning?**\n",
    "\n",
    "**Concept**: Update **ALL** the parameters (weights) in the model during training.\n",
    "\n",
    "```\n",
    "Model has 7 billion parameters\n",
    "‚Üì\n",
    "ALL 7 billion parameters get updated\n",
    "‚Üì\n",
    "Need to store: gradients, optimizer states, activations\n",
    "```\n",
    "\n",
    "### **üí• The Problems (Why Full Fine-Tuning is HARD)**\n",
    "\n",
    "Let's do the math for a **7B parameter model** (like Llama 2 7B):\n",
    "\n",
    "#### **Problem 1: Memory Requirements**\n",
    "\n",
    "**For Training, you need**:\n",
    "\n",
    "1. **Model weights**: 7B parameters √ó 4 bytes (FP32) = **28 GB**\n",
    "\n",
    "2. **Gradients**: Same size as weights = **28 GB**\n",
    "\n",
    "3. **Optimizer states** (Adam optimizer):\n",
    "   - First moment estimates: **28 GB**\n",
    "   - Second moment estimates: **28 GB**\n",
    "   - Total optimizer: **56 GB**\n",
    "\n",
    "4. **Activations** (forward pass intermediate values): **~10-20 GB**\n",
    "\n",
    "**TOTAL MEMORY NEEDED**: **~120-130 GB of GPU memory!** ü§Ø\n",
    "\n",
    "**Reality check**:\n",
    "- NVIDIA A100 (top-tier GPU): 80 GB\n",
    "- NVIDIA RTX 4090 (consumer): 24 GB\n",
    "- **You can't even fit it on ONE top-tier GPU!**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Problem 2: Training Time**\n",
    "\n",
    "**Full fine-tuning**:\n",
    "- Updates all 7 billion parameters\n",
    "- Every single batch requires:\n",
    "  - Forward pass through entire model\n",
    "  - Backward pass through entire model\n",
    "  - Optimizer update for all parameters\n",
    "\n",
    "**Time estimate**:\n",
    "- Single A100 GPU: **Days to weeks**\n",
    "- Cost: **$1000s-$10000s** in GPU rental\n",
    "\n",
    "---\n",
    "\n",
    "#### **Problem 3: Catastrophic Forgetting**\n",
    "\n",
    "**The phenomenon**:\n",
    "```\n",
    "Pre-trained model: Great at general tasks (90% accuracy)\n",
    "‚Üì\n",
    "Fine-tune on Task A: Great at Task A (95% accuracy)\n",
    "‚Üì\n",
    "But now terrible at general tasks (40% accuracy)!\n",
    "```\n",
    "\n",
    "**Why it happens**:\n",
    "- Full fine-tuning overwrites the general knowledge\n",
    "- The model \"forgets\" what it learned during pre-training\n",
    "- All parameters change ‚Üí general capabilities lost\n",
    "\n",
    "---\n",
    "\n",
    "#### **Problem 4: Overfitting Risk**\n",
    "\n",
    "**Scenario**: You have only 1,000 training examples\n",
    "\n",
    "**Problem**: Updating 7 billion parameters with 1,000 examples\n",
    "- **7,000,000 parameters per example!**\n",
    "- Model memorizes training data\n",
    "- Doesn't generalize to new data\n",
    "\n",
    "**Analogy**: Using a sledgehammer to crack a walnut üî®ü•ú\n",
    "\n",
    "---\n",
    "\n",
    "#### **Problem 5: Storage Nightmare**\n",
    "\n",
    "**If you fine-tune for multiple tasks**:\n",
    "\n",
    "```\n",
    "Task A: 28 GB model\n",
    "Task B: 28 GB model  \n",
    "Task C: 28 GB model\n",
    "...\n",
    "10 tasks = 280 GB storage!\n",
    "```\n",
    "\n",
    "Each task requires storing a **full copy** of the entire model.\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä The Full Fine-Tuning Reality**\n",
    "\n",
    "| Aspect | Requirement |\n",
    "|--------|-------------|\n",
    "| **GPU Memory** | 120+ GB |\n",
    "| **Training Time** | Days to weeks |\n",
    "| **Cost** | $1,000-$10,000+ |\n",
    "| **Data Needed** | 10,000+ examples |\n",
    "| **Risk** | Catastrophic forgetting, overfitting |\n",
    "| **Storage per task** | 28 GB (7B model) |\n",
    "\n",
    "**Conclusion**: Full fine-tuning is **EXPENSIVE, SLOW, RISKY, and WASTEFUL** for most use cases! üò±\n",
    "\n",
    "---\n",
    "\n",
    "## **üí° CHAPTER 3: PARAMETER-EFFICIENT FINE-TUNING (PEFT)**\n",
    "\n",
    "### **The Revolutionary Idea**\n",
    "\n",
    "**Key insight**: \n",
    "> \"What if we DON'T update all 7 billion parameters? What if we update only a TINY subset?\"\n",
    "\n",
    "**Hypothesis**: \n",
    "- Most parameters can stay frozen (unchanged)\n",
    "- Only a small subset needs adaptation\n",
    "- We can add **new, trainable parameters** specifically for adaptation\n",
    "\n",
    "### **PEFT Categories**\n",
    "\n",
    "**1. Adapter Layers** (2019)\n",
    "- Add small neural networks between transformer layers\n",
    "- Train only the adapters, freeze everything else\n",
    "\n",
    "**2. Prefix Tuning** (2021)\n",
    "- Add trainable \"prefix\" tokens to input\n",
    "- Only train these prefix embeddings\n",
    "\n",
    "**3. LoRA** (2021) ‚Üê **‚òÖ Our Focus!**\n",
    "- Add low-rank matrices to existing weights\n",
    "- Train only these small matrices\n",
    "\n",
    "**4. QLoRA** (2023) ‚Üê **‚òÖ Quantization + LoRA!**\n",
    "- Quantize the base model to 4-bit\n",
    "- Apply LoRA on top\n",
    "\n",
    "---\n",
    "\n",
    "## **üéØ CHAPTER 4: LoRA - THE GAME CHANGER**\n",
    "\n",
    "### **üìú The Paper: \"LoRA: Low-Rank Adaptation of Large Language Models\"**\n",
    "\n",
    "**Authors**: Edward Hu, Yelong Shen, et al. (Microsoft)  \n",
    "**Published**: June 2021  \n",
    "**Impact**: Revolutionary - now industry standard\n",
    "\n",
    "---\n",
    "\n",
    "### **üß† The Core Intuition**\n",
    "\n",
    "**Key Observation from Research**:\n",
    "\n",
    "Pre-trained models have **low \"intrinsic dimensionality\"** for task-specific adaptations.\n",
    "\n",
    "**Translation**: \n",
    "- The changes needed for fine-tuning are **simple/low-dimensional**\n",
    "- We don't need to update all parameters\n",
    "- We can **approximate** the weight updates with simpler math\n",
    "\n",
    "**Analogy**:\n",
    "\n",
    "Imagine you're adjusting a complex recipe:\n",
    "- **Full fine-tuning**: Change all 1,000 ingredients individually\n",
    "- **LoRA**: Realize that \"making it sweeter\" affects just a few key ingredients, even though sweetness touches everything\n",
    "\n",
    "---\n",
    "\n",
    "### **üî¨ The Mathematical Foundation**\n",
    "\n",
    "#### **Normal Fine-Tuning**\n",
    "\n",
    "In a neural network layer, we have a weight matrix **W**:\n",
    "\n",
    "```\n",
    "Original weight: W ‚àà ‚Ñù^(d√ók)\n",
    "\n",
    "After fine-tuning: W' = W + ŒîW\n",
    "\n",
    "Where ŒîW ‚àà ‚Ñù^(d√ók) is the full update matrix\n",
    "```\n",
    "\n",
    "**Problem**: ŒîW has **d √ó k parameters** to train!\n",
    "\n",
    "For a typical transformer layer:\n",
    "- d = 4096 (model dimension)\n",
    "- k = 4096 (model dimension)\n",
    "- **ŒîW has 16,777,216 parameters!** üò±\n",
    "\n",
    "---\n",
    "\n",
    "#### **LoRA's Brilliant Trick**\n",
    "\n",
    "**Key idea**: **Decompose ŒîW into two low-rank matrices**\n",
    "\n",
    "```\n",
    "ŒîW = B √ó A\n",
    "\n",
    "Where:\n",
    "- A ‚àà ‚Ñù^(r√ók)  (small matrix)\n",
    "- B ‚àà ‚Ñù^(d√ór)  (small matrix)\n",
    "- r << min(d,k)  (r is the rank, typically 8-64)\n",
    "```\n",
    "\n",
    "**Visual Representation**:\n",
    "\n",
    "```\n",
    "        Full Update Matrix              LoRA Approximation\n",
    "              ŒîW                              B √ó A\n",
    "        [d√ók matrix]                   [d√ór] √ó [r√ók]\n",
    "        \n",
    "     k columns                        k columns\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îå‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  d ‚îÇ             ‚îÇ         =       d ‚îÇ  ‚îÇ√ór‚îÇ             ‚îÇ\n",
    "    ‚îÇ   16M       ‚îÇ                   ‚îÇ  ‚îÇ  ‚îÇ             ‚îÇ\n",
    "    ‚îÇ  params     ‚îÇ                   ‚îî‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   \n",
    "                                      d√ór + r√ók params\n",
    "                                      = 2√ó4096√ó8 = 65,536 params!\n",
    "                                      (256√ó fewer parameters!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üé® The LoRA Architecture**\n",
    "\n",
    "**In a Transformer Layer**:\n",
    "\n",
    "```\n",
    "Input (x)\n",
    "   ‚îÇ\n",
    "   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ                  ‚îÇ\n",
    "   ‚îÇ              [Frozen W]        ‚Üê Original pre-trained weights\n",
    "   ‚îÇ                  ‚îÇ\n",
    "   ‚îÇ                Output (Wx)\n",
    "   ‚îÇ                  ‚îÇ\n",
    "   ‚îÇ              [+]  ‚Üê Addition\n",
    "   ‚îÇ                  ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ[A]‚îÄ‚îÄ[B]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚Üê LoRA adaptation\n",
    "       ‚Üë    ‚Üë\n",
    "    Trainable only\n",
    "\n",
    "Final Output: y = Wx + BAx = (W + BA)x\n",
    "```\n",
    "\n",
    "**Key Points**:\n",
    "\n",
    "1. **W is frozen** - never updated during fine-tuning\n",
    "2. **A and B are trainable** - these are the only parameters we update\n",
    "3. **During inference**: We can merge BA into W, so no speed penalty!\n",
    "\n",
    "---\n",
    "\n",
    "### **üìê Parameter Count Comparison**\n",
    "\n",
    "**Example: Llama 2 7B Model**\n",
    "\n",
    "#### **Full Fine-Tuning**:\n",
    "```\n",
    "Total parameters: 7,000,000,000\n",
    "Trainable: 7,000,000,000 (100%)\n",
    "```\n",
    "\n",
    "#### **LoRA (rank r=8)**:\n",
    "\n",
    "Let's calculate for one layer:\n",
    "```\n",
    "Original weight matrix W: 4096 √ó 4096 = 16,777,216 params\n",
    "\n",
    "LoRA matrices:\n",
    "- Matrix A: 8 √ó 4096 = 32,768 params\n",
    "- Matrix B: 4096 √ó 8 = 32,768 params\n",
    "- Total LoRA: 65,536 params\n",
    "\n",
    "Reduction: 16,777,216 / 65,536 = 256√ó fewer parameters!\n",
    "```\n",
    "\n",
    "**For the entire model** (applying LoRA to attention layers):\n",
    "```\n",
    "Llama 2 7B has ~32 attention layers\n",
    "Each layer gets LoRA on Query, Key, Value, Output projections\n",
    "\n",
    "Approximate trainable parameters:\n",
    "32 layers √ó 4 matrices √ó 65,536 params ‚âà 8,388,608 params\n",
    "\n",
    "Percentage trainable: 8.4M / 7,000M = 0.12%!\n",
    "```\n",
    "\n",
    "**You're only training 0.12% of the parameters!** üéâ\n",
    "\n",
    "---\n",
    "\n",
    "### **üíæ Memory Savings with LoRA**\n",
    "\n",
    "#### **Training Memory Breakdown**:\n",
    "\n",
    "**Full Fine-Tuning** (7B model):\n",
    "- Model weights: 28 GB\n",
    "- Gradients: 28 GB  \n",
    "- Optimizer states: 56 GB\n",
    "- **Total: ~120 GB**\n",
    "\n",
    "**LoRA Fine-Tuning** (7B model, r=8):\n",
    "- Frozen model weights: 28 GB (loaded once)\n",
    "- LoRA parameters: 8.4M √ó 4 bytes = 33.6 MB\n",
    "- Gradients (only for LoRA): 33.6 MB\n",
    "- Optimizer states (only for LoRA): 67.2 MB\n",
    "- **Total: ~28.13 GB** ‚ú®\n",
    "\n",
    "**Memory reduction: 120 GB ‚Üí 28 GB (4.3√ó less!)**\n",
    "\n",
    "**Now fits on a single A100 GPU!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "### **üéõÔ∏è LoRA Hyperparameters**\n",
    "\n",
    "#### **1. Rank (r)**\n",
    "\n",
    "**Definition**: The bottleneck dimension in the low-rank decomposition\n",
    "\n",
    "```\n",
    "ŒîW = B √ó A\n",
    "     [d√ór] [r√ók]\n",
    "         ‚Üë\n",
    "      This is r\n",
    "```\n",
    "\n",
    "**Typical values**: r ‚àà {1, 2, 4, 8, 16, 32, 64}\n",
    "\n",
    "**Trade-offs**:\n",
    "\n",
    "| Rank | Parameters | Expressiveness | Use Case |\n",
    "|------|-----------|----------------|----------|\n",
    "| **r=1** | Minimal | Very limited | Simple tasks |\n",
    "| **r=8** | Balanced | Good | Most tasks (sweet spot!) |\n",
    "| **r=32** | Moderate | High | Complex tasks |\n",
    "| **r=64** | More | Very high | When you have lots of data |\n",
    "\n",
    "**Rule of thumb**: Start with r=8, increase if underfitting\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Alpha (Œ±) - The Scaling Factor**\n",
    "\n",
    "**Purpose**: Controls how much the LoRA adaptation affects the output\n",
    "\n",
    "```\n",
    "Output = Wx + (Œ±/r) √ó BAx\n",
    "              ‚Üë\n",
    "         Scaling factor\n",
    "```\n",
    "\n",
    "**Typical values**: Œ± ‚àà {8, 16, 32}\n",
    "**Common choice**: Œ± = r (e.g., if r=8, then Œ±=8)\n",
    "\n",
    "**What it does**:\n",
    "- **Higher Œ±**: LoRA has more influence (adapts more aggressively)\n",
    "- **Lower Œ±**: LoRA has less influence (preserves pre-trained knowledge more)\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Target Modules**\n",
    "\n",
    "**Question**: Which weight matrices should we apply LoRA to?\n",
    "\n",
    "**In a Transformer layer, we have**:\n",
    "- Query projection: W_Q\n",
    "- Key projection: W_K  \n",
    "- Value projection: W_V\n",
    "- Output projection: W_O\n",
    "- Feed-forward layers: W_1, W_2\n",
    "\n",
    "**Options**:\n",
    "\n",
    "**Option A: LoRA on Attention only** (Common)\n",
    "```\n",
    "Apply LoRA to: W_Q, W_K, W_V, W_O\n",
    "Reasoning: Attention is where \"understanding\" happens\n",
    "Memory: Lowest\n",
    "Performance: Good for most tasks\n",
    "```\n",
    "\n",
    "**Option B: LoRA on Attention + FFN** (More aggressive)\n",
    "```\n",
    "Apply LoRA to: W_Q, W_K, W_V, W_O, W_1, W_2\n",
    "Reasoning: FFN stores \"knowledge\"\n",
    "Memory: Higher\n",
    "Performance: Better for knowledge-intensive tasks\n",
    "```\n",
    "\n",
    "**Option C: Q and V only** (Minimal)\n",
    "```\n",
    "Apply LoRA to: W_Q, W_V\n",
    "Reasoning: Research shows these are often sufficient\n",
    "Memory: Minimal\n",
    "Performance: Surprisingly good!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üî¨ Why Does LoRA Work?**\n",
    "\n",
    "#### **Theoretical Justification**\n",
    "\n",
    "**1. Low Intrinsic Rank Hypothesis**\n",
    "\n",
    "Research shows that the weight updates ŒîW during fine-tuning have **low rank**:\n",
    "\n",
    "```\n",
    "If we compute ŒîW = W_finetuned - W_pretrained\n",
    "And perform SVD (Singular Value Decomposition)\n",
    "‚Üí Most singular values are close to zero!\n",
    "‚Üí Only a few dominant singular values\n",
    "‚Üí The change is \"low-dimensional\"\n",
    "```\n",
    "\n",
    "**Analogy**: \n",
    "- Moving from NYC to Boston doesn't require 3D movement\n",
    "- It's essentially a 2D movement (north-east)\n",
    "- LoRA captures these \"principal directions\" of change\n",
    "\n",
    "---\n",
    "\n",
    "**2. Overparameterization of Neural Networks**\n",
    "\n",
    "**Discovery**: Deep learning models are **massively overparameterized**\n",
    "\n",
    "```\n",
    "7 billion parameters to learn patterns in text?\n",
    "Turns out, you need far fewer degrees of freedom!\n",
    "```\n",
    "\n",
    "**LoRA exploits this**: Most of the network's capacity is already there in the pre-trained weights.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Empirical Success**\n",
    "\n",
    "The paper shows LoRA matches or exceeds full fine-tuning on:\n",
    "- Natural language understanding (GLUE)\n",
    "- Question answering (SQuAD)\n",
    "- Natural language generation (E2E)\n",
    "\n",
    "**With just 0.1% of trainable parameters!**\n",
    "\n",
    "---\n",
    "\n",
    "## **üéØ CHAPTER 5: THE CONNECTION - Quantization + LoRA**\n",
    "\n",
    "Now we connect to our earlier quantization knowledge!\n",
    "\n",
    "### **The Problem LoRA Doesn't Solve**\n",
    "\n",
    "LoRA reduces **trainable parameters**, but:\n",
    "\n",
    "‚ùå Still needs to load the **full base model** (28 GB for 7B model)  \n",
    "‚ùå Still needs high precision for stable training  \n",
    "‚ùå Still requires expensive GPUs\n",
    "\n",
    "**Question**: Can we make it even more efficient?\n",
    "\n",
    "---\n",
    "\n",
    "### **üíé Enter QLoRA: Quantized LoRA**\n",
    "\n",
    "**Paper**: \"QLoRA: Efficient Finetuning of Quantized LLMs\" (2023)  \n",
    "**Authors**: Tim Dettmers et al.\n",
    "\n",
    "**Revolutionary idea**: \n",
    "> Combine quantization with LoRA to fine-tune on a single consumer GPU!\n",
    "\n",
    "---\n",
    "\n",
    "### **üîß QLoRA Architecture**\n",
    "\n",
    "```\n",
    "                QLoRA Stack\n",
    "                \n",
    "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇ  4-bit Quantized Base Model     ‚îÇ  ‚Üê Frozen, compressed\n",
    "     ‚îÇ     (NormalFloat 4-bit)         ‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚îÇ\n",
    "                    ‚Üì\n",
    "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇ    LoRA Adapters (FP16)         ‚îÇ  ‚Üê Trainable, high precision\n",
    "     ‚îÇ      (Rank r=16-64)             ‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚îÇ\n",
    "                    ‚Üì\n",
    "              Task Output\n",
    "```\n",
    "\n",
    "**Key innovations**:\n",
    "\n",
    "1. **Base model in 4-bit** (NF4 - NormalFloat 4-bit)\n",
    "2. **LoRA adapters in FP16** (high precision for training stability)\n",
    "3. **Double quantization** (even quantize the quantization parameters!)\n",
    "4. **Paged optimizers** (better memory management)\n",
    "\n",
    "---\n",
    "\n",
    "### **üé® QLoRA Technical Details**\n",
    "\n",
    "#### **Innovation 1: NormalFloat 4-bit (NF4)**\n",
    "\n",
    "**Problem with standard INT4**:\n",
    "- Neural network weights follow a **normal distribution** (bell curve)\n",
    "- Most values are near zero\n",
    "- INT4 wastes bins on extreme values\n",
    "\n",
    "**NF4 Solution**: \n",
    "```\n",
    "Design quantization bins specifically for normal distribution!\n",
    "\n",
    "Instead of: [-8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7]\n",
    "           (uniform spacing)\n",
    "\n",
    "Use: [-1.0, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0, ...]\n",
    "     (denser near zero, where most weights are!)\n",
    "```\n",
    "\n",
    "**Benefit**: Better representation of weight distribution ‚Üí less accuracy loss\n",
    "\n",
    "---\n",
    "\n",
    "#### **Innovation 2: Double Quantization**\n",
    "\n",
    "**Recall from quantization**:\n",
    "```\n",
    "We need to store:\n",
    "- Quantized values (4-bit)\n",
    "- Scale factors (32-bit) ‚Üê These add up!\n",
    "```\n",
    "\n",
    "**For a 7B model**:\n",
    "- If we use block size = 64\n",
    "- Number of blocks = 7B / 64 ‚âà 109 million blocks\n",
    "- Scale storage = 109M √ó 4 bytes = **436 MB** of overhead!\n",
    "\n",
    "**Double Quantization**:\n",
    "```\n",
    "Quantize the scale factors themselves!\n",
    "- Block-level scales: Quantize to 8-bit\n",
    "- Global scale: Keep in FP32\n",
    "- Now: 109M √ó 1 byte = 109 MB ‚ú®\n",
    "```\n",
    "\n",
    "**Memory saved**: 436 MB ‚Üí 109 MB (4√ó reduction)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Innovation 3: Paged Optimizers**\n",
    "\n",
    "**Problem**: Optimizer states (Adam) require lots of memory\n",
    "\n",
    "**Solution**: Borrow from OS virtual memory:\n",
    "- Store optimizer states in CPU RAM\n",
    "- Page them to GPU memory when needed\n",
    "- Use NVIDIA Unified Memory for automatic transfers\n",
    "\n",
    "**Benefit**: Train larger models on smaller GPUs\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä QLoRA vs LoRA vs Full Fine-Tuning**\n",
    "\n",
    "**Example: Llama 2 7B on a single RTX 4090 (24 GB)**\n",
    "\n",
    "| Method | Base Model Precision | Memory Required | Can Fit? | Trainable Params |\n",
    "|--------|---------------------|-----------------|----------|------------------|\n",
    "| **Full FT** | FP32 | 120+ GB | ‚ùå No | 7B (100%) |\n",
    "| **LoRA** | FP16 | 28 GB | ‚ùå No | 8.4M (0.12%) |\n",
    "| **QLoRA** | NF4 | ~12 GB | ‚úÖ Yes! | 8.4M (0.12%) |\n",
    "\n",
    "**QLoRA memory breakdown**:\n",
    "- Base model (4-bit): 7B √ó 0.5 bytes = **3.5 GB**\n",
    "- LoRA parameters: 8.4M √ó 2 bytes = **16.8 MB**\n",
    "- Gradients: **16.8 MB**\n",
    "- Optimizer states: **33.6 MB**\n",
    "- Activations: **~5-8 GB**\n",
    "- **Total: ~10-12 GB** ‚ú®\n",
    "\n",
    "**You can now fine-tune a 7B model on a consumer GPU!** üéâ\n",
    "\n",
    "---\n",
    "\n",
    "### **üîó How Calibration Connects**\n",
    "\n",
    "Remember **calibration** from quantization?\n",
    "\n",
    "**In QLoRA, calibration is CRITICAL**:\n",
    "\n",
    "```\n",
    "Step 1: Load pre-trained model in FP16\n",
    "Step 2: Use calibration data to find optimal quantization parameters\n",
    "        ‚Üí Compute scale factors for each block\n",
    "        ‚Üí Find NF4 bin boundaries optimized for weight distribution\n",
    "Step 3: Quantize base model to NF4\n",
    "Step 4: Freeze quantized base model\n",
    "Step 5: Add LoRA adapters\n",
    "Step 6: Fine-tune only the LoRA adapters\n",
    "```\n",
    "\n",
    "**The calibration step** ensures:\n",
    "- Minimal accuracy loss from quantization\n",
    "- Optimal representation of pre-trained knowledge\n",
    "- Stable foundation for LoRA training\n",
    "\n",
    "---\n",
    "\n",
    "## **‚öñÔ∏è CHAPTER 6: COMPREHENSIVE COMPARISONS**\n",
    "\n",
    "### **üìä Training Comparison**\n",
    "\n",
    "| Aspect | Full Fine-Tuning | LoRA | QLoRA |\n",
    "|--------|------------------|------|-------|\n",
    "| **Trainable Params** | 7B (100%) | 8.4M (0.12%) | 8.4M (0.12%) |\n",
    "| **GPU Memory** | 120+ GB | 28 GB | 10-12 GB |\n",
    "| **GPU Required** | 8√ó A100 (80GB) | 1√ó A100 (80GB) | 1√ó RTX 4090 (24GB) |\n",
    "| **Training Time** | Baseline (1√ó) | ~1.2√ó | ~1.3√ó |\n",
    "| **Storage per Task** | 28 GB | 33 MB | 33 MB |\n",
    "| **Inference Speed** | Baseline | Baseline* | Slightly slower** |\n",
    "| **Accuracy** | 100% (baseline) | 99-100% | 98-100% |\n",
    "| **Cost** | $$$$$$ | $$$ | $ |\n",
    "\n",
    "*Can merge LoRA weights into base model for zero overhead  \n",
    "**Due to dequantization, unless you use quantized inference\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ When to Use Each Method**\n",
    "\n",
    "#### **Use Full Fine-Tuning When**:\n",
    "‚úÖ You have massive compute resources  \n",
    "‚úÖ You need absolute best performance  \n",
    "‚úÖ You're adapting to a completely different domain  \n",
    "‚úÖ You have 100K+ training examples  \n",
    "‚úÖ Budget is not a concern\n",
    "\n",
    "**Example**: OpenAI fine-tuning GPT-4 for specific enterprise clients\n",
    "\n",
    "---\n",
    "\n",
    "#### **Use LoRA When**:\n",
    "‚úÖ You want good performance with efficiency  \n",
    "‚úÖ You need to deploy multiple task-specific models  \n",
    "‚úÖ You have moderate GPU resources (A100)  \n",
    "‚úÖ You want to preserve base model's general capabilities  \n",
    "‚úÖ You have 1K-100K training examples\n",
    "\n",
    "**Example**: Creating chatbots for different departments in a company\n",
    "\n",
    "---\n",
    "\n",
    "#### **Use QLoRA When**:\n",
    "‚úÖ You have limited GPU resources (consumer GPUs)  \n",
    "‚úÖ You want to experiment rapidly  \n",
    "‚úÖ You're fine with slightly lower accuracy  \n",
    "‚úÖ You need to fine-tune very large models (13B, 30B, 65B)  \n",
    "‚úÖ Budget is tight\n",
    "\n",
    "**Example**: Researchers, startups, personal projects\n",
    "\n",
    "---\n",
    "\n",
    "## **üî¨ CHAPTER 7: ADVANCED TOPICS & TRICKS**\n",
    "\n",
    "### **1. LoRA Rank Selection Strategy**\n",
    "\n",
    "**Empirical findings** from the paper:\n",
    "\n",
    "```python\n",
    "Task Complexity ‚Üí Optimal Rank\n",
    "\n",
    "Simple classification (sentiment): r=4-8\n",
    "Question answering: r=8-16\n",
    "Complex reasoning: r=16-32\n",
    "Code generation: r=32-64\n",
    "Multimodal tasks: r=64-128\n",
    "```\n",
    "\n",
    "**How to find optimal rank**:\n",
    "\n",
    "```\n",
    "1. Start with r=8 (good default)\n",
    "2. Train for a few epochs\n",
    "3. If underfitting (poor performance):\n",
    "   ‚Üí Increase r to 16, then 32\n",
    "4. If overfitting (training loss << validation loss):\n",
    "   ‚Üí Decrease r to 4\n",
    "   ‚Üí Or use dropout/regularization\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Target Module Selection**\n",
    "\n",
    "**Research-backed strategies**:\n",
    "\n",
    "**Strategy A: Query & Value** (Minimal, 2012 paper recommendation)\n",
    "```python\n",
    "target_modules = [\"q_proj\", \"v_proj\"]\n",
    "# Rationale: Q determines attention, V contains information\n",
    "# Parameters: ~50% of full LoRA\n",
    "# Performance: ~95% of full LoRA performance\n",
    "```\n",
    "\n",
    "**Strategy B: All Attention** (Balanced)\n",
    "```python\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "# Rationale: Complete attention adaptation\n",
    "# Parameters: Baseline\n",
    "# Performance: Best for most tasks\n",
    "```\n",
    "\n",
    "**Strategy C: Attention + FFN** (Aggressive)\n",
    "```python\n",
    "target_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "# Rationale: Adapt both attention and knowledge storage\n",
    "# Parameters: ~2√ó baseline\n",
    "# Performance: Best for knowledge-intensive tasks\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Combining Multiple LoRAs**\n",
    "\n",
    "**Scenario**: You have multiple task-specific LoRAs\n",
    "\n",
    "**Option A: Sequential**\n",
    "```\n",
    "Base Model ‚Üí LoRA_task1 ‚Üí LoRA_task2 ‚Üí Output\n",
    "Problem: Second LoRA might conflict with first\n",
    "```\n",
    "\n",
    "**Option B: Weighted Sum**\n",
    "```\n",
    "Output = W¬∑x + Œ±‚ÇÅ¬∑(B‚ÇÅA‚ÇÅ)¬∑x + Œ±‚ÇÇ¬∑(B‚ÇÇA‚ÇÇ)¬∑x\n",
    "Where Œ±‚ÇÅ, Œ±‚ÇÇ control contribution of each LoRA\n",
    "```\n",
    "\n",
    "**Option C: Dynamic Selection** (Router)\n",
    "```\n",
    "Train a small classifier to select which LoRA(s) to activate\n",
    "Based on input type\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Initialization Strategies**\n",
    "\n",
    "**From the LoRA paper**:\n",
    "\n",
    "**Matrix A**: \n",
    "```python\n",
    "# Random Gaussian initialization\n",
    "A ~ N(0, œÉ¬≤)\n",
    "# Where œÉ is calculated to preserve variance\n",
    "```\n",
    "\n",
    "**Matrix B**: \n",
    "```python\n",
    "# Zero initialization\n",
    "B = 0\n",
    "# Rationale: At start, ŒîW = BA = 0¬∑A = 0\n",
    "# So model starts as the pre-trained model\n",
    "# Learns adaptations gradually\n",
    "```\n",
    "\n",
    "**This is brilliant**! üéØ\n",
    "- Training starts with pre-trained weights intact\n",
    "- No sudden disruption\n",
    "- Smooth adaptation\n",
    "\n",
    "---\n",
    "\n",
    "## **üìà CHAPTER 8: EMPIRICAL RESULTS & EVALUATION**\n",
    "\n",
    "### **üèÜ Results from the LoRA Paper**\n",
    "\n",
    "#### **Test 1: GLUE Benchmark** (Natural Language Understanding)\n",
    "\n",
    "**Setup**: Fine-tune GPT-3 175B on GLUE tasks\n",
    "\n",
    "| Method | Trainable Params | Avg Score | Memory |\n",
    "|--------|------------------|-----------|---------|\n",
    "| **Fine-Tuning** | 175B (100%) | 89.5 | 700+ GB |\n",
    "| **Adapter Layers** | 40M (0.023%) | 88.2 | 350 GB |\n",
    "| **Prefix Tuning** | 20M (0.011%) | 87.1 | 350 GB |\n",
    "| **LoRA (r=8)** | 22M (0.013%) | 89.7 | 350 GB |\n",
    "\n",
    "**Key finding**: LoRA **outperforms** full fine-tuning with **0.013%** of parameters! ü§Ø\n",
    "\n",
    "---\n",
    "\n",
    "#### **Test 2: GPT-3 Instruction Following**\n",
    "\n",
    "**Task**: Make GPT-3 follow instructions better\n",
    "\n",
    "| Method | Examples Needed | Success Rate | Storage |\n",
    "|--------|----------------|--------------|---------|\n",
    "| **Few-shot prompting** | N/A | 62% | 0 |\n",
    "| **Fine-tuning** | 10K | 78% | 700 GB |\n",
    "| **LoRA (r=16)** | 10K | 79% | 75 MB |\n",
    "\n",
    "**Amazing**: LoRA matches fine-tuning with **9,333√ó less storage**!\n",
    "\n",
    "---\n",
    "\n",
    "#### **Test 3: Rank Sensitivity Analysis**\n",
    "\n",
    "**Question**: How does rank (r) affect performance?\n",
    "\n",
    "**Experiment**: Vary r from 1 to 256 on GPT-3\n",
    "\n",
    "```\n",
    "Results:\n",
    "r=1:   85.2% (underfitting)\n",
    "r=2:   87.1%\n",
    "r=4:   88.5%\n",
    "r=8:   89.7% ‚Üê Sweet spot!\n",
    "r=16:  89.8%\n",
    "r=32:  89.8%\n",
    "r=64:  89.7% (diminishing returns)\n",
    "r=256: 89.6% (slightly worse!)\n",
    "```\n",
    "\n",
    "**Insight**: \n",
    "- r=8 is often sufficient\n",
    "- Beyond r=32, little improvement\n",
    "- Very high r can actually hurt (overfitting)\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä QLoRA Results**\n",
    "\n",
    "**Test: Llama 2 65B on Alpaca Dataset**\n",
    "\n",
    "| Method | Base Model Size | Trainable | GPU | Accuracy | Cost |\n",
    "|--------|----------------|-----------|-----|----------|------|\n",
    "| **Full FT** | FP32 (260GB) | 65B | 32√ó A100 | 54.2% | $10,000+ |\n",
    "| **LoRA** | FP16 (130GB) | 33M | 8√ó A100 | 53.9% | $2,500 |\n",
    "| **QLoRA** | NF4 (33GB) | 33M | 1√ó A100 | 53.5% | $300 |\n",
    "\n",
    "**Shocking**: \n",
    "- QLoRA on **1 GPU** ‚âà Full fine-tuning on **32 GPUs**\n",
    "- **33√ó cheaper**\n",
    "- Only **0.7% accuracy drop**\n",
    "\n",
    "---\n",
    "\n",
    "## **üõ†Ô∏è CHAPTER 9: PRACTICAL IMPLEMENTATION**\n",
    "\n",
    "### **Code Example: LoRA with Hugging Face PEFT**\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "# 1. Load base model\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # Task type\n",
    "    r=8,                            # Rank\n",
    "    lora_alpha=16,                  # Scaling factor\n",
    "    lora_dropout=0.05,              # Dropout for regularization\n",
    "    target_modules=[                # Which layers to adapt\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\"\n",
    "    ],\n",
    "    bias=\"none\"                     # Don't train biases\n",
    ")\n",
    "\n",
    "# 3. Wrap model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 4. Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "# Output: trainable params: 8,388,608 || all params: 6,738,415,616 || trainable%: 0.124%\n",
    "\n",
    "# 5. Train (use standard HuggingFace Trainer)\n",
    "# ... your training code ...\n",
    "\n",
    "# 6. Save LoRA adapters (only ~33MB!)\n",
    "model.save_pretrained(\"./lora-adapters\")\n",
    "\n",
    "# 7. Load for inference\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./lora-adapters\")\n",
    "\n",
    "# 8. Generate text\n",
    "input_ids = tokenizer(\"Hello, how are\", return_tensors=\"pt\").input_ids\n",
    "outputs = lora_model.generate(input_ids, max_length=50)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Example: QLoRA**\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# 1. Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,              # Use 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True, # Double quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",      # Use NormalFloat 4-bit\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Compute in BF16\n",
    ")\n",
    "\n",
    "# 2. Load quantized base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 3. Prepare for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 4. Configure LoRA (same as before)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 5. Add LoRA adapters\n",
    "from peft import get_peft_model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Now you can train on a single RTX 4090!\n",
    "print(f\"Model memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "# Output: Model memory: ~10.5 GB\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **‚ö†Ô∏è CHAPTER 10: PITFALLS & BEST PRACTICES**\n",
    "\n",
    "### **‚ùå Common Mistakes**\n",
    "\n",
    "#### **Mistake 1: Rank Too High**\n",
    "\n",
    "```python\n",
    "# BAD: Using unnecessarily high rank\n",
    "lora_config = LoraConfig(r=256)  # Overkill!\n",
    "\n",
    "Problem: \n",
    "- Wastes memory\n",
    "- Overfitting risk\n",
    "- Slower training\n",
    "- No accuracy gain\n",
    "\n",
    "Solution: Start with r=8, increase only if needed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Mistake 2: Wrong Target Modules**\n",
    "\n",
    "```python\n",
    "# BAD: Applying LoRA to embeddings or layer norms\n",
    "target_modules = [\"embed_tokens\", \"norm\"]\n",
    "\n",
    "Problem:\n",
    "- Embeddings are small, LoRA doesn't help\n",
    "- Layer norms are not linear transformations\n",
    "- Wastes parameters\n",
    "\n",
    "Solution: Target attention and FFN linear layers only\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Mistake 3: Ignoring Alpha Scaling**\n",
    "\n",
    "```python\n",
    "# BAD: Not setting alpha properly\n",
    "lora_config = LoraConfig(r=8, lora_alpha=1)\n",
    "\n",
    "Problem:\n",
    "- LoRA contribution too small\n",
    "- Model doesn't adapt well\n",
    "\n",
    "Solution: Use alpha = r or alpha = 2*r as starting point\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Mistake 4: Training All Layers Equally**\n",
    "\n",
    "```python\n",
    "# SUBOPTIMAL: Same LoRA config for all layers\n",
    "# Better approach: Layer-specific ranks\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Later layers (close to output) need more adaptation\n",
    "lora_config = {\n",
    "    \"early_layers\": LoraConfig(r=4),\n",
    "    \"middle_layers\": LoraConfig(r=8),\n",
    "    \"late_layers\": LoraConfig(r=16)\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **‚úÖ Best Practices**\n",
    "\n",
    "#### **1. Data Preparation**\n",
    "\n",
    "```python\n",
    "# Good practice: Prepare diverse, high-quality data\n",
    "# Bad: Using only 100 examples\n",
    "# Good: Using 1,000-10,000 diverse examples\n",
    "\n",
    "# Format data properly\n",
    "train_data = [\n",
    "    {\n",
    "        \"instruction\": \"Summarize this article\",\n",
    "        \"input\": \"Long article text...\",\n",
    "        \"output\": \"Summary...\"\n",
    "    },\n",
    "    # ... more examples\n",
    "]\n",
    "\n",
    "# Include edge cases\n",
    "# Include negative examples\n",
    "# Balance across categories\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Gradual Rank Increase**\n",
    "\n",
    "```python\n",
    "# Strategy: Start small, increase as needed\n",
    "\n",
    "# Phase 1: Quick check with r=4\n",
    "train(epochs=1, rank=4)\n",
    "# If underfitting ‚Üí increase\n",
    "\n",
    "# Phase 2: Main training with r=8\n",
    "train(epochs=5, rank=8)\n",
    "# If still underfitting ‚Üí increase\n",
    "\n",
    "# Phase 3: Final push with r=16 (if needed)\n",
    "train(epochs=3, rank=16)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Monitoring Training**\n",
    "\n",
    "```python\n",
    "# Track these metrics:\n",
    "\n",
    "metrics_to_watch = {\n",
    "    \"train_loss\": \"Should decrease steadily\",\n",
    "    \"val_loss\": \"Should decrease, watch for divergence\",\n",
    "    \"perplexity\": \"Lower is better\",\n",
    "    \"val_accuracy\": \"Main goal metric\",\n",
    "    \"gradient_norm\": \"Watch for exploding gradients\"\n",
    "}\n",
    "\n",
    "# Red flags:\n",
    "# - Val loss increasing while train loss decreasing ‚Üí Overfitting\n",
    "# - Both losses stuck ‚Üí Learning rate too low or rank too small\n",
    "# - Losses oscillating wildly ‚Üí Learning rate too high\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Learning Rate Selection**\n",
    "\n",
    "```python\n",
    "# LoRA learning rates differ from full fine-tuning!\n",
    "\n",
    "# Full fine-tuning: 1e-5 to 5e-5\n",
    "# LoRA: 1e-4 to 3e-4 (10√ó higher!)\n",
    "# QLoRA: 2e-4 to 5e-4\n",
    "\n",
    "# Why higher?\n",
    "# - Only training a small subset of parameters\n",
    "# - These parameters start from zero (matrix B)\n",
    "# - Need stronger signal to learn quickly\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=2e-4,  # Higher than full fine-tuning\n",
    "    weight_decay=0.01\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Inference Optimization**\n",
    "\n",
    "```python\n",
    "# After training, merge LoRA weights for faster inference\n",
    "\n",
    "# Option A: Merge during inference\n",
    "model = model.merge_and_unload()\n",
    "# Now LoRA is baked into base weights\n",
    "# No computational overhead!\n",
    "\n",
    "# Option B: Keep separate for flexibility\n",
    "# Can load different LoRAs for different tasks\n",
    "base_model = load_base_model()\n",
    "lora_task_a = PeftModel.from_pretrained(base_model, \"lora_task_a\")\n",
    "lora_task_b = PeftModel.from_pretrained(base_model, \"lora_task_b\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **üéØ CHAPTER 11: ADVANCED EVALUATION STRATEGIES**\n",
    "\n",
    "### **Evaluation Metrics**\n",
    "\n",
    "#### **1. Perplexity** (For Language Models)\n",
    "\n",
    "```python\n",
    "def calculate_perplexity(model, test_data):\n",
    "    \"\"\"\n",
    "    Lower perplexity = better model\n",
    "    Measures how \"surprised\" the model is\n",
    "    \"\"\"\n",
    "    loss = evaluate_model(model, test_data)\n",
    "    perplexity = torch.exp(loss)\n",
    "    return perplexity\n",
    "\n",
    "# Example results:\n",
    "# Base model: Perplexity = 12.5\n",
    "# LoRA (r=8): Perplexity = 12.7 (acceptable!)\n",
    "# LoRA (r=32): Perplexity = 12.6\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Task-Specific Metrics**\n",
    "\n",
    "```python\n",
    "# For different tasks:\n",
    "\n",
    "tasks = {\n",
    "    \"Classification\": [\"accuracy\", \"f1_score\", \"precision\", \"recall\"],\n",
    "    \"Generation\": [\"BLEU\", \"ROUGE\", \"BERTScore\"],\n",
    "    \"Q&A\": [\"exact_match\", \"f1_score\"],\n",
    "    \"Summarization\": [\"ROUGE-L\", \"factual_consistency\"],\n",
    "    \"Code\": [\"pass@k\", \"compilation_rate\"]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Comparing Adaptations**\n",
    "\n",
    "```python\n",
    "# A/B Test: LoRA vs Full Fine-Tuning\n",
    "\n",
    "results = {\n",
    "    \"Full FT\": {\n",
    "        \"accuracy\": 0.945,\n",
    "        \"params\": \"7B\",\n",
    "        \"memory\": \"120 GB\",\n",
    "        \"time\": \"48 hours\",\n",
    "        \"cost\": \"$5000\"\n",
    "    },\n",
    "    \"LoRA (r=8)\": {\n",
    "        \"accuracy\": 0.942,  # Only 0.3% drop!\n",
    "        \"params\": \"8.4M\",\n",
    "        \"memory\": \"28 GB\",\n",
    "        \"time\": \"6 hours\",\n",
    "        \"cost\": \"$300\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Efficiency metric:\n",
    "efficiency = accuracy / (cost * time)\n",
    "# LoRA wins by huge margin!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Ablation Studies**\n",
    "\n",
    "**Question**: What contributes to LoRA's performance?\n",
    "\n",
    "#### **Test 1: Rank Ablation**\n",
    "\n",
    "```python\n",
    "# Train with different ranks, measure accuracy\n",
    "\n",
    "ranks = [1, 2, 4, 8, 16, 32, 64]\n",
    "results = {\n",
    "    1:  {\"acc\": 0.823, \"params\": \"1M\"},\n",
    "    2:  {\"acc\": 0.871, \"params\": \"2M\"},\n",
    "    4:  {\"acc\": 0.912, \"params\": \"4M\"},\n",
    "    8:  {\"acc\": 0.942, \"params\": \"8M\"},  # Sweet spot\n",
    "    16: {\"acc\": 0.945, \"params\": \"16M\"},\n",
    "    32: {\"acc\": 0.946, \"params\": \"32M\"},\n",
    "    64: {\"acc\": 0.944, \"params\": \"64M\"}  # Overfitting!\n",
    "}\n",
    "\n",
    "# Insight: r=8-16 is the sweet spot\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Test 2: Module Ablation**\n",
    "\n",
    "```python\n",
    "# Which modules benefit most from LoRA?\n",
    "\n",
    "experiments = {\n",
    "    \"Q only\": {\"acc\": 0.901, \"params\": \"2M\"},\n",
    "    \"K only\": {\"acc\": 0.897, \"params\": \"2M\"},\n",
    "    \"V only\": {\"acc\": 0.905, \"params\": \"2M\"},\n",
    "    \"Q+V\": {\"acc\": 0.928, \"params\": \"4M\"},  # Good tradeoff\n",
    "    \"Q+K+V\": {\"acc\": 0.937, \"params\": \"6M\"},\n",
    "    \"Q+K+V+O\": {\"acc\": 0.942, \"params\": \"8M\"},  # Full attention\n",
    "    \"All (attn+FFN)\": {\"acc\": 0.946, \"params\": \"14M\"}\n",
    "}\n",
    "\n",
    "# Insight: Q+V gives 95% of full performance with 50% params\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **üåü CHAPTER 12: FUTURE DIRECTIONS & RESEARCH**\n",
    "\n",
    "### **Active Research Areas**\n",
    "\n",
    "#### **1. Adaptive LoRA (AdaLoRA)**\n",
    "\n",
    "**Idea**: Dynamically adjust rank during training\n",
    "\n",
    "```python\n",
    "# Start with high rank\n",
    "# Gradually prune less important dimensions\n",
    "# End with optimal rank\n",
    "\n",
    "r_initial = 64\n",
    "r_final = 8\n",
    "\n",
    "# SVD-based pruning of LoRA matrices\n",
    "# Keep only top-k singular values\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. LoRA+ **\n",
    "\n",
    "**Improvement**: Use different learning rates for A and B\n",
    "\n",
    "```python\n",
    "# Original LoRA: same LR for both\n",
    "# LoRA+: Higher LR for B, lower for A\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': A_params, 'lr': 1e-4},\n",
    "    {'params': B_params, 'lr': 3e-4}  # 3√ó higher\n",
    "])\n",
    "\n",
    "# Result: Faster convergence, better performance\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Multi-Task LoRA**\n",
    "\n",
    "**Challenge**: One model, multiple tasks\n",
    "\n",
    "```python\n",
    "# Approach: Task-specific LoRA modules\n",
    "class MultiTaskLoRA(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.base_model = load_model()\n",
    "        self.task_routers = {\n",
    "            \"summarization\": LoRA(r=8),\n",
    "            \"translation\": LoRA(r=16),\n",
    "            \"qa\": LoRA(r=8)\n",
    "        }\n",
    "    \n",
    "    def forward(self, x, task):\n",
    "        base_output = self.base_model(x)\n",
    "        task_lora = self.task_routers[task]\n",
    "        return base_output + task_lora(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Ultra-Low Rank LoRA**\n",
    "\n",
    "**Question**: Can we go below r=4?\n",
    "\n",
    "```python\n",
    "# Research: Using r=1 with clever initialization\n",
    "# Inspired by lottery ticket hypothesis\n",
    "# Find the \"winning ticket\" direction\n",
    "\n",
    "# Potential: 1000√ó fewer parameters than full fine-tuning!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **üìö CHAPTER 13: COMPREHENSIVE SUMMARY**\n",
    "\n",
    "### **üéØ The Big Picture**\n",
    "\n",
    "```\n",
    "Evolution of Fine-Tuning:\n",
    "\n",
    "1. Full Fine-Tuning (2018)\n",
    "   ‚Üì Too expensive, catastrophic forgetting\n",
    "   \n",
    "2. Adapter Layers (2019)\n",
    "   ‚Üì Better, but still adds latency\n",
    "   \n",
    "3. Prefix Tuning (2021)\n",
    "   ‚Üì Efficient, but limited expressiveness\n",
    "   \n",
    "4. LoRA (2021) ‚òÖ\n",
    "   ‚Üì Efficient + Expressive + No latency\n",
    "   \n",
    "5. QLoRA (2023) ‚òÖ‚òÖ\n",
    "   ‚Üì Works on consumer hardware!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üîë Key Takeaways**\n",
    "\n",
    "#### **About LoRA**:\n",
    "\n",
    "1. **Core idea**: Low-rank decomposition of weight updates\n",
    "   ```\n",
    "   ŒîW = BA where B ‚àà ‚Ñù^(d√ór), A ‚àà ‚Ñù^(r√ók), r << d,k\n",
    "   ```\n",
    "\n",
    "2. **Benefits**:\n",
    "   - 99% fewer trainable parameters\n",
    "   - 4√ó less memory\n",
    "   - Mergeable weights (no inference overhead)\n",
    "   - Task-specific adapters (modular)\n",
    "\n",
    "3. **Hyperparameters**:\n",
    "   - Rank r: Start with 8\n",
    "   - Alpha Œ±: Set to r or 2r\n",
    "   - Target modules: Q, K, V, O (attention layers)\n",
    "\n",
    "4. **When to use**:\n",
    "   - Limited GPU resources\n",
    "   - Multiple task deployments\n",
    "   - Need to preserve base model\n",
    "   - Have 1K+ training examples\n",
    "\n",
    "---\n",
    "\n",
    "#### **About QLoRA**:\n",
    "\n",
    "1. **Innovations**:\n",
    "   - NormalFloat 4-bit quantization\n",
    "   - Double quantization\n",
    "   - Paged optimizers\n",
    "   - 4-bit base + FP16 adapters\n",
    "\n",
    "2. **Benefits**:\n",
    "   - Fine-tune 65B models on 1 GPU\n",
    "   - 10√ó less memory than LoRA\n",
    "   - 30√ó cheaper than full fine-tuning\n",
    "   - Minimal accuracy loss (~1%)\n",
    "\n",
    "3. **When to use**:\n",
    "   - Consumer GPUs (RTX 4090, etc.)\n",
    "   - Very large models (13B+)\n",
    "   - Experimentation and research\n",
    "   - Budget constraints\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä Decision Matrix**\n",
    "\n",
    "```\n",
    "Choose based on:\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                                                     ‚îÇ\n",
    "‚îÇ  Have 8√ó A100 GPUs? ‚Üí Full Fine-Tuning or LoRA    ‚îÇ\n",
    "‚îÇ                                                     ‚îÇ\n",
    "‚îÇ  Have 1√ó A100 GPU? ‚Üí LoRA                          ‚îÇ\n",
    "‚îÇ                                                     ‚îÇ\n",
    "‚îÇ  Have RTX 4090? ‚Üí QLoRA                            ‚îÇ\n",
    "‚îÇ                                                     ‚îÇ\n",
    "‚îÇ  Have 1K examples? ‚Üí LoRA/QLoRA                    ‚îÇ\n",
    "‚îÇ                                                     ‚îÇ\n",
    "‚îÇ  Have 100K examples? ‚Üí Any method                  ‚îÇ\n",
    "‚îÇ                                                     ‚îÇ\n",
    "‚îÇ  Need best accuracy? ‚Üí Full FT or LoRA (r=32)     ‚îÇ\n",
    "‚îÇ                                                     ‚îÇ\n",
    "‚îÇ  Need efficiency? ‚Üí QLoRA                          ‚îÇ\n",
    "‚îÇ                                                     ‚îÇ\n",
    "‚îÇ  Multiple tasks? ‚Üí LoRA (modular adapters)         ‚îÇ\n",
    "‚îÇ                                                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üéì Connecting to Quantization**\n",
    "\n",
    "**The Beautiful Synergy**:\n",
    "\n",
    "```\n",
    "Quantization (from earlier):\n",
    "- Compresses model from FP32 ‚Üí INT4\n",
    "- Reduces memory by 8√ó\n",
    "- Makes inference efficient\n",
    "- But doesn't help training much alone\n",
    "\n",
    "+\n",
    "\n",
    "LoRA:\n",
    "- Reduces trainable parameters by 1000√ó\n",
    "- Makes training efficient\n",
    "- But base model still needs memory\n",
    "\n",
    "=\n",
    "\n",
    "QLoRA (The Perfect Combination):\n",
    "- Quantized base model (4-bit) ‚Üí Saves memory ‚úì\n",
    "- LoRA adapters (FP16) ‚Üí Efficient training ‚úì\n",
    "- Result: Train 65B models on consumer GPUs! ‚úì‚úì‚úì\n",
    "```\n",
    "\n",
    "**The calibration step** is crucial:\n",
    "- Ensures quantized base model is accurate\n",
    "- Provides stable foundation for LoRA\n",
    "- Minimizes accuracy loss from compression\n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ CHAPTER 14: PRACTICAL RECIPE**\n",
    "\n",
    "### **Step-by-Step: Your First QLoRA Fine-Tuning**\n",
    "\n",
    "```python\n",
    "# Recipe for fine-tuning Llama 2 7B on your task\n",
    "\n",
    "# ============================================\n",
    "# STEP 1: Prepare Your Data\n",
    "# ============================================\n",
    "import json\n",
    "\n",
    "data = []\n",
    "for example in your_data:\n",
    "    data.append({\n",
    "        \"instruction\": \"Your task description\",\n",
    "        \"input\": example[\"input_text\"],\n",
    "        \"output\": example[\"target_output\"]\n",
    "    })\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"train.json\", \"w\") as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: Install Dependencies\n",
    "# ============================================\n",
    "# pip install transformers peft bitsandbytes accelerate\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: Load Quantized Model\n",
    "# ============================================\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: Configure LoRA\n",
    "# ============================================\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                              # Rank\n",
    "    lora_alpha=32,                     # Scaling\n",
    "    target_modules=[                   # Target layers\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Prepare model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ============================================\n",
    "# STEP 5: Setup Training\n",
    "# ============================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,               # Higher LR for LoRA\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_32bit\"         # QLoRA optimizer\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# STEP 6: Train!\n",
    "# ============================================\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ============================================\n",
    "# STEP 7: Save & Use\n",
    "# ============================================\n",
    "model.save_pretrained(\"./lora-adapters\")\n",
    "tokenizer.save_pretrained(\"./lora-adapters\")\n",
    "\n",
    "# Inference\n",
    "inputs = tokenizer(\"Your prompt here\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **üéâ CONGRATULATIONS!**\n",
    "\n",
    "You now understand:\n",
    "\n",
    "‚úÖ Why full fine-tuning is problematic  \n",
    "‚úÖ How LoRA works mathematically  \n",
    "‚úÖ The connection between quantization and LoRA  \n",
    "‚úÖ What QLoRA adds on top  \n",
    "‚úÖ When to use each method  \n",
    "‚úÖ How to implement them  \n",
    "‚úÖ Best practices and pitfalls  \n",
    "‚úÖ How to evaluate results  \n",
    "\n",
    "**You've gone from ZERO to HERO in LLM fine-tuning!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**Want to go deeper on any specific topic?** Let me know and I'll dive into more detail! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e77e9b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

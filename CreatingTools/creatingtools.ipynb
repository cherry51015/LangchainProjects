{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc8edce",
   "metadata": {},
   "source": [
    "#### here we are discussing about tools like AirxivQueryRun, WikipediaQueryRun to call them when llm dont have answer and to check from those sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d047a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['HF_TOKEN']=os.getenv('HUGGINGFACE_API')\n",
    "api_key=os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa1912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper,ArxivAPIWrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b05ca",
   "metadata": {},
   "source": [
    "#### in order to convert this retriever to a tool we need to import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588c3242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper_wiki=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\n",
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper_arxiv=ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "arxiv=ArxivQueryRun(api_wrapper=api_wrapper_arxiv)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d72f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[arxiv,wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aea837e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000224EA354E90>, search_kwargs={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs=loader.load()\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "db=FAISS.from_documents(documents,embeddings)\n",
    "retriever=db.as_retriever()\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f463bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith-search'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(retriever,\"langsmith-search\",\"search any information about Langsmith\")\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\mydhi\\\\OneDrive\\\\Desktop\\\\LangchainProjects\\\\venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " 'langsmith-search',\n",
       " StructuredTool(name='langsmith-search', description='search any information about Langsmith', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x000002252EEBFE20>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x000002252EEBF9C0>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.append(retriever_tool)\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fda2112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\mydhi\\\\OneDrive\\\\Desktop\\\\LangchainProjects\\\\venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " StructuredTool(name='langsmith-search', description='search any information about Langsmith', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x000002252EEBFE20>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x000002252EEBF9C0>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools=[arxiv,wiki,retriever_tool]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdebc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2782a7a6",
   "metadata": {},
   "source": [
    "Gotcha ‚Äî this phrase pops up **everywhere** (APIs, LangChain, dev tools), so let‚Äôs lock it in *cleanly and intuitively*, not buzzword-y.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is a **Tool**? üõ†Ô∏è\n",
    "\n",
    "A **tool** is something that **does actual work**.\n",
    "\n",
    "Think:\n",
    "üëâ *‚ÄúI need something done.‚Äù*\n",
    "üëâ You call a **tool**.\n",
    "\n",
    "### In programming terms\n",
    "\n",
    "A tool is usually:\n",
    "\n",
    "* a **function**\n",
    "* an **API**\n",
    "* a **library method**\n",
    "* a **command**\n",
    "\n",
    "### Simple example\n",
    "\n",
    "```python\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "```\n",
    "\n",
    "This function is a **tool** because it performs a task.\n",
    "\n",
    "### Real-world analogy\n",
    "\n",
    "* Calculator ‚Üí tool\n",
    "* Hammer ‚Üí tool\n",
    "* Google Maps ‚Üí tool\n",
    "\n",
    "They **do** things.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What is a **Wrapper**? üéÅ\n",
    "\n",
    "A **wrapper** is something that **surrounds a tool** to make it:\n",
    "\n",
    "* easier to use\n",
    "* safer\n",
    "* more consistent\n",
    "* more powerful\n",
    "\n",
    "Think:\n",
    "üëâ *‚ÄúI don‚Äôt change what it does‚Ä¶ I change how you use it.‚Äù*\n",
    "\n",
    "### Simple example\n",
    "\n",
    "```python\n",
    "def safe_add(a, b):\n",
    "    print(\"Adding numbers...\")\n",
    "    return add(a, b)\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "* `add()` ‚Üí **tool**\n",
    "* `safe_add()` ‚Üí **wrapper**\n",
    "\n",
    "The wrapper:\n",
    "\n",
    "* calls the tool\n",
    "* adds extra behavior\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Tool vs Wrapper (one-line intuition)\n",
    "\n",
    "| Concept | Meaning                                      |\n",
    "| ------- | -------------------------------------------- |\n",
    "| Tool    | **Does the work**                            |\n",
    "| Wrapper | **Controls / enhances how the work is done** |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Why Wrappers Exist (VERY important) ‚ö†Ô∏è\n",
    "\n",
    "Wrappers are used to:\n",
    "\n",
    "### 1Ô∏è‚É£ Hide complexity\n",
    "\n",
    "```python\n",
    "# without wrapper\n",
    "response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "# with wrapper\n",
    "response = fetch(url)\n",
    "```\n",
    "\n",
    "### 2Ô∏è‚É£ Add safety\n",
    "\n",
    "```python\n",
    "def safe_divide(a, b):\n",
    "    if b == 0:\n",
    "        return 0\n",
    "    return a / b\n",
    "```\n",
    "\n",
    "### 3Ô∏è‚É£ Standardize usage\n",
    "\n",
    "Everyone calls **one wrapper**, not 10 different tools differently.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Real-World Software Examples üåç\n",
    "\n",
    "### Python example\n",
    "\n",
    "```python\n",
    "import math\n",
    "math.sqrt(16)\n",
    "```\n",
    "\n",
    "* `sqrt()` ‚Üí tool\n",
    "* `math` module ‚Üí wrapper around math operations\n",
    "\n",
    "---\n",
    "\n",
    "### Web APIs\n",
    "\n",
    "```python\n",
    "requests.get(\"https://api.example.com\")\n",
    "```\n",
    "\n",
    "* HTTP re\n",
    "## 6. Tools & Wrappers in **AI / LangChain** ü§ñ\n",
    "\n",
    "This is where you‚Äôre probably hearing this term.\n",
    "\n",
    "### Tool (in AI)\n",
    "\n",
    "A tool is something the LLM can **call**:\n",
    "\n",
    "* search\n",
    "* calculator\n",
    "* database query\n",
    "* Python function\n",
    "\n",
    "```python\n",
    "def get_weather(city):\n",
    "    return \"32¬∞C\"\n",
    "```\n",
    "\n",
    "That function is a **tool**.\n",
    "\n",
    "---\n",
    "\n",
    "### Wrapper (in AI)\n",
    "\n",
    "A wrapper:\n",
    "\n",
    "* defines **how the LLM calls the tool**\n",
    "* validates input/output\n",
    "* formats results\n",
    "\n",
    "LangChain wraps tools so LLMs can safely use them.\n",
    "\n",
    "```\n",
    "LLM ‚Üí Wrapper ‚Üí Tool ‚Üí Result ‚Üí Wrapper ‚Üí LLM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Super-clear analogy (remember this) üß†\n",
    "\n",
    "üçï **Pizza Shop**\n",
    "\n",
    "* Oven = **Tool** (bakes pizza)\n",
    "* Chef = **Wrapper** (decides temp, time, toppings)\n",
    "\n",
    "Oven alone is powerful but dumb.\n",
    "Chef makes it usable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c90a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run all this tools with Agents and LLM Models\n",
    "\n",
    "## Tools, LLM-->AgentExecutor\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "openai.api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc5730",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchainhub\u001b[39;00m \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m prompt = \u001b[43mhub\u001b[49m.pull(\u001b[33m\"\u001b[39m\u001b[33mhwchase17/openai-functions-agent\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(prompt.messages)\n",
      "\u001b[31mNameError\u001b[39m: name 'hub' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchainhub import \n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "print(prompt.messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515ab41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mydhi\\OneDrive\\Desktop\\LangchainProjects\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_openai_tools_agent' from 'langchain.agents' (c:\\Users\\mydhi\\OneDrive\\Desktop\\LangchainProjects\\venv\\Lib\\site-packages\\langchain\\agents\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## Agents\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_openai_tools_agent\n\u001b[32m      3\u001b[39m agent=create_openai_tools_agent(llm,tools,prompt)\n\u001b[32m      4\u001b[39m agent\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'create_openai_tools_agent' from 'langchain.agents' (c:\\Users\\mydhi\\OneDrive\\Desktop\\LangchainProjects\\venv\\Lib\\site-packages\\langchain\\agents\\__init__.py)"
     ]
    }
   ],
   "source": [
    "## Agents\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lagchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64157e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\":\"Tell me about Langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca57ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\":\"What is machine learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\":\"What's the paper 1706.03762 about?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cfe39d",
   "metadata": {},
   "source": [
    "Alright, cherry ‚Äî deep breath üòÑ\n",
    "What you pasted is **not random magic**. It‚Äôs a very **clean pipeline**, but LangChain hides it behind scary names. I‚Äôll peel it layer by layer until it feels *obvious*, not memorized.\n",
    "\n",
    "I‚Äôll go in **execution order**, not file order ‚Äî that‚Äôs the key to intuition.\n",
    "\n",
    "---\n",
    "\n",
    "# 0Ô∏è‚É£ Big picture (keep this mental model)\n",
    "\n",
    "Your code builds this pipeline:\n",
    "\n",
    "```\n",
    "User input\n",
    "   ‚Üì\n",
    "Prompt Template\n",
    "   ‚Üì\n",
    "LLM (Groq / Llama3)\n",
    "   ‚Üì\n",
    "Agent (decides: think OR call tool)\n",
    "   ‚Üì\n",
    "Tool (Wikipedia / Arxiv / LangSmith)\n",
    "   ‚Üì\n",
    "AgentExecutor (loops until final answer)\n",
    "```\n",
    "\n",
    "Everything else is **plumbing**.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Prompt Template (the brain‚Äôs instructions)\n",
    "\n",
    "```python\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages\n",
    "```\n",
    "\n",
    "### What `hub.pull()` does\n",
    "\n",
    "* Downloads a **prebuilt prompt**\n",
    "* Think of it as a **standard agent brain**\n",
    "\n",
    "You didn‚Äôt write the instructions ‚Äî LangChain gave you a *battle-tested* one.\n",
    "\n",
    "---\n",
    "\n",
    "### What `prompt.messages` shows\n",
    "\n",
    "```python\n",
    "[\n",
    " SystemMessagePromptTemplate(\"You are a helpful assistant\"),\n",
    " MessagesPlaceholder(\"chat_history\"),\n",
    " HumanMessagePromptTemplate(\"{input}\"),\n",
    " MessagesPlaceholder(\"agent_scratchpad\")\n",
    "]\n",
    "```\n",
    "\n",
    "This is HUGE. Let‚Äôs decode each line.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ SystemMessagePromptTemplate\n",
    "\n",
    "```python\n",
    "\"You are a helpful assistant\"\n",
    "```\n",
    "\n",
    "üìå This sets **behavior**, not content\n",
    "Same idea as:\n",
    "\n",
    "> ‚ÄúYou are ChatGPT‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ MessagesPlaceholder(\"chat_history\")\n",
    "\n",
    "```python\n",
    "MessagesPlaceholder(variable_name='chat_history', optional=True)\n",
    "```\n",
    "\n",
    "This is:\n",
    "\n",
    "* Memory\n",
    "* Previous conversation turns\n",
    "\n",
    "If no history ‚Üí ignored\n",
    "If history exists ‚Üí injected automatically\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ HumanMessagePromptTemplate\n",
    "\n",
    "```python\n",
    "\"{input}\"\n",
    "```\n",
    "\n",
    "This is where **your user query** goes:\n",
    "\n",
    "```python\n",
    "agent_executor.invoke({\n",
    "    \"input\": \"Tell me about Langsmith\"\n",
    "})\n",
    "```\n",
    "\n",
    "So this becomes:\n",
    "\n",
    "```\n",
    "Human: Tell me about Langsmith\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ MessagesPlaceholder(\"agent_scratchpad\")\n",
    "\n",
    "üî• THIS IS THE MOST IMPORTANT PART üî•\n",
    "\n",
    "This is where the agent:\n",
    "\n",
    "* thinks\n",
    "* decides tools\n",
    "* stores intermediate steps\n",
    "\n",
    "You NEVER fill this manually.\n",
    "LangChain fills it during execution.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary so far\n",
    "\n",
    "| Part             | Purpose                     |\n",
    "| ---------------- | --------------------------- |\n",
    "| system           | personality                 |\n",
    "| chat_history     | memory                      |\n",
    "| input            | user question               |\n",
    "| agent_scratchpad | agent thinking + tool calls |\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Tools (what the agent is allowed to use)\n",
    "\n",
    "You passed:\n",
    "\n",
    "```python\n",
    "tools = [wikipedia, arxiv, langsmith-search]\n",
    "```\n",
    "\n",
    "Each tool becomes **OpenAI-style function metadata**:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"langsmith-search\",\n",
    "  \"description\": \"...\",\n",
    "  \"parameters\": {\n",
    "    \"query\": \"string\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "üß† The LLM **does not execute tools**\n",
    "It only says:\n",
    "\n",
    "> ‚ÄúI want to call `langsmith-search` with query=X‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ create_openai_tools_agent (the decision-maker)\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "```\n",
    "\n",
    "This creates an **agent runnable**.\n",
    "\n",
    "### What this agent does internally\n",
    "\n",
    "Every step:\n",
    "\n",
    "1. Read prompt\n",
    "2. Look at tools\n",
    "3. Decide:\n",
    "\n",
    "   * Answer directly ‚ùå\n",
    "   * OR call a tool ‚úÖ\n",
    "\n",
    "It‚Äôs basically:\n",
    "\n",
    "```text\n",
    "\"If I need external info ‚Üí call tool\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why this output looks scary\n",
    "\n",
    "```python\n",
    "RunnableAssign(...)\n",
    "| ChatPromptTemplate(...)\n",
    "| RunnableBinding(...)\n",
    "| OpenAIToolsAgentOutputParser()\n",
    "```\n",
    "\n",
    "That‚Äôs just **LangChain‚Äôs pipeline syntax**.\n",
    "\n",
    "Think Unix pipes:\n",
    "\n",
    "```bash\n",
    "cat file | grep word | sort\n",
    "```\n",
    "\n",
    "Same idea.\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ RunnableAssign (where scratchpad magic happens)\n",
    "\n",
    "```python\n",
    "RunnableAssign(mapper={\n",
    "  agent_scratchpad:\n",
    "    lambda x: format_to_openai_tool_messages(x['intermediate_steps'])\n",
    "})\n",
    "```\n",
    "\n",
    "### Translate to English\n",
    "\n",
    "> ‚ÄúTake previous steps (thoughts + tool calls)\n",
    "> and format them so the LLM can read them again.‚Äù\n",
    "\n",
    "This allows:\n",
    "\n",
    "* multi-step reasoning\n",
    "* tool chaining\n",
    "\n",
    "Without this ‚Üí agent forgets what it already did.\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ ChatPromptTemplate (final prompt assembly)\n",
    "\n",
    "```python\n",
    "ChatPromptTemplate(\n",
    "  input_variables=['input', 'agent_scratchpad']\n",
    ")\n",
    "```\n",
    "\n",
    "At runtime, LangChain fills:\n",
    "\n",
    "```text\n",
    "System: You are a helpful assistant\n",
    "Human: Tell me about Langsmith\n",
    "Agent scratchpad:\n",
    "  ‚Üí I should search LangSmith\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ RunnableBinding (LLM + tools)\n",
    "\n",
    "```python\n",
    "RunnableBinding(\n",
    "  model_name='Llama3-8b-8192',\n",
    "  kwargs={'tools': [...]}\n",
    ")\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "* LLM = Groq‚Äôs Llama3\n",
    "* Tools are passed using **function-calling format**\n",
    "\n",
    "The LLM can now output:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"tool_call\": \"langsmith-search\",\n",
    "  \"arguments\": { \"query\": \"Langsmith\" }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ OpenAIToolsAgentOutputParser\n",
    "\n",
    "This:\n",
    "\n",
    "* Reads the LLM output\n",
    "* Detects:\n",
    "\n",
    "  * tool calls\n",
    "  * final answers\n",
    "\n",
    "Without it ‚Üí LangChain wouldn‚Äôt know what the LLM intended.\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ AgentExecutor (the loop controller)\n",
    "\n",
    "```python\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")\n",
    "```\n",
    "\n",
    "üìå THIS is what actually **runs** the agent.\n",
    "\n",
    "### What AgentExecutor does\n",
    "\n",
    "```text\n",
    "LOOP:\n",
    "  call agent\n",
    "  IF tool requested:\n",
    "     run tool\n",
    "     store result\n",
    "     repeat\n",
    "  ELSE:\n",
    "     return final answer\n",
    "```\n",
    "\n",
    "That‚Äôs why you see logs like:\n",
    "\n",
    "```\n",
    "Invoking: `langsmith-search`\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ What happens when you invoke\n",
    "\n",
    "```python\n",
    "agent_executor.invoke({\n",
    "    \"input\": \"Tell me about Langsmith\"\n",
    "})\n",
    "```\n",
    "\n",
    "### Step-by-step runtime flow\n",
    "\n",
    "1. User input inserted into prompt\n",
    "2. LLM sees question\n",
    "3. LLM decides:\n",
    "   ‚ùå I don‚Äôt know this from memory\n",
    "   ‚úÖ I should search\n",
    "4. Tool `langsmith-search` called\n",
    "5. Tool returns text\n",
    "6. Text added to `agent_scratchpad`\n",
    "7. LLM called again\n",
    "8. LLM writes final answer\n",
    "\n",
    "üî• This is **ReAct-style reasoning** (Reason + Act).\n",
    "\n",
    "---\n",
    "\n",
    "# 10Ô∏è‚É£ One mental model to rule them all üß†\n",
    "\n",
    "```text\n",
    "Prompt ‚Üí Agent ‚Üí Tool ‚Üí Scratchpad ‚Üí Agent ‚Üí Answer\n",
    "```\n",
    "\n",
    "Everything else is just:\n",
    "\n",
    "* wrappers\n",
    "* adapters\n",
    "* safety layers\n",
    "\n",
    "---\n",
    "\n",
    "# 11Ô∏è‚É£ Why your earlier errors happened (IMPORTANT)\n",
    "\n",
    "You earlier saw errors like:\n",
    "\n",
    "```\n",
    "missing variables {'chat_history'}\n",
    "```\n",
    "\n",
    "Now you know why:\n",
    "\n",
    "* Prompt expects placeholders\n",
    "* AgentExecutor must fill them\n",
    "* Missing = crash\n",
    "\n",
    "---\n",
    "\n",
    "# 12Ô∏è‚É£ If you had to explain this in ONE line in interview\n",
    "\n",
    "> ‚ÄúLangChain agents are LLM-driven decision loops where the model selects tools via function calling, executes them through an executor, and uses a scratchpad to reason across steps.‚Äù\n",
    "\n",
    "Say that and sip water üòé\n",
    "\n",
    "---\n",
    "\n",
    "If you want next, I can:\n",
    "\n",
    "* rewrite this **without LangChain magic**\n",
    "* show a **minimal custom agent**\n",
    "* explain **Runnable, LCEL, pipes**\n",
    "* debug agents step-by-step like a debugger\n",
    "\n",
    "Just tell me where you feel the fog thickest üå´Ô∏è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ece6a8",
   "metadata": {},
   "source": [
    "Perfect question. This is *exactly* the confusion point for almost everyone, so you‚Äôre right on track üëç\n",
    "Let‚Äôs slow this wayyy down and kill the mystery around **scratchpad** once and for all.\n",
    "\n",
    "I‚Äôll answer **both parts** clearly:\n",
    "\n",
    "1. **What the scratchpad actually is**\n",
    "2. **Why we need it at all**\n",
    "3. **Yes, did this come ‚Äúpre-built‚Äù from hub.pull? (short answer: yes, but‚Ä¶)**\n",
    "\n",
    "No buzzwords. No magic.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ First: forget the word ‚Äúscratchpad‚Äù\n",
    "\n",
    "The name is honestly terrible üòÖ\n",
    "Replace it in your head with:\n",
    "\n",
    "> **‚Äúagent memory of what just happened‚Äù**\n",
    "\n",
    "That‚Äôs it. Nothing more.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ What problem does the scratchpad solve?\n",
    "\n",
    "LLMs are **stateless**.\n",
    "\n",
    "That means:\n",
    "\n",
    "* The model **forgets** previous tool calls\n",
    "* It does NOT remember:\n",
    "\n",
    "  * which tool it called\n",
    "  * what result came back\n",
    "  * why it called it\n",
    "\n",
    "Unless‚Ä¶ **you put that information back into the next prompt**.\n",
    "\n",
    "üí° That re-inserted information is the **scratchpad**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ What happens WITHOUT a scratchpad (very important)\n",
    "\n",
    "Imagine this flow:\n",
    "\n",
    "```\n",
    "User: Tell me about LangSmith\n",
    "LLM: I should search LangSmith\n",
    "Tool: returns info\n",
    "LLM (next call): ‚ùì what tool? what result?\n",
    "```\n",
    "\n",
    "The LLM has **no idea**:\n",
    "\n",
    "* that it already searched\n",
    "* what the search result was\n",
    "\n",
    "So it may:\n",
    "\n",
    "* call the same tool again (infinite loop)\n",
    "* hallucinate\n",
    "* answer incorrectly\n",
    "\n",
    "‚ùå That‚Äôs why scratchpad is REQUIRED.\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ What the scratchpad actually contains\n",
    "\n",
    "In your code:\n",
    "\n",
    "```python\n",
    "MessagesPlaceholder(variable_name='agent_scratchpad')\n",
    "```\n",
    "\n",
    "At runtime, this gets filled with things like:\n",
    "\n",
    "```\n",
    "Thought: I should search for LangSmith\n",
    "Action: langsmith-search\n",
    "Action Input: {\"query\": \"LangSmith\"}\n",
    "Observation: LangSmith is a platform for ...\n",
    "```\n",
    "\n",
    "Then this whole block is fed back to the LLM on the **next step**.\n",
    "\n",
    "So the LLM now sees:\n",
    "\n",
    "> ‚ÄúAh, I already searched, here is the result. Now I can answer.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Where does this content come from?\n",
    "\n",
    "This line is the key:\n",
    "\n",
    "```python\n",
    "RunnableLambda(\n",
    "  lambda x: format_to_openai_tool_messages(x['intermediate_steps'])\n",
    ")\n",
    "```\n",
    "\n",
    "Translate to English:\n",
    "\n",
    "> ‚ÄúTake all previous tool calls + outputs\n",
    "> and convert them into messages the LLM understands.‚Äù\n",
    "\n",
    "`intermediate_steps` = raw history\n",
    "`scratchpad` = formatted history\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Why it‚Äôs called a *scratchpad* (analogy)\n",
    "\n",
    "Think of a human solving a problem:\n",
    "\n",
    "* You don‚Äôt solve everything in your head\n",
    "* You write notes:\n",
    "\n",
    "  * ‚ÄúDid this‚Äù\n",
    "  * ‚ÄúGot this result‚Äù\n",
    "  * ‚ÄúNow do next thing‚Äù\n",
    "\n",
    "That paper = **scratchpad**\n",
    "\n",
    "The LLM is doing the same thing, but:\n",
    "\n",
    "* it can‚Äôt write notes\n",
    "* so LangChain writes notes **for it**\n",
    "* and shows them back each time\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Is scratchpad ‚Äúthinking‚Äù? ü§î\n",
    "\n",
    "Important clarification.\n",
    "\n",
    "‚ùå It is NOT:\n",
    "\n",
    "* hidden chain-of-thought\n",
    "* secret reasoning\n",
    "\n",
    "‚úÖ It IS:\n",
    "\n",
    "* tool calls\n",
    "* tool outputs\n",
    "* short reasoning steps **needed for correctness**\n",
    "\n",
    "LangChain is careful to keep it structured and safe.\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Now your second question (very important):\n",
    "\n",
    "> *‚Äúu mean we have pulled this from langchain document which is already a inbuilt agent with the help of hub.pull?‚Äù*\n",
    "\n",
    "### Yes ‚Äî but with nuance.\n",
    "\n",
    "### What `hub.pull(\"hwchase17/openai-functions-agent\")` gives you:\n",
    "\n",
    "‚úî A **predefined prompt** that:\n",
    "\n",
    "* expects a scratchpad\n",
    "* expects tools\n",
    "* is compatible with function calling\n",
    "\n",
    "‚ùå It does NOT:\n",
    "\n",
    "* execute tools\n",
    "* manage loops\n",
    "* store memory\n",
    "\n",
    "That part is done by:\n",
    "\n",
    "* `create_openai_tools_agent`\n",
    "* `AgentExecutor`\n",
    "\n",
    "So think of it like this:\n",
    "\n",
    "| Part                | Comes from                  |\n",
    "| ------------------- | --------------------------- |\n",
    "| Prompt structure    | `hub.pull`                  |\n",
    "| Tool decision logic | `create_openai_tools_agent` |\n",
    "| Scratchpad filling  | RunnableAssign              |\n",
    "| Tool execution loop | AgentExecutor               |\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Why LangChain hides scratchpad from beginners\n",
    "\n",
    "Because if you saw this first day:\n",
    "\n",
    "```text\n",
    "Thought ‚Üí Action ‚Üí Observation ‚Üí Thought ‚Üí Action ‚Üí ...\n",
    "```\n",
    "\n",
    "your brain would explode ü§Ø\n",
    "\n",
    "So they:\n",
    "\n",
    "* name it ‚Äúagent_scratchpad‚Äù\n",
    "* auto-fill it\n",
    "* expose it only when you go deep (like now)\n",
    "\n",
    "---\n",
    "\n",
    "## üîü One-line intuition (lock this in)\n",
    "\n",
    "> **Scratchpad exists so the LLM remembers what tools it already used and what came back, across multiple steps.**\n",
    "\n",
    "If you remember just this, you‚Äôre good.\n",
    "\n",
    "---\n",
    "\n",
    "## 11Ô∏è‚É£ Final check: answer in YOUR words\n",
    "\n",
    "Try saying this (mentally or here):\n",
    "\n",
    "> ‚ÄúThe scratchpad is a placeholder where LangChain injects previous tool calls and their results so the LLM can continue reasoning instead of starting fresh each step.‚Äù\n",
    "\n",
    "If that sentence makes sense ‚Üí you‚Äôve got it ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "Next, I can:\n",
    "\n",
    "* show a **no-scratchpad bug demo**\n",
    "* build a **tiny agent from scratch (10 lines)**\n",
    "* explain how **LangGraph replaces scratchpads**\n",
    "* explain why **chat_history ‚â† scratchpad**\n",
    "\n",
    "Tell me which one you want üëÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804ecb8",
   "metadata": {},
   "source": [
    "Ahhh okay ‚Äî *now* I see exactly what you‚Äôre asking üëç\n",
    "You‚Äôre not asking ‚Äúcan open-source models do agents?‚Äù, you‚Äôre asking:\n",
    "\n",
    "> **‚ÄúWhy are we using `create_openai_tools_agent` if we are NOT using OpenAI, and what do we use for open-source models like Groq / Ollama?‚Äù**\n",
    "\n",
    "This is a VERY legit confusion. Let‚Äôs clear it cleanly.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Why is it called `create_openai_tools_agent`?\n",
    "\n",
    "This is **naming debt**, not vendor lock-in.\n",
    "\n",
    "`create_openai_tools_agent` does **NOT** mean:\n",
    "\n",
    "* OpenAI model only ‚ùå\n",
    "* OpenAI API required ‚ùå\n",
    "\n",
    "It means:\n",
    "\n",
    "> **Uses the OpenAI *tool-calling schema***\n",
    "> (JSON function calling format)\n",
    "\n",
    "That schema became the **de-facto standard**, so many non-OpenAI models support it.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ What actually matters (not the name)\n",
    "\n",
    "This function assumes the model can:\n",
    "\n",
    "‚úî Accept a list of tools as JSON schema\n",
    "‚úî Output structured tool calls\n",
    "‚úî Follow function-calling conventions\n",
    "\n",
    "That‚Äôs it.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Why it worked with Groq in your code\n",
    "\n",
    "You used:\n",
    "\n",
    "```python\n",
    "ChatGroq(model=\"Llama3-8b-8192\")\n",
    "```\n",
    "\n",
    "Groq models:\n",
    "\n",
    "* emulate OpenAI‚Äôs function-calling format\n",
    "* return tool calls in the same structure\n",
    "\n",
    "So LangChain is happy.\n",
    "\n",
    "üëâ **Same agent, different backend.**\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ So what if we use Ollama (open-source, local)?\n",
    "\n",
    "### Case A: Ollama model supports tool calling (newer ones do)\n",
    "\n",
    "Then this still works üëá\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "```\n",
    "\n",
    "Even though:\n",
    "\n",
    "* model = LLaMA / Mistral\n",
    "* runner = Ollama\n",
    "* name says ‚Äúopenai‚Äù\n",
    "\n",
    "Because the **protocol** matches.\n",
    "\n",
    "---\n",
    "\n",
    "### Case B: Ollama model does NOT support tool calling\n",
    "\n",
    "Then ‚ùå `create_openai_tools_agent` will not work.\n",
    "\n",
    "You must use:\n",
    "\n",
    "* ReAct-style agents\n",
    "* text-based tool selection\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_react_agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "```\n",
    "\n",
    "This uses:\n",
    "\n",
    "```\n",
    "Thought: ...\n",
    "Action: search\n",
    "Action Input: ...\n",
    "```\n",
    "\n",
    "Instead of JSON.\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Agent types in LangChain (important table)\n",
    "\n",
    "| Agent creator                  | Tool format           | Model requirement         |\n",
    "| ------------------------------ | --------------------- | ------------------------- |\n",
    "| `create_openai_tools_agent`    | JSON function calling | Must support tool calling |\n",
    "| `create_react_agent`           | Text (Thought/Action) | Any text LLM              |\n",
    "| `create_structured_chat_agent` | Structured text       | Partial structure         |\n",
    "| `create_json_agent`            | JSON only             | Strong JSON support       |\n",
    "\n",
    "So **open-source ‚â† problem**, only **format support matters**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Why LangChain didn‚Äôt rename it\n",
    "\n",
    "Because:\n",
    "\n",
    "* OpenAI introduced function calling first\n",
    "* Everyone copied it\n",
    "* Renaming would break 1000s of apps\n",
    "\n",
    "So yeah‚Ä¶ unfortunate naming üòÖ\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ How to decide what to use (rule of thumb)\n",
    "\n",
    "Ask ONE question:\n",
    "\n",
    "> **Can my model reliably return tool calls in JSON schema?**\n",
    "\n",
    "* YES ‚Üí `create_openai_tools_agent`\n",
    "* NO ‚Üí `create_react_agent`\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Your exact setup (final clarity)\n",
    "\n",
    "You used:\n",
    "\n",
    "* Groq LLaMA 3\n",
    "* OpenAI-style tools\n",
    "* LangChain agent\n",
    "\n",
    "This is **100% correct** and **industry-standard**.\n",
    "\n",
    "You did NOT misuse anything.\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ One-line takeaway (memorize this)\n",
    "\n",
    "> **`create_openai_tools_agent` refers to the tool-calling format, not the model provider.**\n",
    "\n",
    "If you want, next I can:\n",
    "\n",
    "* check **which Ollama models support tools**\n",
    "* show **same agent rewritten with ReAct**\n",
    "* explain **why JSON tool calling is better**\n",
    "* help you choose **best agent type per model**\n",
    "\n",
    "Just tell me üëç\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e8b061",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LangChain)",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

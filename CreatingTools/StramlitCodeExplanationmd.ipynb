{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e82052e4",
   "metadata": {},
   "source": [
    "I‚Äôm really glad you said this, cherry ‚Äî this is *exactly* the right confusion to have.\n",
    "You‚Äôre not ‚Äúmissing basics‚Äù; you‚Äôre missing a **mental model**. Once that clicks, this whole thing becomes obvious instead of magical.\n",
    "\n",
    "So let‚Äôs do what you asked üëá\n",
    "**No jargon dump. No copying final code.**\n",
    "We‚Äôll **build the chat app the way a human thinks**, *one intention at a time*.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† First: clear the fog (VERY IMPORTANT)\n",
    "\n",
    "You‚Äôre confused because you‚Äôre mixing up **three different things**:\n",
    "\n",
    "| Thing              | What it is               | Purpose                |\n",
    "| ------------------ | ------------------------ | ---------------------- |\n",
    "| `st.chat_input`    | **Input widget**         | Get text from user     |\n",
    "| `st.chat_message`  | **Display widget**       | Show a chat bubble     |\n",
    "| `st.session_state` | **Memory (Python dict)** | Remember past messages |\n",
    "\n",
    "üëâ **None of them automatically do the others‚Äô job**\n",
    "\n",
    "This line is the key:\n",
    "\n",
    "```python\n",
    "st.chat_message(\"user\").write(prompt)\n",
    "```\n",
    "\n",
    "That is **EXPLICIT DISPLAY**.\n",
    "Nothing else shows anything on screen.\n",
    "\n",
    "---\n",
    "\n",
    "# üîë Golden Rule (memorize this)\n",
    "\n",
    "> **Streamlit shows ONLY what you explicitly render**\n",
    "\n",
    "Saving to session_state ‚ùå does NOT display\n",
    "chat_input ‚ùå does NOT display\n",
    "chat_message ‚úÖ DOES display\n",
    "\n",
    "---\n",
    "\n",
    "# Step 0Ô∏è‚É£: What are we trying to build?\n",
    "\n",
    "Say it in English first:\n",
    "\n",
    "> ‚ÄúI want a chat app where:\n",
    ">\n",
    "> 1. User types a message\n",
    "> 2. Their message appears as a chat bubble\n",
    "> 3. The message is remembered\n",
    "> 4. On refresh, old messages reappear‚Äù\n",
    "\n",
    "Now we code **ONLY what we need**.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 1Ô∏è‚É£: Just take input (NO CHAT)\n",
    "\n",
    "```python\n",
    "prompt = st.text_input(\"Say something\")\n",
    "```\n",
    "\n",
    "### What it does\n",
    "\n",
    "* Shows input box\n",
    "* Returns text when user types\n",
    "\n",
    "### Input ‚Üí Output\n",
    "\n",
    "```\n",
    "User types: \"hello\"\n",
    "prompt == \"hello\"\n",
    "```\n",
    "\n",
    "‚ùå Nothing appears in chat\n",
    "‚ùå No bubbles\n",
    "‚ùå No memory\n",
    "\n",
    "Good. One job only.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 2Ô∏è‚É£: Show text on screen\n",
    "\n",
    "```python\n",
    "if prompt:\n",
    "    st.write(prompt)\n",
    "```\n",
    "\n",
    "Now:\n",
    "\n",
    "* User types\n",
    "* Text appears\n",
    "\n",
    "But this is NOT a chat UI ‚Äî just text.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 3Ô∏è‚É£: Turn text into a CHAT bubble\n",
    "\n",
    "```python\n",
    "if prompt:\n",
    "    st.chat_message(\"user\").write(prompt)\n",
    "```\n",
    "\n",
    "üéâ BOOM ‚Äî bubble appears.\n",
    "\n",
    "### What is `st.chat_message`?\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "> ‚ÄúCreate a chat bubble for THIS speaker‚Äù\n",
    "\n",
    "```python\n",
    "st.chat_message(role)\n",
    "```\n",
    "\n",
    "### Roles (VERY IMPORTANT)\n",
    "\n",
    "| Role          | Meaning           |\n",
    "| ------------- | ----------------- |\n",
    "| `\"user\"`      | Right-side bubble |\n",
    "| `\"assistant\"` | Left-side bubble  |\n",
    "| `\"system\"`    | Hidden / control  |\n",
    "\n",
    "Streamlit uses the **role string** to style the bubble.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 4Ô∏è‚É£: Why chat_input exists\n",
    "\n",
    "Now replace text_input:\n",
    "\n",
    "```python\n",
    "prompt = st.chat_input(\"Say something\")\n",
    "```\n",
    "\n",
    "### Difference?\n",
    "\n",
    "| text_input     | chat_input         |\n",
    "| -------------- | ------------------ |\n",
    "| Normal textbox | Chat-style textbox |\n",
    "| Always visible | Appears at bottom  |\n",
    "| Not chat-aware | Designed for chat  |\n",
    "\n",
    "üö® IMPORTANT:\n",
    "`st.chat_input()` **does NOT display messages**\n",
    "It only returns text.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 5Ô∏è‚É£: The BIG problem (messages disappear)\n",
    "\n",
    "Type a message ‚Üí bubble appears\n",
    "Then type again ‚Üí first message is gone üòê\n",
    "\n",
    "Why?\n",
    "\n",
    "Because **Streamlit reruns the file from top to bottom**.\n",
    "\n",
    "So we need **memory**.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 6Ô∏è‚É£: Enter `st.session_state`\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "```python\n",
    "st.session_state = {}\n",
    "```\n",
    "\n",
    "But it **survives reruns**.\n",
    "\n",
    "So we say:\n",
    "\n",
    "```python\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "```\n",
    "\n",
    "Now we have a permanent box:\n",
    "\n",
    "```python\n",
    "messages = []\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Step 7Ô∏è‚É£: Save message to memory\n",
    "\n",
    "```python\n",
    "st.session_state.messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": prompt\n",
    "})\n",
    "```\n",
    "\n",
    "This does ONLY this:\n",
    "üì¶ **Stores data**\n",
    "\n",
    "‚ùå No display\n",
    "‚ùå No UI\n",
    "\n",
    "---\n",
    "\n",
    "# Step 8Ô∏è‚É£: Display messages from memory\n",
    "\n",
    "This is the MOST IMPORTANT loop üëá\n",
    "\n",
    "```python\n",
    "for msg in st.session_state.messages:\n",
    "    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
    "```\n",
    "\n",
    "### Read it like English:\n",
    "\n",
    "> ‚ÄúFor every saved message,\n",
    "> create a bubble for its role\n",
    "> and write its content‚Äù\n",
    "\n",
    "üî• THIS is what re-renders old messages.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 9Ô∏è‚É£: Why we do BOTH append + chat_message\n",
    "\n",
    "Now answer your exact question üëá\n",
    "\n",
    "### This code:\n",
    "\n",
    "```python\n",
    "st.session_state.messages.append(\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    ")\n",
    "st.chat_message(\"user\").write(prompt)\n",
    "```\n",
    "\n",
    "### What each line does\n",
    "\n",
    "| Line                | Purpose          |\n",
    "| ------------------- | ---------------- |\n",
    "| `append(...)`       | Save message     |\n",
    "| `chat_message(...)` | Show message NOW |\n",
    "\n",
    "If you remove the second line:\n",
    "\n",
    "* Message saved\n",
    "* BUT won‚Äôt appear until next rerun\n",
    "\n",
    "If you remove the first line:\n",
    "\n",
    "* Message appears\n",
    "* BUT disappears on next input\n",
    "\n",
    "üëâ **You need both**\n",
    "\n",
    "---\n",
    "\n",
    "# Step üîü: Add assistant reply (fake first)\n",
    "\n",
    "```python\n",
    "reply = \"Hello! I am a bot.\"\n",
    "```\n",
    "\n",
    "Save it:\n",
    "\n",
    "```python\n",
    "st.session_state.messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": reply\n",
    "})\n",
    "```\n",
    "\n",
    "Display it:\n",
    "\n",
    "```python\n",
    "st.chat_message(\"assistant\").write(reply)\n",
    "```\n",
    "\n",
    "üéâ Now you have a full chat.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† FINAL INTUITIVE VERSION (read-only)\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "\n",
    "# 1. Create memory\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# 2. Show old messages\n",
    "for msg in st.session_state.messages:\n",
    "    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
    "\n",
    "# 3. Take new input\n",
    "prompt = st.chat_input(\"Say something\")\n",
    "\n",
    "if prompt:\n",
    "    # 4. Save user message\n",
    "    st.session_state.messages.append(\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    )\n",
    "\n",
    "    # 5. Display user message\n",
    "    st.chat_message(\"user\").write(prompt)\n",
    "\n",
    "    # 6. Create assistant reply\n",
    "    reply = \"I heard you say: \" + prompt\n",
    "\n",
    "    # 7. Save assistant message\n",
    "    st.session_state.messages.append(\n",
    "        {\"role\": \"assistant\", \"content\": reply}\n",
    "    )\n",
    "\n",
    "    # 8. Display assistant message\n",
    "    st.chat_message(\"assistant\").write(reply)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üß© Mental Model (tattoo this)\n",
    "\n",
    "* `chat_input` ‚Üí **input**\n",
    "* `chat_message` ‚Üí **display**\n",
    "* `session_state` ‚Üí **memory**\n",
    "* loop over messages ‚Üí **re-render UI**\n",
    "\n",
    "---\n",
    "\n",
    "## üí¨ Be honest with me\n",
    "\n",
    "Do you now understand:\n",
    "\n",
    "1. Why messages appear?\n",
    "2. Why they disappear without session_state?\n",
    "3. Why we append + display separately?\n",
    "\n",
    "If yes ‚Üí next we‚Äôll plug in **LLM + agent** using the SAME mental model\n",
    "If not ‚Üí we‚Äôll redo this with diagrams + dry run\n",
    "\n",
    "You‚Äôre actually learning this the *right* way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a41a2",
   "metadata": {},
   "source": [
    "This is a **great question** ‚Äî you‚Äôre now asking the *right* ‚Äúunder-the-hood‚Äù things.\n",
    "Let‚Äôs slow this down and **demystify the agent + tools pipeline** so it stops feeling like black magic.\n",
    "\n",
    "I‚Äôll do this in 5 layers:\n",
    "\n",
    "1. **What actually happens when you call `initialize_agent`**\n",
    "2. **Who decides which tool to call**\n",
    "3. **How a tool call is detected, routed, executed, and returned**\n",
    "4. **Order, limits, retries, stopping**\n",
    "5. **Reality check: deprecated vs current APIs (important in 2026)**\n",
    "\n",
    "No code dump first ‚Äî only mental models. Then we‚Äôll map it back to *your exact code*.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ What you *think* is happening vs what *actually* happens\n",
    "\n",
    "### What it feels like\n",
    "\n",
    "> ‚ÄúSome agent magically chooses tools, runs them, and answers.‚Äù\n",
    "\n",
    "### What *actually* happens\n",
    "\n",
    "There is **NO autonomous tool execution**.\n",
    "\n",
    "Instead, this happens:\n",
    "\n",
    "> üîÅ A **loop** where the LLM keeps talking to *itself*\n",
    "> üß† The LLM outputs **structured text**\n",
    "> ü§ñ LangChain **parses that text**\n",
    "> üõ†Ô∏è LangChain **executes the tool**\n",
    "> üì© Tool output is fed back to the LLM\n",
    "> üõë Loop ends when LLM says ‚ÄúFinal Answer‚Äù\n",
    "\n",
    "The LLM is **always in control**.\n",
    "LangChain is just a **traffic cop + executor**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ What `initialize_agent` REALLY creates\n",
    "\n",
    "This line:\n",
    "\n",
    "```python\n",
    "search_agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handling_parsing_errors=True\n",
    ")\n",
    "```\n",
    "\n",
    "does **NOT** create a single object.\n",
    "\n",
    "It creates **three things glued together**:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ        Agent            ‚îÇ  ‚Üê reasoning (LLM + prompt)\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ     AgentExecutor       ‚îÇ  ‚Üê loop + safety + limits\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ      Tool Registry      ‚îÇ  ‚Üê name ‚Üí function mapping\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "So when you later call:\n",
    "\n",
    "```python\n",
    "search_agent.run(...)\n",
    "```\n",
    "\n",
    "You are actually calling:\n",
    "\n",
    "> **AgentExecutor.run()**\n",
    "\n",
    "This is why you don‚Äôt see `AgentExecutor` explicitly ‚Äî\n",
    "`initialize_agent` hides it.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ How the agent knows which tool to call\n",
    "\n",
    "### üîë Critical truth (burn this in):\n",
    "\n",
    "> **The LLM chooses the tool. LangChain never ‚Äúdecides‚Äù.**\n",
    "\n",
    "#### Tools are described like this (internally):\n",
    "\n",
    "```text\n",
    "Search: Useful for searching the web\n",
    "ArxivQueryRun: Useful for academic papers\n",
    "WikipediaQueryRun: Useful for encyclopedic knowledge\n",
    "```\n",
    "\n",
    "LangChain injects these descriptions into the **system prompt**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† The ReAct protocol (your agent type)\n",
    "\n",
    "You‚Äôre using:\n",
    "\n",
    "```python\n",
    "AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
    "```\n",
    "\n",
    "That means the LLM is instructed to respond in this format:\n",
    "\n",
    "```\n",
    "Thought: I need to search the web\n",
    "Action: Search\n",
    "Action Input: \"machine learning definition\"\n",
    "```\n",
    "\n",
    "LangChain then:\n",
    "\n",
    "1. Reads the LLM output\n",
    "2. Sees `Action: Search`\n",
    "3. Matches `\"Search\"` ‚Üí `DuckDuckGoSearchRun`\n",
    "4. Calls:\n",
    "\n",
    "   ```python\n",
    "   search.run(\"machine learning definition\")\n",
    "   ```\n",
    "5. Gets result\n",
    "6. Feeds it back to the LLM as:\n",
    "\n",
    "```\n",
    "Observation: <tool output>\n",
    "```\n",
    "\n",
    "Then the loop repeats.\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Who ‚Äúdecodes‚Äù tool calls?\n",
    "\n",
    "**Not Streamlit**\n",
    "**Not you**\n",
    "**Not the tool**\n",
    "\n",
    "üëâ **The AgentExecutor**\n",
    "\n",
    "Internally, it does roughly this:\n",
    "\n",
    "```python\n",
    "while not done:\n",
    "    llm_output = llm(prompt_with_history)\n",
    "\n",
    "    if \"Action:\" in llm_output:\n",
    "        tool_name, tool_input = parse(llm_output)\n",
    "        result = tools[tool_name].run(tool_input)\n",
    "        append Observation(result)\n",
    "    else:\n",
    "        return Final Answer\n",
    "```\n",
    "\n",
    "That‚Äôs it. No AI inside LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Order of execution (VERY IMPORTANT)\n",
    "\n",
    "### ‚ùì ‚ÄúWhich tool runs first?‚Äù\n",
    "\n",
    "üëâ **Whichever the LLM mentions first**\n",
    "\n",
    "Example internal flow:\n",
    "\n",
    "```\n",
    "Thought: I need background info\n",
    "Action: WikipediaQueryRun\n",
    "Observation: ...\n",
    "\n",
    "Thought: I need recent info\n",
    "Action: Search\n",
    "Observation: ...\n",
    "\n",
    "Thought: I can answer now\n",
    "Final Answer: ...\n",
    "```\n",
    "\n",
    "There is:\n",
    "\n",
    "* ‚ùå No fixed order\n",
    "* ‚ùå No priority\n",
    "* ‚ùå No auto-parallelism\n",
    "\n",
    "Only **LLM reasoning**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ How many times can tools run?\n",
    "\n",
    "Controlled by **AgentExecutor defaults**:\n",
    "\n",
    "| Setting            | Default |\n",
    "| ------------------ | ------- |\n",
    "| max_iterations     | 15      |\n",
    "| early_stopping     | ‚Äúforce‚Äù |\n",
    "| max_execution_time | None    |\n",
    "\n",
    "So:\n",
    "\n",
    "* Tool calls can repeat\n",
    "* Same tool can be called multiple times\n",
    "* Loop stops when:\n",
    "\n",
    "  * LLM outputs `Final Answer`\n",
    "  * or max iterations reached\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Why `StreamlitCallbackHandler` works\n",
    "\n",
    "This line:\n",
    "\n",
    "```python\n",
    "st_cb = StreamlitCallbackHandler(st.container())\n",
    "```\n",
    "\n",
    "Hooks into **LangChain‚Äôs callback system**.\n",
    "\n",
    "It listens to:\n",
    "\n",
    "* `on_chain_start`\n",
    "* `on_agent_action`\n",
    "* `on_tool_start`\n",
    "* `on_tool_end`\n",
    "* `on_chain_end`\n",
    "\n",
    "So every time the agent thinks or acts, Streamlit renders it.\n",
    "\n",
    "‚ö†Ô∏è Callback handler does **nothing to tool logic** ‚Äî it‚Äôs only a viewer.\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ BIG ISSUE in your code (important)\n",
    "\n",
    "You wrote:\n",
    "\n",
    "```python\n",
    "response = search_agent.run(st.session_state.messages, callbacks=[st_cb])\n",
    "```\n",
    "\n",
    "üö® This is wrong input type.\n",
    "\n",
    "`.run()` expects **string**, not list of messages.\n",
    "\n",
    "### Correct:\n",
    "\n",
    "```python\n",
    "response = search_agent.run(prompt, callbacks=[st_cb])\n",
    "```\n",
    "\n",
    "The agent already keeps its own scratchpad.\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Deprecation & current status (2026 reality check)\n",
    "\n",
    "This matters a LOT.\n",
    "\n",
    "### ‚úÖ Still valid (not deprecated)\n",
    "\n",
    "* `StreamlitCallbackHandler`\n",
    "* `DuckDuckGoSearchRun`\n",
    "* `ArxivQueryRun`\n",
    "* `WikipediaQueryRun`\n",
    "* `ChatGroq`\n",
    "\n",
    "### ‚ö†Ô∏è Soft-deprecated / legacy\n",
    "\n",
    "* `initialize_agent`\n",
    "* `AgentType.ZERO_SHOT_REACT_DESCRIPTION`\n",
    "\n",
    "They **still work**, but LangChain is moving toward:\n",
    "\n",
    "```python\n",
    "create_openai_tools_agent\n",
    "AgentExecutor\n",
    "Runnable interfaces\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Why you don‚Äôt see `create_openai_tools_agent` here\n",
    "\n",
    "Because:\n",
    "\n",
    "* `create_openai_tools_agent` ‚Üí **OpenAI-style tool calling**\n",
    "* `ZERO_SHOT_REACT_DESCRIPTION` ‚Üí **text-based ReAct**\n",
    "\n",
    "Your Llama3 model **does NOT use OpenAI tool schemas**, so LangChain falls back to ReAct.\n",
    "\n",
    "üëâ That‚Äôs why this still works.\n",
    "\n",
    "---\n",
    "\n",
    "## 10Ô∏è‚É£ One-sentence summary (this is gold)\n",
    "\n",
    "> The LLM **decides** the tool,\n",
    "> LangChain **parses** the decision,\n",
    "> the Tool **executes**,\n",
    "> the AgentExecutor **loops**,\n",
    "> StreamlitCallbackHandler **displays**,\n",
    "> and nothing is autonomous except the LLM‚Äôs reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† If I test your understanding\n",
    "\n",
    "Answer this mentally:\n",
    "\n",
    "> ‚ÄúIf I remove DuckDuckGoSearchRun from `tools`, can the agent still search?‚Äù\n",
    "\n",
    "Correct answer:\n",
    "‚ùå No ‚Äî the LLM may *want* to, but it has no tool to execute.\n",
    "\n",
    "If that makes sense ‚Üí you‚Äôve crossed the **agent barrier** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "### Next (only if you want)\n",
    "\n",
    "We can:\n",
    "\n",
    "* Rewrite this using **AgentExecutor explicitly**\n",
    "* Compare **ReAct vs OpenAI tool calling**\n",
    "* Visualize the **exact prompt** sent to the LLM\n",
    "* Build a **custom tool from scratch**\n",
    "\n",
    "Just tell me which rabbit hole you want üêá\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a12e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

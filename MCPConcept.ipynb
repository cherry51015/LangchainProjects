{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "607ba937",
   "metadata": {},
   "source": [
    "# MCP: Model Context Protocol â€” Zero to Hero\n",
    "\n",
    "## Part 1: The Problem â€” Why Did MCP Even Exist?\n",
    "\n",
    "Let me start with a story.\n",
    "\n",
    "---\n",
    "\n",
    "### The World Before MCP\n",
    "\n",
    "Imagine you hire a brilliant new employee â€” let's call them Alex. Alex is incredibly smart, can write code, analyze data, draft emails, answer questions. But Alex has one bizarre limitation: **they can only work with information you physically hand them on paper.**\n",
    "\n",
    "Alex cannot:\n",
    "- Open your company's database themselves\n",
    "- Check your calendar\n",
    "- Read files from your computer\n",
    "- Search the internet\n",
    "- Query your CRM\n",
    "\n",
    "Every single time Alex needs external information, *you* have to go get it, print it out, and hand it to them. Then Alex does their work. Then if they need something else, you go get that too.\n",
    "\n",
    "**That's exactly what LLMs (like GPT-4, Claude, etc.) were like before MCP.**\n",
    "\n",
    "They were brilliant minds trapped in a box. They could only work with text you pasted into the chat window.\n",
    "\n",
    "---\n",
    "\n",
    "### The \"1000 Integrations\" Problem\n",
    "\n",
    "Now let's say you want to build a smarter AI assistant for your company. You want it to:\n",
    "- Read from your PostgreSQL database\n",
    "- Write to your Notion workspace\n",
    "- Query GitHub issues\n",
    "- Check Slack messages\n",
    "- Look at your Google Calendar\n",
    "\n",
    "Before MCP, every single AI company, every single developer, every single product had to **build custom integrations for each of these tools, for each of their AI models.**\n",
    "\n",
    "So Anthropic builds Claude â†’ they write a GitHub connector for Claude.  \n",
    "OpenAI builds GPT â†’ they write a GitHub connector for GPT.  \n",
    "Some startup builds their AI product â†’ they write *another* GitHub connector.\n",
    "\n",
    "GitHub itself gets called by 500 different teams in 500 different incompatible ways.\n",
    "\n",
    "This is the **M Ã— N problem**: M models Ã— N tools = MÃ—N custom integrations. It scales terribly.\n",
    "\n",
    "```\n",
    "Claude â”€â”€â”€â”€ custom code â”€â”€â”€â†’ GitHub\n",
    "Claude â”€â”€â”€â”€ custom code â”€â”€â”€â†’ Notion  \n",
    "Claude â”€â”€â”€â”€ custom code â”€â”€â”€â†’ Postgres\n",
    "GPT-4  â”€â”€â”€â”€ custom code â”€â”€â”€â†’ GitHub  â† same thing, built again!\n",
    "GPT-4  â”€â”€â”€â”€ custom code â”€â”€â”€â†’ Notion  â† same thing, built again!\n",
    "Gemini â”€â”€â”€â”€ custom code â”€â”€â”€â†’ GitHub  â† same thing, built again!\n",
    "```\n",
    "\n",
    "Every integration is a snowflake. Nothing is reusable. It's a maintenance nightmare.\n",
    "\n",
    "---\n",
    "\n",
    "### The Real-World Pain\n",
    "\n",
    "Here's a concrete scenario. You're a developer and you want Claude to help you debug your app. You want Claude to:\n",
    "\n",
    "1. Look at the error logs in your server\n",
    "2. Check the relevant code in your GitHub repo\n",
    "3. Look at recent deployments in your CI/CD system\n",
    "4. Suggest a fix\n",
    "\n",
    "**Without MCP:** You manually copy-paste logs, then copy-paste code, then describe the deployment. You're the middleman. Claude is a brain with no hands.\n",
    "\n",
    "**With MCP:** Claude reaches out to your log server, your GitHub, your CI/CD system â€” all by itself â€” and comes back with a full diagnosis. You just asked the question.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: What is MCP?\n",
    "\n",
    "**MCP = Model Context Protocol**\n",
    "\n",
    "It's an **open standard protocol** (like HTTP, but for AI tools) created by Anthropic in late 2024. It defines a *universal language* for how AI models talk to external tools, data sources, and services.\n",
    "\n",
    "The key insight: **instead of MÃ—N custom integrations, you build M model connectors + N tool servers = M+N total work.**\n",
    "\n",
    "```\n",
    "Claude â”€â”€â”€â”€ MCP â”€â”€â”€â”€â†’ [ MCP Server: GitHub ]\n",
    "GPT-4  â”€â”€â”€â”€ MCP â”€â”€â”€â”€â†’ [ MCP Server: GitHub ]  â† same server, reused!\n",
    "Gemini â”€â”€â”€â”€ MCP â”€â”€â”€â”€â†’ [ MCP Server: GitHub ]  â† same server, reused!\n",
    "                       [ MCP Server: Notion ]\n",
    "                       [ MCP Server: Postgres ]\n",
    "```\n",
    "\n",
    "GitHub only needs to build *one* MCP server. Every AI model that speaks MCP can use it automatically. This is the USB-C moment for AI integrations.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: The Architecture â€” How It Actually Works\n",
    "\n",
    "MCP follows a **client-server architecture**. There are three main players:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              HOST APPLICATION           â”‚\n",
    "â”‚  (Claude Desktop, Cursor, your app)     â”‚\n",
    "â”‚                                         â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚\n",
    "â”‚   â”‚ MCP CLIENT  â”‚â—„â”€â”€â”€â”€ talks to â”€â”€â”€â”€â”€â”€â–º â”‚ MCP SERVER\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚ (GitHub, Postgres, etc.)\n",
    "â”‚         â–²                               â”‚\n",
    "â”‚         â”‚                               â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                        â”‚\n",
    "â”‚   â”‚    LLM     â”‚                        â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Let's break each piece down:\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ  The Host\n",
    "\n",
    "The **host** is the application the user is actually running. Examples:\n",
    "- Claude Desktop (Anthropic's desktop app)\n",
    "- Cursor (AI code editor)\n",
    "- Your own custom AI application\n",
    "\n",
    "The host is responsible for:\n",
    "- Running the LLM\n",
    "- Managing MCP clients\n",
    "- Presenting results to the user\n",
    "- Enforcing security and permissions\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“± The MCP Client\n",
    "\n",
    "The **client** lives *inside* the host application. It's the bridge between the LLM and the outside world.\n",
    "\n",
    "Think of the client as Alex's personal assistant. When Alex (the LLM) says \"I need to check the GitHub repo\", the client is the one who actually goes and does it.\n",
    "\n",
    "Each client maintains a **1-to-1 connection** with one MCP server.\n",
    "\n",
    "Responsibilities:\n",
    "- Connect to MCP servers\n",
    "- Discover what capabilities each server offers\n",
    "- Send requests to servers on behalf of the LLM\n",
    "- Return results back to the LLM\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ–¥ï¸ The MCP Server\n",
    "\n",
    "The **server** is the thing that actually has access to external resources. It's a lightweight process that wraps a tool or data source and exposes it in MCP's standard format.\n",
    "\n",
    "Examples of MCP servers:\n",
    "- A GitHub MCP server (can read/write repos, issues, PRs)\n",
    "- A PostgreSQL MCP server (can run queries)\n",
    "- A filesystem MCP server (can read/write files on your computer)\n",
    "- A Slack MCP server (can read/post messages)\n",
    "- A web search MCP server (can search the internet)\n",
    "\n",
    "The server exposes three types of things to clients:\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: The Three Primitives â€” What Servers Actually Expose\n",
    "\n",
    "This is the core of MCP. Every server can expose up to three types of capabilities:\n",
    "\n",
    "### 1. ğŸ”§ Tools\n",
    "**What:** Functions the LLM can *call* to take actions or get computed results.  \n",
    "**Analogy:** Tools are like API endpoints â€” \"do this thing.\"  \n",
    "**Examples:**\n",
    "- `search_github_issues(query, repo)` â†’ returns list of issues\n",
    "- `run_sql_query(sql)` â†’ returns query results\n",
    "- `send_slack_message(channel, text)` â†’ sends a message\n",
    "- `create_file(path, content)` â†’ creates a file\n",
    "\n",
    "Tools are **model-controlled** â€” the LLM decides when to call them and with what arguments. This is the most powerful primitive.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ğŸ“„ Resources\n",
    "**What:** Data that the server exposes for the LLM to *read*.  \n",
    "**Analogy:** Resources are like files or URLs â€” \"here's some data.\"  \n",
    "**Examples:**\n",
    "- The contents of a file: `file:///home/user/project/main.py`\n",
    "- A database schema: `postgres://mydb/schema`\n",
    "- A webpage: `https://docs.example.com/api`\n",
    "\n",
    "Resources are **application-controlled** â€” the host decides which resources to include in the LLM's context. The LLM doesn't directly call resources; the app decides what to include.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ğŸ’¬ Prompts\n",
    "**What:** Pre-written prompt templates that the server provides.  \n",
    "**Analogy:** Prompts are like saved macros or slash commands â€” \"use this template.\"  \n",
    "**Examples:**\n",
    "- A \"debug this code\" prompt that automatically structures a debugging workflow\n",
    "- A \"summarize this PR\" prompt\n",
    "- A \"write a commit message\" prompt\n",
    "\n",
    "Prompts are **user-controlled** â€” the user explicitly picks which prompt template to invoke.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: The Communication Protocol â€” How Do They Talk?\n",
    "\n",
    "MCP uses **JSON-RPC 2.0** as its message format. This is a simple, well-established standard where:\n",
    "- Messages are JSON objects\n",
    "- Each request has a method name and parameters\n",
    "- Responses return results or errors\n",
    "\n",
    "### Transport Layers\n",
    "\n",
    "MCP supports two transport mechanisms:\n",
    "\n",
    "**1. Stdio (Standard Input/Output)**  \n",
    "Used when the server runs as a local process on the same machine. The client launches the server as a subprocess and communicates via stdin/stdout pipes.\n",
    "\n",
    "```\n",
    "Host App â†’ spawns â†’ MCP Server process\n",
    "         â†â”€â”€â”€â”€â”€â”€â”€ JSON messages via stdin/stdout â”€â”€â”€â”€â”€â”€â†’\n",
    "```\n",
    "\n",
    "This is the most common setup for local tools (filesystem access, local databases, local dev tools).\n",
    "\n",
    "**2. HTTP + SSE (Server-Sent Events)**  \n",
    "Used when the server is remote â€” running on a different machine or in the cloud.\n",
    "\n",
    "```\n",
    "Host App â”€â”€â”€â”€ HTTP POST â”€â”€â”€â†’ Remote MCP Server\n",
    "         â†â”€â”€â”€ SSE stream â”€â”€â”€\n",
    "```\n",
    "\n",
    "SSE allows the server to stream responses back, which is useful for long-running operations.\n",
    "\n",
    "---\n",
    "\n",
    "### The Lifecycle of a Request\n",
    "\n",
    "Here's exactly what happens when you ask an AI assistant something that requires a tool:\n",
    "\n",
    "```\n",
    "You: \"What are the open issues in my GitHub repo?\"\n",
    "\n",
    "1. HOST receives your message, passes it to the LLM\n",
    "\n",
    "2. LLM thinks: \"I need to check GitHub. Let me call the \n",
    "   search_issues tool.\"\n",
    "   \n",
    "3. LLM outputs a tool call:\n",
    "   {\n",
    "     \"tool\": \"search_github_issues\",\n",
    "     \"arguments\": { \"repo\": \"myuser/myrepo\", \"state\": \"open\" }\n",
    "   }\n",
    "\n",
    "4. MCP CLIENT receives this tool call\n",
    "\n",
    "5. CLIENT sends to MCP SERVER (GitHub server):\n",
    "   {\n",
    "     \"jsonrpc\": \"2.0\",\n",
    "     \"method\": \"tools/call\",\n",
    "     \"params\": {\n",
    "       \"name\": \"search_github_issues\",\n",
    "       \"arguments\": { \"repo\": \"myuser/myrepo\", \"state\": \"open\" }\n",
    "     }\n",
    "   }\n",
    "\n",
    "6. MCP SERVER calls the actual GitHub API with your credentials\n",
    "\n",
    "7. GitHub API returns data to the MCP server\n",
    "\n",
    "8. MCP SERVER sends the result back to the CLIENT:\n",
    "   {\n",
    "     \"jsonrpc\": \"2.0\",\n",
    "     \"result\": {\n",
    "       \"content\": [{ \"type\": \"text\", \"text\": \"Issue #42: Bug in login...\" }]\n",
    "     }\n",
    "   }\n",
    "\n",
    "9. CLIENT passes result back to the LLM\n",
    "\n",
    "10. LLM formulates its response using this real data\n",
    "\n",
    "11. You see: \"You have 3 open issues: #42 Bug in login, #43 \n",
    "    Performance issue, #44 Docs missing...\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6: The Connection Handshake â€” Initialization\n",
    "\n",
    "Before any of the above can happen, the client and server need to introduce themselves. Here's how:\n",
    "\n",
    "**Step 1: Initialize**\n",
    "```json\n",
    "Client â†’ Server:\n",
    "{\n",
    "  \"method\": \"initialize\",\n",
    "  \"params\": {\n",
    "    \"protocolVersion\": \"2024-11-05\",\n",
    "    \"clientInfo\": { \"name\": \"Claude Desktop\", \"version\": \"1.0\" },\n",
    "    \"capabilities\": { \"sampling\": {} }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Step 2: Server responds with its capabilities**\n",
    "```json\n",
    "Server â†’ Client:\n",
    "{\n",
    "  \"result\": {\n",
    "    \"protocolVersion\": \"2024-11-05\",\n",
    "    \"serverInfo\": { \"name\": \"github-mcp-server\", \"version\": \"0.1\" },\n",
    "    \"capabilities\": {\n",
    "      \"tools\": { \"listChanged\": true },\n",
    "      \"resources\": { \"subscribe\": true }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Step 3: Initialized notification**\n",
    "```json\n",
    "Client â†’ Server:\n",
    "{ \"method\": \"notifications/initialized\" }\n",
    "```\n",
    "\n",
    "Now they're connected. The client can start asking \"what tools do you have?\" and the server will list everything it offers.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 7: Discovery â€” How Does the LLM Know What Tools Exist?\n",
    "\n",
    "When a client connects to a server, it asks for a list of everything available:\n",
    "\n",
    "**List Tools:**\n",
    "```json\n",
    "Client â†’ Server: { \"method\": \"tools/list\" }\n",
    "\n",
    "Server â†’ Client:\n",
    "{\n",
    "  \"tools\": [\n",
    "    {\n",
    "      \"name\": \"search_issues\",\n",
    "      \"description\": \"Search GitHub issues in a repository\",\n",
    "      \"inputSchema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"repo\": { \"type\": \"string\", \"description\": \"owner/repo format\" },\n",
    "          \"query\": { \"type\": \"string\", \"description\": \"search query\" }\n",
    "        },\n",
    "        \"required\": [\"repo\"]\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"create_issue\",\n",
    "      \"description\": \"Create a new GitHub issue\",\n",
    "      ...\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "This tool list gets injected into the LLM's context (system prompt). The LLM now *knows* these tools exist and understands how to call them. This is how the LLM learns what it can do at runtime â€” it's dynamic, not hardcoded.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 8: Security â€” This Seems Dangerous?\n",
    "\n",
    "You're right to wonder. Giving an AI model the ability to call tools that can read your files, write to databases, and send messages is powerful and potentially scary.\n",
    "\n",
    "MCP has several security mechanisms:\n",
    "\n",
    "**1. User Consent**  \n",
    "Hosts must present tool calls to users and get approval before executing, especially for destructive operations. Claude Desktop, for example, shows you \"Claude wants to use the filesystem tool â€” Allow?\"\n",
    "\n",
    "**2. Sandboxing**  \n",
    "Each MCP server only has access to what it's configured for. A filesystem server might only have access to your project folder, not your entire computer.\n",
    "\n",
    "**3. Local by Default**  \n",
    "Most MCP servers run locally on your machine (via stdio), so no data goes to a third-party server. Your credentials never leave your machine.\n",
    "\n",
    "**4. Scoped Credentials**  \n",
    "Servers use scoped API tokens with minimal necessary permissions. A GitHub server might only have read access.\n",
    "\n",
    "**5. The LLM Doesn't Execute â€” It Requests**  \n",
    "The LLM cannot directly execute anything. It can only *request* a tool call. The host/client decides whether to honor that request. This is an important architectural decision â€” humans remain in control.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 9: Building Your Own MCP Server â€” Practical\n",
    "\n",
    "Let's make this concrete. Here's how you'd build a simple MCP server in Python using the official SDK:\n",
    "\n",
    "```python\n",
    "from mcp.server import Server\n",
    "from mcp.server.stdio import stdio_server\n",
    "from mcp import types\n",
    "\n",
    "# Create the server\n",
    "app = Server(\"my-todo-server\")\n",
    "\n",
    "# In-memory todo list\n",
    "todos = []\n",
    "\n",
    "# Define a tool: add a todo\n",
    "@app.list_tools()\n",
    "async def list_tools():\n",
    "    return [\n",
    "        types.Tool(\n",
    "            name=\"add_todo\",\n",
    "            description=\"Add a new todo item\",\n",
    "            inputSchema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"task\": {\"type\": \"string\", \"description\": \"The task to add\"}\n",
    "                },\n",
    "                \"required\": [\"task\"]\n",
    "            }\n",
    "        ),\n",
    "        types.Tool(\n",
    "            name=\"list_todos\",\n",
    "            description=\"List all todo items\",\n",
    "            inputSchema={\"type\": \"object\", \"properties\": {}}\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Handle tool calls\n",
    "@app.call_tool()\n",
    "async def call_tool(name: str, arguments: dict):\n",
    "    if name == \"add_todo\":\n",
    "        task = arguments[\"task\"]\n",
    "        todos.append(task)\n",
    "        return [types.TextContent(type=\"text\", text=f\"Added: {task}\")]\n",
    "    \n",
    "    elif name == \"list_todos\":\n",
    "        if not todos:\n",
    "            return [types.TextContent(type=\"text\", text=\"No todos yet!\")]\n",
    "        todo_list = \"\\n\".join(f\"- {t}\" for t in todos)\n",
    "        return [types.TextContent(type=\"text\", text=todo_list)]\n",
    "\n",
    "# Run the server\n",
    "async def main():\n",
    "    async with stdio_server() as (read, write):\n",
    "        await app.run(read, write, app.create_initialization_options())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    asyncio.run(main())\n",
    "```\n",
    "\n",
    "That's it. You just built an MCP server. Now any MCP-compatible AI client can use it.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 10: Configuring MCP in a Host (Claude Desktop Example)\n",
    "\n",
    "Once you have an MCP server, you tell your host app about it. For Claude Desktop, you edit a config file:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"my-todo-server\": {\n",
    "      \"command\": \"python\",\n",
    "      \"args\": [\"/path/to/my_todo_server.py\"]\n",
    "    },\n",
    "    \"github\": {\n",
    "      \"command\": \"npx\",\n",
    "      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n",
    "      \"env\": {\n",
    "        \"GITHUB_TOKEN\": \"ghp_your_token_here\"\n",
    "      }\n",
    "    },\n",
    "    \"filesystem\": {\n",
    "      \"command\": \"npx\",\n",
    "      \"args\": [\n",
    "        \"-y\",\n",
    "        \"@modelcontextprotocol/server-filesystem\",\n",
    "        \"/Users/you/projects\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Claude Desktop reads this, launches each server as a subprocess, connects via stdio, and Claude now has access to all these tools.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 11: Sampling â€” The Reverse Direction\n",
    "\n",
    "Here's an advanced concept most people miss: **MCP isn't just one-directional.**\n",
    "\n",
    "In normal flow: Host/LLM â†’ Client â†’ Server (LLM drives everything).\n",
    "\n",
    "But servers can also ask the *LLM* to do things. This is called **Sampling**.\n",
    "\n",
    "**Use case:** Imagine a complex MCP server that needs to summarize 100 documents. Rather than the server implementing its own summarization logic, it can ask the host's LLM to help.\n",
    "\n",
    "```\n",
    "MCP Server â†’ Client â†’ Host â†’ LLM: \"Please summarize this text\"\n",
    "                            LLM â†’ Client â†’ Server: \"Here's the summary\"\n",
    "```\n",
    "\n",
    "This enables **agentic loops** where servers themselves become orchestrators, spinning up sub-tasks for the LLM. It's how you build complex multi-step AI agents with MCP.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 12: MCP vs Other Things â€” Clarifying Confusion\n",
    "\n",
    "**MCP vs Function Calling (OpenAI's tool use):**  \n",
    "Function calling is the LLM's ability to request a tool call. MCP is the *transport and discovery layer* for those tools. They work together â€” MCP tools get exposed to the LLM *as* function calls.\n",
    "\n",
    "**MCP vs LangChain tools:**  \n",
    "LangChain tools are Python functions registered in LangChain's framework. They're not portable â€” they only work in LangChain apps. MCP tools are language-agnostic, framework-agnostic, and any compatible host can use them.\n",
    "\n",
    "**MCP vs REST APIs:**  \n",
    "REST APIs require custom integration code for every caller. MCP is a *standard* so any MCP client can talk to any MCP server without custom code, just like how any browser can load any HTTP website.\n",
    "\n",
    "**MCP vs Plugins (ChatGPT plugins):**  \n",
    "ChatGPT plugins were OpenAI-specific, required their approval, and were shut down. MCP is open, decentralized â€” anyone can build a server, anyone can use it.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 13: The Ecosystem Today\n",
    "\n",
    "The MCP ecosystem has exploded. There are hundreds of community-built MCP servers for:\n",
    "\n",
    "**Developer Tools:** GitHub, GitLab, Jira, Linear, Sentry  \n",
    "**Databases:** PostgreSQL, MySQL, SQLite, MongoDB, Redis  \n",
    "**Productivity:** Notion, Google Docs, Obsidian, Todoist  \n",
    "**Communication:** Slack, Discord, Gmail, Outlook  \n",
    "**Cloud:** AWS, GCP, Azure, Terraform  \n",
    "**Local:** Filesystem, Terminal/Shell, Browser automation  \n",
    "**Data:** Pandas, Excel, CSV processing  \n",
    "\n",
    "Hosts that support MCP: Claude Desktop, Cursor, Zed editor, Continue.dev, Cline, Windsurf, and hundreds of custom apps.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 14: The Mental Model â€” Summary\n",
    "\n",
    "Here's the complete picture in your head:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  THE WORLD BEFORE MCP: Every integration is custom          â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  LLM-A â”€â”€customâ”€â”€â–º Tool-1                                   â”‚\n",
    "â”‚  LLM-A â”€â”€customâ”€â”€â–º Tool-2   â† snowflakes, unmaintainable   â”‚\n",
    "â”‚  LLM-B â”€â”€customâ”€â”€â–º Tool-1                                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  THE WORLD WITH MCP: Standard protocol like USB-C           â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚  User    â”‚      â”‚     HOST     â”‚      â”‚ MCP Server  â”‚   â”‚\n",
    "â”‚  â”‚          â”‚â—„â”€â”€â”€â”€â–ºâ”‚  (App + LLM) â”‚â—„â”€â”€â”€â”€â–ºâ”‚  (GitHub)   â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  MCP Client  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ MCP Server  â”‚   â”‚\n",
    "â”‚                                          â”‚ (Postgres)  â”‚   â”‚\n",
    "â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚                                          â”‚ MCP Server  â”‚   â”‚\n",
    "â”‚                                          â”‚ (Slack)     â”‚   â”‚\n",
    "â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "The LLM is now a **brain with hands.** It can reach out into the world, read data, take actions, and come back with real results â€” all through a standardized, secure, composable protocol.\n",
    "\n",
    "---\n",
    "\n",
    "## TL;DR â€” The Core Ideas\n",
    "\n",
    "| Concept | Plain English |\n",
    "|---|---|\n",
    "| **MCP** | USB-C standard for connecting AI brains to tools |\n",
    "| **Host** | The app the user runs (Claude Desktop, Cursor) |\n",
    "| **Client** | The bridge inside the host that talks to servers |\n",
    "| **Server** | A wrapper around a tool/data source that speaks MCP |\n",
    "| **Tools** | Actions the LLM can invoke (functions) |\n",
    "| **Resources** | Data the LLM can read (files, URLs, database rows) |\n",
    "| **Prompts** | Reusable prompt templates the server provides |\n",
    "| **Transport** | Stdio for local, HTTP+SSE for remote |\n",
    "| **Protocol** | JSON-RPC 2.0 messages |\n",
    "| **Security** | Local-first, user consent, scoped permissions |\n",
    "\n",
    "MCP is essentially the **TCP/IP moment for AI agents** â€” a foundational protocol that makes everything interoperable, so the ecosystem can grow without everyone rebuilding the same integrations over and over. It's what turns an AI from a chatbot into an *agent*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f242a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
